{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center> TAREA 1 <br><img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/MARCA-Color.jpg\" title=\"Title text\" width=\"20%\" height=\"15%\" /><br>Andrea Carolina Reales Villalba -- Jesus Eduardo Ortiz Sandoval <BR>INF 477 REDES NEURONALES </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=text-justify><center><h2><b>Primer punto <br></h2></center>\n",
    "    En esta primera parte del desarrollo de la tarea, encontramos un <b>\"approach\"</b> muy interesante a la estructuración y programacion de ANN en computadpras. En primera instancia observamos todo lo referente a los tipos de datos, la estructuracion de un dataframe, las funciones de <b> SPLIT, NORMALIZACIÓN, PREPROCESAMIENTO, etc. <br></b>\n",
    " El primer dato de relevancia que encontramos es la cantidad de muestras, y como fueron divididas para realizar todos los procesos: <br><br>\n",
    "    Total muestras entrenamiento: 9745 <br>\n",
    "    Total muestras validación: 4060  <br>\n",
    "    Total muestras testeo: 2437   <br><hr>\n",
    "    Es bueno hacer unas acotaciones del primer punto del trabajo:<br>\n",
    "    <ul>\n",
    "<li>Librerias Juntas</li>\n",
    "<li>Eliminación de Columnas</li>\n",
    "        <li><font color=red><b>60-25-15</FONT></B></li>\n",
    "</ul> \n",
    "    <hr>\n",
    " Podemos afirmar positivamente que el objetivo de este punto es el de hacer un analisis de sensibilidad a los parametros y funciones que son utilizadas para el entrenamiento de una arquitectura neuronal de multiples capas, consecuentemente y secuencialmente los parametros que seran evaluados son los siguientes:<br>\n",
    "    <ul>\n",
    "        <li><b><font color=blue>Función de activacion * (ReLU-SIGMOIDAL)</font></b></li>\n",
    "        <li><b>Learning Rate</b></li>\n",
    "          <li><b>Progressive decay</b></li>\n",
    "          <li><b>Mini-Batches</b></li>\n",
    "          <li><b>Optimizadores</b></li>\n",
    "          <li><b>Weight Decay</b></li>\n",
    "          <li><b>Dropout</b></li>\n",
    "          <li><b>K Folds</b></li>\n",
    "    </ul>\n",
    "    </div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"funcionsigmoide.PNG\" WIDTH=640 HEIGHT=480 BORDER=0  ALIGN=\"CENTER\"> \n",
    "    <IMG SRC=\"funcionrelu.PNG\" WIDTH=640 HEIGHT=480 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=text-justify>\n",
    "    Cuando aumentamos el <b>lear rate</b> observamos que el algoritma no logra converger obteniendo como  resultado todos los valores de perdida NaN, esto nos indica que hay un punto en el que <b> learning rate</b> vuelve inutil el algoritmo, por que en lugar de generar mejor aprendizaje, encuentra valores imposibles de computar.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"decay.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"mini-batch.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizadores (Adam, RMSprop, Adagrad*, Adadelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"optimizadores.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> \n",
    "<IMG SRC=\"optimizador-relu.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"weightdecay.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> \n",
    "<IMG SRC=\"wdhigh.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\">     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"dropout.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SCORES</B><BR>[0.4115317242001191,\n",
    " 0.3742644849069428,\n",
    " <font color=red><b>0.8357612235751625,</font></b>\n",
    " 0.5041005475329031,\n",
    " 0.4887298948646142]<br><br>\n",
    " <div class=text-justify> En la parte superior obtenemos los resultados de los scores de K=5 validaciones con sus respectivos modelos generados, es importante resaltar que en la mayoria de los modelos tuvimos un accuracy promedio de 50, y en un modelo el tercero para ser mas especifico tuvimos un score de 83, lo que nos indica que hay una buena relación de datos, pero que tambien hay una combinación que en el momento del entrenamiento puede ayudar muucho mas para la optimización de nuestra red.\n",
    "    </div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión Punto 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=text-justify> Poder realizar este tipo de trabajo genera un <b>\"background</b> muy fuerte para poder trabajar con redes neuronales, en muchos casos cuando se encuentran este tipo de algoritmos en internet, vienen programados con funciones predeterminadas, o que trabajan para un tipo de dataframe, y cuando se valida con las necesidades especificas de un proyecto no se logran obtener resultados similares, esto debido a que cada set de datos requiere de un tratamiento especial de información, asi como de sintonización de todos estos parametros que nos permiten optimizar el trabajo computacional y poder encontrar los mejores resultados de error sin caer en un posible <b>\"overfiting\"</b>\n",
    "    </div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=text-justify><h2><b><center> Segundo punto </center><br></h2> \n",
    "    </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogramas GW 1 capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"puntob.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogramas GW Redes Profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"puntoc.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogramas GW con Glorot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"puntod.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogramas GW con F.A Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"puntoe.PNG\" WIDTH=940 HEIGHT=680 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red con parametros propios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"puntof.PNG\" WIDTH=640 HEIGHT=480 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion Softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"puntog.PNG\" WIDTH=640 HEIGHT=480 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal Shallow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"puntoh.PNG\" WIDTH=640 HEIGHT=480 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión Punto 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=text-justify> \n",
    "Este punto es un completo ideal para el primer ejercicio por que se pudo analizar a profundidad un elemento clave de las redes neuronales como lo son los pesos, su inicialización, y que ocurre tras el entrenamiento, poder comprender en que consiste el desvanecimiento del gradiente, y sobre todo algo que es muy importante, poder entender que las redes profundas tienen su aplicación especifica, y que en este caso se obtienen mejores resultados con redes poco profundas. \n",
    " </div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=text-justify><h2><b><center> Tercer punto </center></h2> \n",
    "    </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=text-justify>\n",
    "    Este ejercicio es el de mayor complejidad de la tarea, pues se debia poder entender correctamente el set de datos, elegir la representación adecuada de los datos y luego diseñar un arreglo neuronal que pudiera cumplir con el objetivo desde la perspectiva que se tomara como <b>Regresión</b> o <b>Clasificación</b>, dependiendo de cual es nuestra elección los parametros de la red y de medición cambian inmediatamente. <br>\n",
    " El primer dato de relevancia que encontramos es la cantidad de muestras, y como fueron divididas para realizar todos los procesos: <br><br>\n",
    "Numero de caracteristicas de entrada= 11 Total de muestras= 3500 Total datos entrenamiento= 38500\n",
    "Numero de caracteristicas de entrada= 11 Total de muestras= 1050 Total datos test= 11550\n",
    "      </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model = Sequential()\n",
    "idim=X_train.shape[1]\n",
    "model.add(Dense(10,input_dim=idim,kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10,input_dim=idim,kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(76, kernel_initializer='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "sgd = Adam(lr=0.02)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy',metrics = [\"accuracy\"])\n",
    "history = model.fit(X_train, y_onehot, epochs=30,  validation_data=(X_val, y_onehot1))\n",
    " \t```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"PUNTO31.PNG\" WIDTH=640 HEIGHT=480 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "plt.figure(1,figsize=(12,8))\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.legend(('Val_Acc', 'Acc' ,'Val_Loss','Loss'))\n",
    "plt.title('Entrenamiento con Relu-Softmax and Categorical Cross Entropy')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epochs')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG SRC=\"PUNTO32.PNG\" WIDTH=640 HEIGHT=480 BORDER=0  ALIGN=\"CENTER\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión Punto 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=text-justify> \n",
    "Este ejercicio tiene un nivel de complejidad alto, ya que se tienen una gran cantidad de datos, diferentes clases y varias formas de abordarlo,aca se puso en practica todo lo aprendido en los puntos anteriores, en cada entrenamiento ibamos evaluando los diferentes parametros para poder elegir correctamente la representación de los datos o saber si lo abordamos como regreso o como problema de clasificación.\n",
    " </div>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
