{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center> Predicción de Entalpia de Atomización <br>Andrea Reales && Jesus Ortiz </center>\n",
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"15%\" height=\"15%\" />\n",
    "a) Construya un *dataframe* con los datos a analizar y descríbalo brevemete. Además, realice la división de éste en los conjuntos de entrenamiento, validación y testeo correspondientes. Comente por qué se deben eliminar ciertas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#datos= pd.read_csv(\"C:/Users/jesus/Desktop/REDES NEURONALES/roboBohr.csv\")\n",
    "datos= pd.read_csv(\"C:/Users/Jesus/Documents/GitHub/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante hacer un analisis previo de los datos cargados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1266</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.817765</td>\n",
       "      <td>12.469551</td>\n",
       "      <td>12.458130</td>\n",
       "      <td>12.454607</td>\n",
       "      <td>12.447345</td>\n",
       "      <td>12.433065</td>\n",
       "      <td>12.426926</td>\n",
       "      <td>12.387474</td>\n",
       "      <td>12.365984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.013763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.649126</td>\n",
       "      <td>18.527789</td>\n",
       "      <td>17.891535</td>\n",
       "      <td>17.887995</td>\n",
       "      <td>17.871731</td>\n",
       "      <td>17.852586</td>\n",
       "      <td>17.729842</td>\n",
       "      <td>15.864270</td>\n",
       "      <td>15.227643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.161019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.830377</td>\n",
       "      <td>12.512263</td>\n",
       "      <td>12.404775</td>\n",
       "      <td>12.394493</td>\n",
       "      <td>12.391564</td>\n",
       "      <td>12.324461</td>\n",
       "      <td>12.238106</td>\n",
       "      <td>10.423249</td>\n",
       "      <td>8.698826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.376619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.875810</td>\n",
       "      <td>17.871259</td>\n",
       "      <td>17.862402</td>\n",
       "      <td>17.850920</td>\n",
       "      <td>17.850440</td>\n",
       "      <td>12.558105</td>\n",
       "      <td>12.557645</td>\n",
       "      <td>12.517583</td>\n",
       "      <td>12.444141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.776438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.883818</td>\n",
       "      <td>17.868256</td>\n",
       "      <td>17.864221</td>\n",
       "      <td>17.818540</td>\n",
       "      <td>12.508657</td>\n",
       "      <td>12.490519</td>\n",
       "      <td>12.450098</td>\n",
       "      <td>10.597068</td>\n",
       "      <td>10.595914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.537140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>17.038820</td>\n",
       "      <td>16.981436</td>\n",
       "      <td>16.167446</td>\n",
       "      <td>16.137631</td>\n",
       "      <td>16.053239</td>\n",
       "      <td>15.713944</td>\n",
       "      <td>15.432893</td>\n",
       "      <td>15.421116</td>\n",
       "      <td>13.799676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.169604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>17.040919</td>\n",
       "      <td>16.975955</td>\n",
       "      <td>16.168874</td>\n",
       "      <td>16.131888</td>\n",
       "      <td>16.073074</td>\n",
       "      <td>15.843838</td>\n",
       "      <td>15.638061</td>\n",
       "      <td>15.160532</td>\n",
       "      <td>13.712149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.378477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>15.190748</td>\n",
       "      <td>15.134397</td>\n",
       "      <td>15.078282</td>\n",
       "      <td>13.721467</td>\n",
       "      <td>13.720334</td>\n",
       "      <td>13.671396</td>\n",
       "      <td>13.655370</td>\n",
       "      <td>13.654554</td>\n",
       "      <td>13.654217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.673737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.648642</td>\n",
       "      <td>18.559611</td>\n",
       "      <td>17.674347</td>\n",
       "      <td>16.152675</td>\n",
       "      <td>14.266867</td>\n",
       "      <td>13.666125</td>\n",
       "      <td>13.657868</td>\n",
       "      <td>13.642132</td>\n",
       "      <td>13.629813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.427851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.563342</td>\n",
       "      <td>17.562598</td>\n",
       "      <td>12.653657</td>\n",
       "      <td>12.540799</td>\n",
       "      <td>12.539160</td>\n",
       "      <td>12.536825</td>\n",
       "      <td>12.508203</td>\n",
       "      <td>12.489843</td>\n",
       "      <td>11.941684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.744178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>18.593826</td>\n",
       "      <td>17.902116</td>\n",
       "      <td>17.791648</td>\n",
       "      <td>17.709349</td>\n",
       "      <td>17.659515</td>\n",
       "      <td>17.569676</td>\n",
       "      <td>17.188025</td>\n",
       "      <td>17.152301</td>\n",
       "      <td>16.273652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.244325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.893322</td>\n",
       "      <td>17.892435</td>\n",
       "      <td>17.891329</td>\n",
       "      <td>17.835056</td>\n",
       "      <td>17.797049</td>\n",
       "      <td>17.796887</td>\n",
       "      <td>17.796494</td>\n",
       "      <td>12.519782</td>\n",
       "      <td>12.518904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.149376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.892036</td>\n",
       "      <td>17.835452</td>\n",
       "      <td>17.796401</td>\n",
       "      <td>12.544441</td>\n",
       "      <td>12.520178</td>\n",
       "      <td>12.520042</td>\n",
       "      <td>12.501077</td>\n",
       "      <td>12.205003</td>\n",
       "      <td>10.614983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.572029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>388.023441</td>\n",
       "      <td>66.102901</td>\n",
       "      <td>35.415029</td>\n",
       "      <td>35.414463</td>\n",
       "      <td>21.068417</td>\n",
       "      <td>20.972084</td>\n",
       "      <td>18.855496</td>\n",
       "      <td>18.854617</td>\n",
       "      <td>18.833136</td>\n",
       "      <td>18.832318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.696046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>19.145558</td>\n",
       "      <td>17.855414</td>\n",
       "      <td>17.779134</td>\n",
       "      <td>12.927104</td>\n",
       "      <td>12.337087</td>\n",
       "      <td>12.336829</td>\n",
       "      <td>12.276143</td>\n",
       "      <td>12.205558</td>\n",
       "      <td>10.753149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.293733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.686211</td>\n",
       "      <td>18.654076</td>\n",
       "      <td>18.029737</td>\n",
       "      <td>16.104124</td>\n",
       "      <td>15.732605</td>\n",
       "      <td>15.382404</td>\n",
       "      <td>14.561514</td>\n",
       "      <td>13.653314</td>\n",
       "      <td>13.653061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.248055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>23.621854</td>\n",
       "      <td>23.620385</td>\n",
       "      <td>20.741845</td>\n",
       "      <td>18.712919</td>\n",
       "      <td>15.755895</td>\n",
       "      <td>15.645016</td>\n",
       "      <td>15.533408</td>\n",
       "      <td>15.392356</td>\n",
       "      <td>14.897643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.170327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.882498</td>\n",
       "      <td>20.704776</td>\n",
       "      <td>18.830990</td>\n",
       "      <td>18.239768</td>\n",
       "      <td>18.026817</td>\n",
       "      <td>14.817389</td>\n",
       "      <td>14.804453</td>\n",
       "      <td>14.229886</td>\n",
       "      <td>13.687719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.116150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>17.152774</td>\n",
       "      <td>16.979009</td>\n",
       "      <td>16.165980</td>\n",
       "      <td>16.131888</td>\n",
       "      <td>16.122930</td>\n",
       "      <td>16.090475</td>\n",
       "      <td>15.157053</td>\n",
       "      <td>12.520047</td>\n",
       "      <td>12.519557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.642478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.792001</td>\n",
       "      <td>20.781995</td>\n",
       "      <td>18.699353</td>\n",
       "      <td>15.970705</td>\n",
       "      <td>15.420394</td>\n",
       "      <td>14.874331</td>\n",
       "      <td>12.647200</td>\n",
       "      <td>12.647035</td>\n",
       "      <td>12.614159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.329572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.752137</td>\n",
       "      <td>19.148052</td>\n",
       "      <td>18.575154</td>\n",
       "      <td>17.872719</td>\n",
       "      <td>17.850143</td>\n",
       "      <td>17.739271</td>\n",
       "      <td>14.677496</td>\n",
       "      <td>12.976874</td>\n",
       "      <td>12.620736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.170857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>388.023441</td>\n",
       "      <td>29.794358</td>\n",
       "      <td>29.450794</td>\n",
       "      <td>19.838451</td>\n",
       "      <td>19.640707</td>\n",
       "      <td>18.080196</td>\n",
       "      <td>15.228163</td>\n",
       "      <td>15.179768</td>\n",
       "      <td>15.179574</td>\n",
       "      <td>13.853881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.796349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.707815</td>\n",
       "      <td>17.725040</td>\n",
       "      <td>17.690824</td>\n",
       "      <td>12.899468</td>\n",
       "      <td>12.809439</td>\n",
       "      <td>12.740222</td>\n",
       "      <td>12.667372</td>\n",
       "      <td>12.576495</td>\n",
       "      <td>12.359702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.500986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>17.139133</td>\n",
       "      <td>16.062083</td>\n",
       "      <td>15.810563</td>\n",
       "      <td>15.258986</td>\n",
       "      <td>15.258296</td>\n",
       "      <td>13.676434</td>\n",
       "      <td>13.675309</td>\n",
       "      <td>13.654199</td>\n",
       "      <td>13.652824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.520089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>15.192917</td>\n",
       "      <td>15.192453</td>\n",
       "      <td>15.191957</td>\n",
       "      <td>13.696268</td>\n",
       "      <td>13.695720</td>\n",
       "      <td>13.655091</td>\n",
       "      <td>13.653141</td>\n",
       "      <td>13.653066</td>\n",
       "      <td>13.651874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.197458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.722774</td>\n",
       "      <td>18.596453</td>\n",
       "      <td>17.866674</td>\n",
       "      <td>17.743940</td>\n",
       "      <td>14.701212</td>\n",
       "      <td>13.740966</td>\n",
       "      <td>13.738704</td>\n",
       "      <td>13.654970</td>\n",
       "      <td>13.653160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.505988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.765497</td>\n",
       "      <td>19.143140</td>\n",
       "      <td>18.645819</td>\n",
       "      <td>17.736610</td>\n",
       "      <td>17.699663</td>\n",
       "      <td>17.687518</td>\n",
       "      <td>14.701643</td>\n",
       "      <td>14.216043</td>\n",
       "      <td>13.366176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.299903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.800040</td>\n",
       "      <td>12.574005</td>\n",
       "      <td>12.558918</td>\n",
       "      <td>12.517330</td>\n",
       "      <td>12.384754</td>\n",
       "      <td>12.355934</td>\n",
       "      <td>12.344451</td>\n",
       "      <td>12.316290</td>\n",
       "      <td>12.306096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.331558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>388.023441</td>\n",
       "      <td>46.500552</td>\n",
       "      <td>46.493093</td>\n",
       "      <td>34.676868</td>\n",
       "      <td>29.085904</td>\n",
       "      <td>19.000431</td>\n",
       "      <td>18.995270</td>\n",
       "      <td>18.631019</td>\n",
       "      <td>18.630794</td>\n",
       "      <td>15.138030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.290619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>18.092880</td>\n",
       "      <td>15.249478</td>\n",
       "      <td>15.177545</td>\n",
       "      <td>13.653715</td>\n",
       "      <td>13.653147</td>\n",
       "      <td>13.652852</td>\n",
       "      <td>13.652687</td>\n",
       "      <td>13.652476</td>\n",
       "      <td>13.651550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.586835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16212</th>\n",
       "      <td>388.023441</td>\n",
       "      <td>29.658517</td>\n",
       "      <td>29.655842</td>\n",
       "      <td>23.552305</td>\n",
       "      <td>23.550885</td>\n",
       "      <td>23.446952</td>\n",
       "      <td>23.446603</td>\n",
       "      <td>23.424316</td>\n",
       "      <td>22.152848</td>\n",
       "      <td>22.017541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.849553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>18.635498</td>\n",
       "      <td>17.804251</td>\n",
       "      <td>15.228614</td>\n",
       "      <td>13.653764</td>\n",
       "      <td>13.653754</td>\n",
       "      <td>13.653413</td>\n",
       "      <td>13.652802</td>\n",
       "      <td>13.652570</td>\n",
       "      <td>13.651962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.408032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16214</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>19.148633</td>\n",
       "      <td>13.653248</td>\n",
       "      <td>13.652988</td>\n",
       "      <td>13.652850</td>\n",
       "      <td>13.652754</td>\n",
       "      <td>13.652495</td>\n",
       "      <td>13.652167</td>\n",
       "      <td>12.965136</td>\n",
       "      <td>12.767064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.100940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16215</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.698171</td>\n",
       "      <td>15.924907</td>\n",
       "      <td>15.855703</td>\n",
       "      <td>13.654009</td>\n",
       "      <td>13.653321</td>\n",
       "      <td>13.653014</td>\n",
       "      <td>13.652836</td>\n",
       "      <td>13.652436</td>\n",
       "      <td>13.651938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.170072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16216</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>18.643484</td>\n",
       "      <td>18.640762</td>\n",
       "      <td>17.802607</td>\n",
       "      <td>17.802277</td>\n",
       "      <td>13.653867</td>\n",
       "      <td>13.653335</td>\n",
       "      <td>13.652740</td>\n",
       "      <td>13.652676</td>\n",
       "      <td>13.652322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.163202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16217</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.870020</td>\n",
       "      <td>15.238941</td>\n",
       "      <td>15.176526</td>\n",
       "      <td>15.176523</td>\n",
       "      <td>12.525824</td>\n",
       "      <td>12.525722</td>\n",
       "      <td>12.507239</td>\n",
       "      <td>12.484568</td>\n",
       "      <td>12.465571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.003575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16218</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>19.150693</td>\n",
       "      <td>16.405490</td>\n",
       "      <td>15.022080</td>\n",
       "      <td>14.895449</td>\n",
       "      <td>12.521816</td>\n",
       "      <td>12.521367</td>\n",
       "      <td>12.488777</td>\n",
       "      <td>12.488520</td>\n",
       "      <td>12.442787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16219</th>\n",
       "      <td>36.858105</td>\n",
       "      <td>14.107138</td>\n",
       "      <td>12.674530</td>\n",
       "      <td>12.544437</td>\n",
       "      <td>12.505901</td>\n",
       "      <td>12.480945</td>\n",
       "      <td>12.469779</td>\n",
       "      <td>12.345929</td>\n",
       "      <td>12.286274</td>\n",
       "      <td>12.247041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.987681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16220</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.698855</td>\n",
       "      <td>20.698855</td>\n",
       "      <td>15.893020</td>\n",
       "      <td>15.892046</td>\n",
       "      <td>15.875667</td>\n",
       "      <td>15.875528</td>\n",
       "      <td>13.653570</td>\n",
       "      <td>13.652995</td>\n",
       "      <td>13.652591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.908604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16221</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>15.754498</td>\n",
       "      <td>15.753712</td>\n",
       "      <td>13.653954</td>\n",
       "      <td>13.653736</td>\n",
       "      <td>13.653736</td>\n",
       "      <td>13.653482</td>\n",
       "      <td>13.653142</td>\n",
       "      <td>13.653000</td>\n",
       "      <td>13.652847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.330795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16222</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.677578</td>\n",
       "      <td>12.519029</td>\n",
       "      <td>12.479941</td>\n",
       "      <td>12.477739</td>\n",
       "      <td>12.452035</td>\n",
       "      <td>12.426147</td>\n",
       "      <td>12.421406</td>\n",
       "      <td>12.308018</td>\n",
       "      <td>12.290278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.007444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16223</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.743557</td>\n",
       "      <td>20.741361</td>\n",
       "      <td>18.598127</td>\n",
       "      <td>18.595527</td>\n",
       "      <td>14.582338</td>\n",
       "      <td>14.576939</td>\n",
       "      <td>12.993091</td>\n",
       "      <td>12.611977</td>\n",
       "      <td>12.602461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.191880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16224</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.801457</td>\n",
       "      <td>20.801457</td>\n",
       "      <td>18.634187</td>\n",
       "      <td>18.634183</td>\n",
       "      <td>14.566950</td>\n",
       "      <td>14.566947</td>\n",
       "      <td>14.254289</td>\n",
       "      <td>14.252941</td>\n",
       "      <td>12.900472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.746408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16225</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.581963</td>\n",
       "      <td>15.924696</td>\n",
       "      <td>15.751649</td>\n",
       "      <td>15.452521</td>\n",
       "      <td>14.852202</td>\n",
       "      <td>13.654823</td>\n",
       "      <td>13.653847</td>\n",
       "      <td>13.653692</td>\n",
       "      <td>13.652830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.988657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16226</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>16.359301</td>\n",
       "      <td>16.358704</td>\n",
       "      <td>15.148839</td>\n",
       "      <td>13.825358</td>\n",
       "      <td>13.825283</td>\n",
       "      <td>13.654598</td>\n",
       "      <td>13.653577</td>\n",
       "      <td>13.653275</td>\n",
       "      <td>13.652769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.752393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16227</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>16.435036</td>\n",
       "      <td>16.433452</td>\n",
       "      <td>15.192854</td>\n",
       "      <td>15.192072</td>\n",
       "      <td>15.191944</td>\n",
       "      <td>13.836318</td>\n",
       "      <td>13.787761</td>\n",
       "      <td>13.744369</td>\n",
       "      <td>13.695440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.044004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16228</th>\n",
       "      <td>36.858105</td>\n",
       "      <td>14.159026</td>\n",
       "      <td>13.653760</td>\n",
       "      <td>13.653143</td>\n",
       "      <td>13.653143</td>\n",
       "      <td>13.652803</td>\n",
       "      <td>13.652756</td>\n",
       "      <td>13.652713</td>\n",
       "      <td>13.092874</td>\n",
       "      <td>12.769178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.988833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16229</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>16.525299</td>\n",
       "      <td>16.435115</td>\n",
       "      <td>14.202458</td>\n",
       "      <td>13.827072</td>\n",
       "      <td>13.743711</td>\n",
       "      <td>13.695585</td>\n",
       "      <td>13.694933</td>\n",
       "      <td>12.759605</td>\n",
       "      <td>12.672241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.635767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16230</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.731531</td>\n",
       "      <td>20.731285</td>\n",
       "      <td>16.422832</td>\n",
       "      <td>16.422798</td>\n",
       "      <td>16.136916</td>\n",
       "      <td>16.135040</td>\n",
       "      <td>15.893446</td>\n",
       "      <td>15.853311</td>\n",
       "      <td>13.748468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.633728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16231</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.221994</td>\n",
       "      <td>18.690270</td>\n",
       "      <td>18.632741</td>\n",
       "      <td>18.405014</td>\n",
       "      <td>17.735798</td>\n",
       "      <td>14.707738</td>\n",
       "      <td>13.842602</td>\n",
       "      <td>13.689474</td>\n",
       "      <td>13.663879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.206702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16232</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.860880</td>\n",
       "      <td>17.780884</td>\n",
       "      <td>17.779987</td>\n",
       "      <td>12.511356</td>\n",
       "      <td>12.510997</td>\n",
       "      <td>12.481791</td>\n",
       "      <td>12.475366</td>\n",
       "      <td>10.571260</td>\n",
       "      <td>10.504954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.823134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16233</th>\n",
       "      <td>388.023441</td>\n",
       "      <td>46.723458</td>\n",
       "      <td>46.722364</td>\n",
       "      <td>28.505545</td>\n",
       "      <td>28.472771</td>\n",
       "      <td>19.796532</td>\n",
       "      <td>19.731292</td>\n",
       "      <td>14.209230</td>\n",
       "      <td>14.144322</td>\n",
       "      <td>13.364883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.964498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16234</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>23.607539</td>\n",
       "      <td>23.606041</td>\n",
       "      <td>21.378625</td>\n",
       "      <td>17.851697</td>\n",
       "      <td>15.649065</td>\n",
       "      <td>15.422739</td>\n",
       "      <td>13.654045</td>\n",
       "      <td>13.653121</td>\n",
       "      <td>13.653119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.918597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16235</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.643181</td>\n",
       "      <td>20.588968</td>\n",
       "      <td>18.703556</td>\n",
       "      <td>18.673772</td>\n",
       "      <td>14.895437</td>\n",
       "      <td>14.894662</td>\n",
       "      <td>13.684300</td>\n",
       "      <td>13.683890</td>\n",
       "      <td>13.650207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.794917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16236</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.610314</td>\n",
       "      <td>20.554562</td>\n",
       "      <td>18.683667</td>\n",
       "      <td>18.653061</td>\n",
       "      <td>14.895197</td>\n",
       "      <td>14.894752</td>\n",
       "      <td>13.684066</td>\n",
       "      <td>13.683760</td>\n",
       "      <td>13.650190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.809176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16237</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.753166</td>\n",
       "      <td>18.624076</td>\n",
       "      <td>17.872009</td>\n",
       "      <td>17.851690</td>\n",
       "      <td>17.851254</td>\n",
       "      <td>17.742176</td>\n",
       "      <td>14.655754</td>\n",
       "      <td>12.706683</td>\n",
       "      <td>12.557785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.876123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16238</th>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.724740</td>\n",
       "      <td>18.579933</td>\n",
       "      <td>17.741621</td>\n",
       "      <td>14.716676</td>\n",
       "      <td>13.697829</td>\n",
       "      <td>13.697558</td>\n",
       "      <td>13.653512</td>\n",
       "      <td>13.652942</td>\n",
       "      <td>13.652387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.105268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16239</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>20.820797</td>\n",
       "      <td>19.150234</td>\n",
       "      <td>19.148721</td>\n",
       "      <td>15.135514</td>\n",
       "      <td>15.123685</td>\n",
       "      <td>12.942704</td>\n",
       "      <td>12.938162</td>\n",
       "      <td>12.488633</td>\n",
       "      <td>12.488061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.801464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16240</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>15.707759</td>\n",
       "      <td>15.707644</td>\n",
       "      <td>13.653838</td>\n",
       "      <td>13.653570</td>\n",
       "      <td>13.653314</td>\n",
       "      <td>13.652591</td>\n",
       "      <td>13.652585</td>\n",
       "      <td>13.652550</td>\n",
       "      <td>12.743890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.335088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16241</th>\n",
       "      <td>53.358707</td>\n",
       "      <td>15.708752</td>\n",
       "      <td>15.708094</td>\n",
       "      <td>13.653893</td>\n",
       "      <td>13.653176</td>\n",
       "      <td>13.653120</td>\n",
       "      <td>13.652930</td>\n",
       "      <td>13.652528</td>\n",
       "      <td>13.652322</td>\n",
       "      <td>12.743688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.336696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16242 rows × 1276 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0          1          2          3          4          5  \\\n",
       "0       73.516695  17.817765  12.469551  12.458130  12.454607  12.447345   \n",
       "1       73.516695  20.649126  18.527789  17.891535  17.887995  17.871731   \n",
       "2       73.516695  17.830377  12.512263  12.404775  12.394493  12.391564   \n",
       "3       73.516695  17.875810  17.871259  17.862402  17.850920  17.850440   \n",
       "4       73.516695  17.883818  17.868256  17.864221  17.818540  12.508657   \n",
       "5       53.358707  17.038820  16.981436  16.167446  16.137631  16.053239   \n",
       "6       53.358707  17.040919  16.975955  16.168874  16.131888  16.073074   \n",
       "7       53.358707  15.190748  15.134397  15.078282  13.721467  13.720334   \n",
       "8       73.516695  20.648642  18.559611  17.674347  16.152675  14.266867   \n",
       "9       73.516695  17.563342  17.562598  12.653657  12.540799  12.539160   \n",
       "10      73.516695  18.593826  17.902116  17.791648  17.709349  17.659515   \n",
       "11      73.516695  17.893322  17.892435  17.891329  17.835056  17.797049   \n",
       "12      73.516695  17.892036  17.835452  17.796401  12.544441  12.520178   \n",
       "13     388.023441  66.102901  35.415029  35.414463  21.068417  20.972084   \n",
       "14      73.516695  19.145558  17.855414  17.779134  12.927104  12.337087   \n",
       "15      73.516695  20.686211  18.654076  18.029737  16.104124  15.732605   \n",
       "16      73.516695  23.621854  23.620385  20.741845  18.712919  15.755895   \n",
       "17      73.516695  20.882498  20.704776  18.830990  18.239768  18.026817   \n",
       "18      53.358707  17.152774  16.979009  16.165980  16.131888  16.122930   \n",
       "19      73.516695  20.792001  20.781995  18.699353  15.970705  15.420394   \n",
       "20      73.516695  20.752137  19.148052  18.575154  17.872719  17.850143   \n",
       "21     388.023441  29.794358  29.450794  19.838451  19.640707  18.080196   \n",
       "22      73.516695  20.707815  17.725040  17.690824  12.899468  12.809439   \n",
       "23      53.358707  17.139133  16.062083  15.810563  15.258986  15.258296   \n",
       "24      53.358707  15.192917  15.192453  15.191957  13.696268  13.695720   \n",
       "25      73.516695  20.722774  18.596453  17.866674  17.743940  14.701212   \n",
       "26      73.516695  20.765497  19.143140  18.645819  17.736610  17.699663   \n",
       "27      73.516695  20.800040  12.574005  12.558918  12.517330  12.384754   \n",
       "28     388.023441  46.500552  46.493093  34.676868  29.085904  19.000431   \n",
       "29      53.358707  18.092880  15.249478  15.177545  13.653715  13.653147   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "16212  388.023441  29.658517  29.655842  23.552305  23.550885  23.446952   \n",
       "16213   73.516695  18.635498  17.804251  15.228614  13.653764  13.653754   \n",
       "16214   53.358707  19.148633  13.653248  13.652988  13.652850  13.652754   \n",
       "16215   73.516695  20.698171  15.924907  15.855703  13.654009  13.653321   \n",
       "16216   73.516695  18.643484  18.640762  17.802607  17.802277  13.653867   \n",
       "16217   73.516695  17.870020  15.238941  15.176526  15.176523  12.525824   \n",
       "16218   53.358707  19.150693  16.405490  15.022080  14.895449  12.521816   \n",
       "16219   36.858105  14.107138  12.674530  12.544437  12.505901  12.480945   \n",
       "16220   73.516695  20.698855  20.698855  15.893020  15.892046  15.875667   \n",
       "16221   53.358707  15.754498  15.753712  13.653954  13.653736  13.653736   \n",
       "16222   73.516695  17.677578  12.519029  12.479941  12.477739  12.452035   \n",
       "16223   73.516695  20.743557  20.741361  18.598127  18.595527  14.582338   \n",
       "16224   73.516695  20.801457  20.801457  18.634187  18.634183  14.566950   \n",
       "16225   73.516695  20.581963  15.924696  15.751649  15.452521  14.852202   \n",
       "16226   53.358707  16.359301  16.358704  15.148839  13.825358  13.825283   \n",
       "16227   53.358707  16.435036  16.433452  15.192854  15.192072  15.191944   \n",
       "16228   36.858105  14.159026  13.653760  13.653143  13.653143  13.652803   \n",
       "16229   53.358707  16.525299  16.435115  14.202458  13.827072  13.743711   \n",
       "16230   73.516695  20.731531  20.731285  16.422832  16.422798  16.136916   \n",
       "16231   73.516695  20.221994  18.690270  18.632741  18.405014  17.735798   \n",
       "16232   73.516695  17.860880  17.780884  17.779987  12.511356  12.510997   \n",
       "16233  388.023441  46.723458  46.722364  28.505545  28.472771  19.796532   \n",
       "16234   73.516695  23.607539  23.606041  21.378625  17.851697  15.649065   \n",
       "16235   73.516695  20.643181  20.588968  18.703556  18.673772  14.895437   \n",
       "16236   73.516695  20.610314  20.554562  18.683667  18.653061  14.895197   \n",
       "16237   73.516695  20.753166  18.624076  17.872009  17.851690  17.851254   \n",
       "16238   73.516695  20.724740  18.579933  17.741621  14.716676  13.697829   \n",
       "16239   53.358707  20.820797  19.150234  19.148721  15.135514  15.123685   \n",
       "16240   53.358707  15.707759  15.707644  13.653838  13.653570  13.653314   \n",
       "16241   53.358707  15.708752  15.708094  13.653893  13.653176  13.653120   \n",
       "\n",
       "               6          7          8          9    ...      1266  1267  \\\n",
       "0      12.433065  12.426926  12.387474  12.365984    ...       0.0   0.0   \n",
       "1      17.852586  17.729842  15.864270  15.227643    ...       0.0   0.0   \n",
       "2      12.324461  12.238106  10.423249   8.698826    ...       0.0   0.0   \n",
       "3      12.558105  12.557645  12.517583  12.444141    ...       0.0   0.0   \n",
       "4      12.490519  12.450098  10.597068  10.595914    ...       0.0   0.0   \n",
       "5      15.713944  15.432893  15.421116  13.799676    ...       0.0   0.0   \n",
       "6      15.843838  15.638061  15.160532  13.712149    ...       0.0   0.0   \n",
       "7      13.671396  13.655370  13.654554  13.654217    ...       0.0   0.0   \n",
       "8      13.666125  13.657868  13.642132  13.629813    ...       0.0   0.0   \n",
       "9      12.536825  12.508203  12.489843  11.941684    ...       0.0   0.0   \n",
       "10     17.569676  17.188025  17.152301  16.273652    ...       0.0   0.0   \n",
       "11     17.796887  17.796494  12.519782  12.518904    ...       0.0   0.0   \n",
       "12     12.520042  12.501077  12.205003  10.614983    ...       0.0   0.0   \n",
       "13     18.855496  18.854617  18.833136  18.832318    ...       0.0   0.0   \n",
       "14     12.336829  12.276143  12.205558  10.753149    ...       0.0   0.0   \n",
       "15     15.382404  14.561514  13.653314  13.653061    ...       0.0   0.0   \n",
       "16     15.645016  15.533408  15.392356  14.897643    ...       0.0   0.0   \n",
       "17     14.817389  14.804453  14.229886  13.687719    ...       0.0   0.0   \n",
       "18     16.090475  15.157053  12.520047  12.519557    ...       0.0   0.0   \n",
       "19     14.874331  12.647200  12.647035  12.614159    ...       0.0   0.0   \n",
       "20     17.739271  14.677496  12.976874  12.620736    ...       0.0   0.0   \n",
       "21     15.228163  15.179768  15.179574  13.853881    ...       0.0   0.0   \n",
       "22     12.740222  12.667372  12.576495  12.359702    ...       0.0   0.0   \n",
       "23     13.676434  13.675309  13.654199  13.652824    ...       0.0   0.0   \n",
       "24     13.655091  13.653141  13.653066  13.651874    ...       0.0   0.0   \n",
       "25     13.740966  13.738704  13.654970  13.653160    ...       0.0   0.0   \n",
       "26     17.687518  14.701643  14.216043  13.366176    ...       0.0   0.0   \n",
       "27     12.355934  12.344451  12.316290  12.306096    ...       0.0   0.0   \n",
       "28     18.995270  18.631019  18.630794  15.138030    ...       0.0   0.0   \n",
       "29     13.652852  13.652687  13.652476  13.651550    ...       0.0   0.0   \n",
       "...          ...        ...        ...        ...    ...       ...   ...   \n",
       "16212  23.446603  23.424316  22.152848  22.017541    ...       0.0   0.0   \n",
       "16213  13.653413  13.652802  13.652570  13.651962    ...       0.0   0.0   \n",
       "16214  13.652495  13.652167  12.965136  12.767064    ...       0.0   0.0   \n",
       "16215  13.653014  13.652836  13.652436  13.651938    ...       0.0   0.0   \n",
       "16216  13.653335  13.652740  13.652676  13.652322    ...       0.0   0.0   \n",
       "16217  12.525722  12.507239  12.484568  12.465571    ...       0.0   0.0   \n",
       "16218  12.521367  12.488777  12.488520  12.442787    ...       0.0   0.0   \n",
       "16219  12.469779  12.345929  12.286274  12.247041    ...       0.0   0.0   \n",
       "16220  15.875528  13.653570  13.652995  13.652591    ...       0.0   0.0   \n",
       "16221  13.653482  13.653142  13.653000  13.652847    ...       0.0   0.0   \n",
       "16222  12.426147  12.421406  12.308018  12.290278    ...       0.0   0.0   \n",
       "16223  14.576939  12.993091  12.611977  12.602461    ...       0.0   0.0   \n",
       "16224  14.566947  14.254289  14.252941  12.900472    ...       0.0   0.0   \n",
       "16225  13.654823  13.653847  13.653692  13.652830    ...       0.0   0.0   \n",
       "16226  13.654598  13.653577  13.653275  13.652769    ...       0.0   0.0   \n",
       "16227  13.836318  13.787761  13.744369  13.695440    ...       0.0   0.0   \n",
       "16228  13.652756  13.652713  13.092874  12.769178    ...       0.0   0.0   \n",
       "16229  13.695585  13.694933  12.759605  12.672241    ...       0.0   0.0   \n",
       "16230  16.135040  15.893446  15.853311  13.748468    ...       0.0   0.0   \n",
       "16231  14.707738  13.842602  13.689474  13.663879    ...       0.0   0.0   \n",
       "16232  12.481791  12.475366  10.571260  10.504954    ...       0.0   0.0   \n",
       "16233  19.731292  14.209230  14.144322  13.364883    ...       0.0   0.0   \n",
       "16234  15.422739  13.654045  13.653121  13.653119    ...       0.0   0.0   \n",
       "16235  14.894662  13.684300  13.683890  13.650207    ...       0.0   0.0   \n",
       "16236  14.894752  13.684066  13.683760  13.650190    ...       0.0   0.0   \n",
       "16237  17.742176  14.655754  12.706683  12.557785    ...       0.0   0.0   \n",
       "16238  13.697558  13.653512  13.652942  13.652387    ...       0.0   0.0   \n",
       "16239  12.942704  12.938162  12.488633  12.488061    ...       0.0   0.0   \n",
       "16240  13.652591  13.652585  13.652550  12.743890    ...       0.0   0.0   \n",
       "16241  13.652930  13.652528  13.652322  12.743688    ...       0.0   0.0   \n",
       "\n",
       "       1268  1269  1270  1271  1272  1273  1274        Eat  \n",
       "0       0.0   0.5   0.0   0.0   0.0   0.0   0.0 -19.013763  \n",
       "1       0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.161019  \n",
       "2       0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.376619  \n",
       "3       0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.776438  \n",
       "4       0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.537140  \n",
       "5       0.0   0.0   0.0   0.0   0.0   0.0   0.0 -16.169604  \n",
       "6       0.0   0.0   0.0   0.0   0.0   0.0   0.0 -17.378477  \n",
       "7       0.0   0.0   0.0   0.0   0.0   0.0   0.0 -15.673737  \n",
       "8       0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.427851  \n",
       "9       0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.744178  \n",
       "10      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.244325  \n",
       "11      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -14.149376  \n",
       "12      0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.572029  \n",
       "13      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.696046  \n",
       "14      0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.293733  \n",
       "15      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.248055  \n",
       "16      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.170327  \n",
       "17      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.116150  \n",
       "18      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.642478  \n",
       "19      0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.329572  \n",
       "20      0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.170857  \n",
       "21      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.796349  \n",
       "22      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.500986  \n",
       "23      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.520089  \n",
       "24      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.197458  \n",
       "25      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.505988  \n",
       "26      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -14.299903  \n",
       "27      0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.331558  \n",
       "28      0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.290619  \n",
       "29      0.0   0.0   0.0   0.0   0.0   0.0   0.0  -9.586835  \n",
       "...     ...   ...   ...   ...   ...   ...   ...        ...  \n",
       "16212   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -6.849553  \n",
       "16213   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.408032  \n",
       "16214   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.100940  \n",
       "16215   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.170072  \n",
       "16216   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.163202  \n",
       "16217   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -15.003575  \n",
       "16218   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.932200  \n",
       "16219   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.987681  \n",
       "16220   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.908604  \n",
       "16221   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.330795  \n",
       "16222   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.007444  \n",
       "16223   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -15.191880  \n",
       "16224   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -6.746408  \n",
       "16225   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.988657  \n",
       "16226   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.752393  \n",
       "16227   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -12.044004  \n",
       "16228   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.988833  \n",
       "16229   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -10.635767  \n",
       "16230   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.633728  \n",
       "16231   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.206702  \n",
       "16232   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -6.823134  \n",
       "16233   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.964498  \n",
       "16234   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.918597  \n",
       "16235   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.794917  \n",
       "16236   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -11.809176  \n",
       "16237   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -8.876123  \n",
       "16238   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.105268  \n",
       "16239   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -16.801464  \n",
       "16240   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.335088  \n",
       "16241   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -13.336696  \n",
       "\n",
       "[16242 rows x 1276 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un dataset de 16242 muestras cada una con 1276 columnas, en nuestro caso equivalen  a XXXXX cada una de ellas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total muestras entrenamiento: 9745\n",
      "Total muestras entrenamiento: 4060\n",
      "Total muestras entrenamiento: 2437\n"
     ]
    }
   ],
   "source": [
    "tra1=len(df_train)\n",
    "val1=len(df_val)\n",
    "test1=len(df_test)\n",
    "print('Total muestras entrenamiento: %d'%(tra1))\n",
    "print('Total muestras entrenamiento: %d'%(val1))\n",
    "print('Total muestras entrenamiento: %d'%(test1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.1) Una buena práctica es la de normalizar los datos antes de trabajar con el modelo. **Explique por qué se aconseja dicho preprocesamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "...\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n",
    "...\n",
    "X_train_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_val_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toda dataset con mucha información requiere normalizar los datos ya que estos contienen caracteristicas que nos permiten ver como estan relacionados los datos y saber cuales son relevantes y cuales no, además si se tienen datos muy separados el error no se puede cuantificar y en el entrenamiento pueden ocurrir repeticiones e incoherencias con la información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Muestre en un gráfico el error cuadrático (MSE) para el conjunto de entrenamiento y de pruebas vs número de *epochs* de entrenamiento, para una red *feedforward* de 3 capas, con 256 unidades ocultas y función de activación sigmoidal. Entrene la red usando gradiente descendente estocástico con tasa de aprendizaje (learning rate) 0.01 y 250 epochs de entrenamiento, en el conjunto de entrenamiento y de validación. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9088/9745 [==========================>...] - ETA: 0s - loss: 1.4779"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1f3e875fd311>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"linear\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20eb5c37eb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Tansig')\n",
    "im.legend(('Val_Loss', 'Loss' ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Repita el paso anterior, utilizado ’**ReLU**’ como función de activación y compare con lo obtenido en b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 8.2687 - val_loss: 2.9805\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 2.0078 - val_loss: 1.7111\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.9595 - val_loss: 1.6027\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.6473 - val_loss: 1.4014\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.6347 - val_loss: 1.3245\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.5528 - val_loss: 2.0329\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.4110 - val_loss: 1.2768\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.3472 - val_loss: 0.9176\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.3033 - val_loss: 0.9384\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.3298 - val_loss: 0.8469\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2857 - val_loss: 0.7814\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2347 - val_loss: 0.7605\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2139 - val_loss: 0.6871\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1954 - val_loss: 0.9228\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2039 - val_loss: 0.6494\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2207 - val_loss: 0.9006\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1834 - val_loss: 0.6223\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1653 - val_loss: 0.6051\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1550 - val_loss: 0.5956\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.1870 - val_loss: 0.5121\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1474 - val_loss: 0.5235\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1458 - val_loss: 0.4807\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1309 - val_loss: 0.4953\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1616 - val_loss: 0.5094\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1267 - val_loss: 0.5264\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1224 - val_loss: 0.4541\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1168 - val_loss: 0.4353\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1206 - val_loss: 0.4289\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1232 - val_loss: 0.4624\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1172 - val_loss: 0.4264\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1056 - val_loss: 0.4338\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1072 - val_loss: 0.5340\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1029 - val_loss: 0.4028\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.1566 - val_loss: 0.4339\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.1208 - val_loss: 0.4353\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0955 - val_loss: 0.4353\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.0917 - val_loss: 0.3884\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0950 - val_loss: 0.3877\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0914 - val_loss: 0.3780\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0842 - val_loss: 0.4320\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0872 - val_loss: 0.3979\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0904 - val_loss: 0.3689\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.0821 - val_loss: 0.3503\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0975 - val_loss: 0.3750\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0763 - val_loss: 0.3515\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0763 - val_loss: 0.3357\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0781 - val_loss: 0.3506\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0853 - val_loss: 0.3947\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0800 - val_loss: 0.3198\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0753 - val_loss: 0.3288\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0752 - val_loss: 0.3553\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0768 - val_loss: 0.3463\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0710 - val_loss: 0.3270\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0661 - val_loss: 0.3962\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0669 - val_loss: 0.3702\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0666 - val_loss: 0.3724\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0644 - val_loss: 0.3117\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0629 - val_loss: 0.2973\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0618 - val_loss: 0.3390\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0613 - val_loss: 0.3334\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0608 - val_loss: 0.3535\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0596 - val_loss: 0.3184\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0569 - val_loss: 0.2917\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0613 - val_loss: 0.3049\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0560 - val_loss: 0.2790\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0541 - val_loss: 0.3084\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0579 - val_loss: 0.2904\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0542 - val_loss: 0.3133\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0617 - val_loss: 0.2966\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0587 - val_loss: 0.3247\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0565 - val_loss: 0.2820\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0529 - val_loss: 0.2840\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0606 - val_loss: 0.2978\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.0556 - val_loss: 0.2965\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0548 - val_loss: 0.3057\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0513 - val_loss: 0.2765\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0504 - val_loss: 0.2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0487 - val_loss: 0.2702\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0546 - val_loss: 0.2675\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0482 - val_loss: 0.2738\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0473 - val_loss: 0.3287\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0486 - val_loss: 0.4655\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0469 - val_loss: 0.3045\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0513 - val_loss: 0.2757\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0465 - val_loss: 0.2867\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0471 - val_loss: 0.2951\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0441 - val_loss: 0.2829\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0452 - val_loss: 0.2689\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0440 - val_loss: 0.2772\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0456 - val_loss: 0.2936\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0510 - val_loss: 0.2770\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0462 - val_loss: 0.2680\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0523 - val_loss: 0.3021\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0444 - val_loss: 0.2811\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0454 - val_loss: 0.2756\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0486 - val_loss: 0.2660\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0412 - val_loss: 0.2749\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0399 - val_loss: 0.2712\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0432 - val_loss: 0.2726\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0469 - val_loss: 0.2631\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0438 - val_loss: 0.2539\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0429 - val_loss: 0.2607\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0402 - val_loss: 0.2713\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0445 - val_loss: 0.2901\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0409 - val_loss: 0.2561\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0367 - val_loss: 0.2632\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0507 - val_loss: 0.2624\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0411 - val_loss: 0.2641\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0410 - val_loss: 0.2700\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0378 - val_loss: 0.2573\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0366 - val_loss: 0.2612\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0392 - val_loss: 0.2595\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0392 - val_loss: 0.2780\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0480 - val_loss: 0.2686\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0383 - val_loss: 0.2564\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0393 - val_loss: 0.2488\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0363 - val_loss: 0.2573\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0378 - val_loss: 0.2789\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0388 - val_loss: 0.2515\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0364 - val_loss: 0.2654\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0338 - val_loss: 0.2553\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0353 - val_loss: 0.2695\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0421 - val_loss: 0.2554\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0342 - val_loss: 0.3109\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0352 - val_loss: 0.2677\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0360 - val_loss: 0.2710\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0344 - val_loss: 0.2681\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0348 - val_loss: 0.2837\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0357 - val_loss: 0.2553\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0346 - val_loss: 0.2528\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0339 - val_loss: 0.2652\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0339 - val_loss: 0.2530\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0340 - val_loss: 0.3177\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0327 - val_loss: 0.2531\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0340 - val_loss: 0.2681\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0338 - val_loss: 0.3079\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0549 - val_loss: 0.2729\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0464 - val_loss: 0.2827\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0363 - val_loss: 0.2604\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0327 - val_loss: 0.2767\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0320 - val_loss: 0.2792\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0325 - val_loss: 0.2813\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0325 - val_loss: 0.2703\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0331 - val_loss: 0.2588\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0362 - val_loss: 0.2719\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0328 - val_loss: 0.2679\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.0312 - val_loss: 0.2540\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0304 - val_loss: 0.2562\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0295 - val_loss: 0.2783\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0290 - val_loss: 0.2584\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0301 - val_loss: 0.2625\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0303 - val_loss: 0.2553\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0308 - val_loss: 0.2629\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0301 - val_loss: 0.2775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0286 - val_loss: 0.2674\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0278 - val_loss: 0.2784\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0286 - val_loss: 0.2626\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0290 - val_loss: 0.2816\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0277 - val_loss: 0.2586\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0274 - val_loss: 0.2609\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0274 - val_loss: 0.2576\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0301 - val_loss: 0.2847\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0293 - val_loss: 0.2562\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0295 - val_loss: 0.2717\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0286 - val_loss: 0.3447\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0275 - val_loss: 0.2906\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0287 - val_loss: 0.2708\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0270 - val_loss: 0.2840\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0287 - val_loss: 0.2891\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0282 - val_loss: 0.2788\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.0276 - val_loss: 0.2894\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0276 - val_loss: 0.2821\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.0267 - val_loss: 0.2625\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0265 - val_loss: 0.2601\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0362 - val_loss: 0.2638\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0305 - val_loss: 0.2652\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.0308 - val_loss: 0.2698\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.0286 - val_loss: 0.2729\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0270 - val_loss: 0.2712\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0261 - val_loss: 0.2566\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0256 - val_loss: 0.2735\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0275 - val_loss: 0.2627\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0304 - val_loss: 0.2869\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0318 - val_loss: 0.2679\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0256 - val_loss: 0.2743\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0250 - val_loss: 0.2748\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0260 - val_loss: 0.2761\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0294 - val_loss: 0.2820\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0293 - val_loss: 0.2926\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0264 - val_loss: 0.2722\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0297 - val_loss: 0.2635\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0297 - val_loss: 0.2698\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0258 - val_loss: 0.2626\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0257 - val_loss: 0.2904\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0250 - val_loss: 0.2651\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0253 - val_loss: 0.2646\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0284 - val_loss: 0.2695\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.0244 - val_loss: 0.2642\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0264 - val_loss: 0.2994\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0258 - val_loss: 0.2764\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0245 - val_loss: 0.2644\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.0254 - val_loss: 0.2724\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0249 - val_loss: 0.2661\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0236 - val_loss: 0.2920\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.0238 - val_loss: 0.2749\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.0264 - val_loss: 0.2731\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0265 - val_loss: 0.2668\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0245 - val_loss: 0.3003\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0235 - val_loss: 0.2760\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0236 - val_loss: 0.3073\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0298 - val_loss: 0.2722\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0258 - val_loss: 0.3114\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.0248 - val_loss: 0.2749\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0237 - val_loss: 0.2740\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0232 - val_loss: 0.2863\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0255 - val_loss: 0.2785\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0245 - val_loss: 0.2737\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0243 - val_loss: 0.2739\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0244 - val_loss: 0.2828\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0241 - val_loss: 0.2747\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0230 - val_loss: 0.2697\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0232 - val_loss: 0.2877\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0231 - val_loss: 0.2692\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0231 - val_loss: 0.2907\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0228 - val_loss: 0.2982\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0243 - val_loss: 0.2784\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0234 - val_loss: 0.3011\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0230 - val_loss: 0.2915\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0219 - val_loss: 0.3004\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0221 - val_loss: 0.2704\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0222 - val_loss: 0.2828\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0258 - val_loss: 0.2756\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0254 - val_loss: 0.2842\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0234 - val_loss: 0.2875\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0234 - val_loss: 0.2777\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0224 - val_loss: 0.2838\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0219 - val_loss: 0.2884\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0211 - val_loss: 0.2768\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0220 - val_loss: 0.2839\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0245 - val_loss: 0.2817\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0210 - val_loss: 0.2890\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0216 - val_loss: 0.2885\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0230 - val_loss: 0.3069\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0309 - val_loss: 0.2835\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0375 - val_loss: 0.2925\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0268 - val_loss: 0.3029\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0408 - val_loss: 0.2904\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0259 - val_loss: 0.2799\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0313 - val_loss: 0.2963\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0231 - val_loss: 0.2943\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20eb5f0dc88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con ReLU')\n",
    "im.legend(('Val_Loss', 'Loss' ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##OJO con la LEARNING RATE\n",
    "Tuvimos que disminuirla en factor de diez por que con un learning rate de 0.01 era tan alto el error que obteniamos puros NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Repita b) y c) variando la tasa de aprendizaje (*learning rate*) en un rango sensible. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_lr = 20\n",
    "lear_rate = np.linspace(0,1,n_lr)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.1),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en este caso aumentamos la learn_rate y el algoritmo no converge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=1),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentamos mas el LR y sigue sin converger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: nan - val_loss: nan\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.09),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.05),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 6.8202 - val_loss: 2.3205\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.4548 - val_loss: 0.2701\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3061 - val_loss: 0.5303\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.2491 - val_loss: 0.3925\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.2195 - val_loss: 0.1536\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1610 - val_loss: 0.1788\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1580 - val_loss: 0.7088\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1643 - val_loss: 0.1420\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.1173 - val_loss: 0.1252\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1189 - val_loss: 0.1066\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1019 - val_loss: 0.1441\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0921 - val_loss: 0.0947\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0975 - val_loss: 0.2419\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0868 - val_loss: 0.1408\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0841 - val_loss: 0.0801\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0812 - val_loss: 0.0715\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0892 - val_loss: 0.4143\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0703 - val_loss: 0.0996\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0904 - val_loss: 0.0793\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0655 - val_loss: 0.0737\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0631 - val_loss: 0.0838\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0616 - val_loss: 0.0854\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0625 - val_loss: 0.0650\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0570 - val_loss: 0.0610\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0572 - val_loss: 0.1262\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0548 - val_loss: 0.0825\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0622 - val_loss: 0.0810\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0531 - val_loss: 0.0926\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0489 - val_loss: 0.0984\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0539 - val_loss: 0.0553\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0474 - val_loss: 0.1279\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0498 - val_loss: 0.0529\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0531 - val_loss: 0.0674\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0463 - val_loss: 0.0854\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0465 - val_loss: 0.0970\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0494 - val_loss: 0.0638\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0405 - val_loss: 0.0692\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0424 - val_loss: 0.0516\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0498 - val_loss: 0.0530\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0434 - val_loss: 0.2687\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0451 - val_loss: 0.0500\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0461 - val_loss: 0.0571\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0424 - val_loss: 0.0928\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0428 - val_loss: 0.0663\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0378 - val_loss: 0.0627\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0443 - val_loss: 0.0480\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0379 - val_loss: 0.0568\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0361 - val_loss: 0.0973\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0383 - val_loss: 0.0480\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0383 - val_loss: 0.0602\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0466 - val_loss: 0.0511\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0396 - val_loss: 0.0478\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0361 - val_loss: 0.0433\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0336 - val_loss: 0.0800\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0346 - val_loss: 0.0476\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0346 - val_loss: 0.0626\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0400 - val_loss: 0.1972\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0333 - val_loss: 0.0511\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0351 - val_loss: 0.0445\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0349 - val_loss: 0.0708\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0340 - val_loss: 0.0482\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0321 - val_loss: 0.0477\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0330 - val_loss: 0.0499\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0293 - val_loss: 0.0434\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0312 - val_loss: 0.0892\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0309 - val_loss: 0.0511\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0309 - val_loss: 0.0463\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0307 - val_loss: 0.0499\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0308 - val_loss: 0.0645\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0289 - val_loss: 0.0739\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0322 - val_loss: 0.0484\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0310 - val_loss: 0.0503\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0288 - val_loss: 0.0612\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0278 - val_loss: 0.0453\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0278 - val_loss: 0.0573\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0286 - val_loss: 0.0688\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0284 - val_loss: 0.0580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 284us/step - loss: 0.0268 - val_loss: 0.0441\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0242 - val_loss: 0.0476\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0290 - val_loss: 0.1147\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0257 - val_loss: 0.0512\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0267 - val_loss: 0.0464\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0273 - val_loss: 0.0837\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0260 - val_loss: 0.0445\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.0341 - val_loss: 0.1091\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0251 - val_loss: 0.0481\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0260 - val_loss: 0.0523\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0246 - val_loss: 0.0515\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0258 - val_loss: 0.0431\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0245 - val_loss: 0.0435\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0251 - val_loss: 0.0567\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.0247 - val_loss: 0.0949\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0241 - val_loss: 0.0454\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0241 - val_loss: 0.0564\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0230 - val_loss: 0.0535\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0269 - val_loss: 0.0678\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0239 - val_loss: 0.0462\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0229 - val_loss: 0.0585\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0250 - val_loss: 0.0490\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0250 - val_loss: 0.0545\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0243 - val_loss: 0.0469\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0243 - val_loss: 0.0416\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0221 - val_loss: 0.0406\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0225 - val_loss: 0.0677\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0234 - val_loss: 0.0432\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0214 - val_loss: 0.0459\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0234 - val_loss: 0.0526\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0216 - val_loss: 0.0455\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0246 - val_loss: 0.0420\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0223 - val_loss: 0.0461\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0222 - val_loss: 0.0474\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0224 - val_loss: 0.0497\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0207 - val_loss: 0.0441\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0231 - val_loss: 0.0400\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0206 - val_loss: 0.0863\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0215 - val_loss: 0.0395\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0214 - val_loss: 0.0394\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0204 - val_loss: 0.0792\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0229 - val_loss: 0.0401\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0217 - val_loss: 0.0434\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0211 - val_loss: 0.0435\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0211 - val_loss: 0.0397\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0211 - val_loss: 0.0411\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0203 - val_loss: 0.0451\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0204 - val_loss: 0.0391\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0194 - val_loss: 0.0437\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0208 - val_loss: 0.0489\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0208 - val_loss: 0.0412\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0225 - val_loss: 0.0416\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0200 - val_loss: 0.0391\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0198 - val_loss: 0.0394\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0218 - val_loss: 0.0394\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0210 - val_loss: 0.0507\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0199 - val_loss: 0.0586\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0189 - val_loss: 0.0438\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0176 - val_loss: 0.0514\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0195 - val_loss: 0.0391\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0196 - val_loss: 0.0410\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0205 - val_loss: 0.0477\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0180 - val_loss: 0.0425\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0183 - val_loss: 0.0431\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0198 - val_loss: 0.0449\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0203 - val_loss: 0.0508\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0187 - val_loss: 0.0401\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0207 - val_loss: 0.1906\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0200 - val_loss: 0.0386\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0207 - val_loss: 0.0550\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0192 - val_loss: 0.0455\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0181 - val_loss: 0.0389\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0185 - val_loss: 0.0381\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0187 - val_loss: 0.0417\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0223 - val_loss: 0.0414\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0203 - val_loss: 0.0417\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0182 - val_loss: 0.0493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0202 - val_loss: 0.0429\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0184 - val_loss: 0.0429\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0183 - val_loss: 0.0995\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0195 - val_loss: 0.0403\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0185 - val_loss: 0.0423\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0182 - val_loss: 0.1088\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0177 - val_loss: 0.0374\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0182 - val_loss: 0.0425\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0170 - val_loss: 0.0474\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0182 - val_loss: 0.0531\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0186 - val_loss: 0.0411\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0176 - val_loss: 0.0395\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.0169 - val_loss: 0.0393\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0178 - val_loss: 0.0573\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0195 - val_loss: 0.0379\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0168 - val_loss: 0.0467\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0191 - val_loss: 0.0457\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0178 - val_loss: 0.0406\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.0180 - val_loss: 0.0449\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0172 - val_loss: 0.0410\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 0.0173 - val_loss: 0.0419\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0169 - val_loss: 0.0536\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0181 - val_loss: 0.0427\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0161 - val_loss: 0.0501\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0158 - val_loss: 0.0418\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0181 - val_loss: 0.0403\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0168 - val_loss: 0.0365\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0163 - val_loss: 0.0365\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0160 - val_loss: 0.0452\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0167 - val_loss: 0.0385\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0165 - val_loss: 0.0371\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0170 - val_loss: 0.0382\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0170 - val_loss: 0.0403\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0162 - val_loss: 0.0380\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0190 - val_loss: 0.0595\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0171 - val_loss: 0.0446\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0177 - val_loss: 0.0409\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0158 - val_loss: 0.0536\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0177 - val_loss: 0.0527\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0194 - val_loss: 0.1028\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0158 - val_loss: 0.0378\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0161 - val_loss: 0.0418\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0166 - val_loss: 0.0482\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0190 - val_loss: 0.0378\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0161 - val_loss: 0.0488\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0149 - val_loss: 0.0373\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0164 - val_loss: 0.0526\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 0.0162 - val_loss: 0.0369\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0162 - val_loss: 0.0392\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.0154 - val_loss: 0.0588\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0162 - val_loss: 0.0365\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0153 - val_loss: 0.0377\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0142 - val_loss: 0.0360\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0151 - val_loss: 0.0425\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0157 - val_loss: 0.0486\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0154 - val_loss: 0.0408\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0147 - val_loss: 0.0380\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0151 - val_loss: 0.0408\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0151 - val_loss: 0.0499\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0163 - val_loss: 0.0574\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0151 - val_loss: 0.0993\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0159 - val_loss: 0.0483\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0163 - val_loss: 0.0357\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0166 - val_loss: 0.0381\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0148 - val_loss: 0.0395\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0140 - val_loss: 0.0379\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0149 - val_loss: 0.0404\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0150 - val_loss: 0.0674\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0152 - val_loss: 0.0372\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0154 - val_loss: 0.0422\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0147 - val_loss: 0.0356\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0161 - val_loss: 0.0528\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0147 - val_loss: 0.0549\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0144 - val_loss: 0.0470\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0146 - val_loss: 0.0456\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0147 - val_loss: 0.0471\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0160 - val_loss: 0.0359\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0146 - val_loss: 0.0449\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0141 - val_loss: 0.0493\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0158 - val_loss: 0.0367\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0149 - val_loss: 0.0428\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0138 - val_loss: 0.0373\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0147 - val_loss: 0.0578\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0148 - val_loss: 0.0366\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0146 - val_loss: 0.0354\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0135 - val_loss: 0.0421\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0148 - val_loss: 0.0372\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0137 - val_loss: 0.0362\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0141 - val_loss: 0.0370\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0160 - val_loss: 0.0798\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0154 - val_loss: 0.0395\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0139 - val_loss: 0.0384\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0136 - val_loss: 0.0392\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.0132 - val_loss: 0.0359\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0152 - val_loss: 0.0377\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0139 - val_loss: 0.0460\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.02),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20eb65b9198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Sigmoid Lr=0.02')\n",
    "im.legend(('Val_Loss', 'Loss' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 4s 390us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: nan - val_loss: nan\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: nan - val_loss: nan\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: nan - val_loss: nan\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: nan - val_loss: nan\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: nan - val_loss: nan\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: nan - val_loss: nan\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: nan - val_loss: nan\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: nan - val_loss: nan\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: nan - val_loss: nan\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: nan - val_loss: nan\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: nan - val_loss: nan\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: nan - val_loss: nan\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: nan - val_loss: nan\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: nan - val_loss: nan\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: nan - val_loss: nan\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: nan - val_loss: nan\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: nan - val_loss: nan\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: nan - val_loss: nan\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: nan - val_loss: nan\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: nan - val_loss: nan\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: nan - val_loss: nan\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: nan - val_loss: nan\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: nan - val_loss: nan\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 4s 402us/step - loss: nan - val_loss: nan\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: nan - val_loss: nan\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: nan - val_loss: nan\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: nan - val_loss: nan\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: nan - val_loss: nan\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: nan - val_loss: nan\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: nan - val_loss: nan\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: nan - val_loss: nan\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: nan - val_loss: nan\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: nan - val_loss: nan\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: nan - val_loss: nan\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: nan - val_loss: nan\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: nan - val_loss: nan\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: nan - val_loss: nan\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: nan - val_loss: nan\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: nan - val_loss: nan\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: nan - val_loss: nan\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: nan - val_loss: nan\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: nan - val_loss: nan\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: nan - val_loss: nan\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 3s 315us/step - loss: nan - val_loss: nan\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: nan - val_loss: nan\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: nan - val_loss: nan\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: nan - val_loss: nan\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: nan - val_loss: nan\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: nan - val_loss: nan\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: nan - val_loss: nan\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: nan - val_loss: nan\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: nan - val_loss: nan\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: nan - val_loss: nan\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: nan - val_loss: nan\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: nan - val_loss: nan\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: nan - val_loss: nan\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: nan - val_loss: nan\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: nan - val_loss: nan\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: nan - val_loss: nan\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: nan - val_loss: nan\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: nan - val_loss: nan\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: nan - val_loss: nan\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: nan - val_loss: nan\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: nan - val_loss: nan\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: nan - val_loss: nan\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: nan - val_loss: nan\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: nan - val_loss: nan\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: nan - val_loss: nan\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: nan - val_loss: nan\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: nan - val_loss: nan\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: nan - val_loss: nan\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: nan - val_loss: nan\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: nan - val_loss: nan\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: nan - val_loss: nan\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: nan - val_loss: nan\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: nan - val_loss: nan\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: nan - val_loss: nan\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.005),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20eb6b66c18>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con ReLU y Lr=0.05')\n",
    "im.legend(('Val_Loss', 'Loss' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: nan - val_loss: nan\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: nan - val_loss: nan\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: nan - val_loss: nan\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: nan - val_loss: nan\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: nan - val_loss: nan\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: nan - val_loss: nan\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: nan - val_loss: nan\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: nan - val_loss: nan\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: nan - val_loss: nan\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: nan - val_loss: nan\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: nan - val_loss: nan\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: nan - val_loss: nan\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: nan - val_loss: nan\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: nan - val_loss: nan\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: nan - val_loss: nan\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 240us/step - loss: nan - val_loss: nan\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: nan - val_loss: nan\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: nan - val_loss: nan\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: nan - val_loss: nan\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: nan - val_loss: nan\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: nan - val_loss: nan\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: nan - val_loss: nan\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: nan - val_loss: nan\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: nan - val_loss: nan\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: nan - val_loss: nan\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: nan - val_loss: nan\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: nan - val_loss: nan\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: nan - val_loss: nan\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: nan - val_loss: nan\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: nan - val_loss: nan\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: nan - val_loss: nan\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: nan - val_loss: nan\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: nan - val_loss: nan\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: nan - val_loss: nan\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: nan - val_loss: nan\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: nan - val_loss: nan\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: nan - val_loss: nan\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: nan - val_loss: nan\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: nan - val_loss: nan\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: nan - val_loss: nan\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: nan - val_loss: nan\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: nan - val_loss: nan\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: nan - val_loss: nan\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: nan - val_loss: nan\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: nan - val_loss: nan\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: nan - val_loss: nan\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: nan - val_loss: nan\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: nan - val_loss: nan\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: nan - val_loss: nan\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: nan - val_loss: nan\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: nan - val_loss: nan\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: nan - val_loss: nan\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: nan - val_loss: nan\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: nan - val_loss: nan\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: nan - val_loss: nan\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: nan - val_loss: nan\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: nan - val_loss: nan\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: nan - val_loss: nan\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: nan - val_loss: nan\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: nan - val_loss: nan\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: nan - val_loss: nan\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: nan - val_loss: nan\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: nan - val_loss: nan\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.003),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20eb708fc50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con ReLU Lr=0.03')\n",
    "im.legend(('Val_Loss', 'Loss' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 9.9870 - val_loss: 9.0922\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 3.8526 - val_loss: 3.7889\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 2.4372 - val_loss: 2.6962\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.8646 - val_loss: 0.9132\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.7583 - val_loss: 3.5121\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.5957 - val_loss: 0.8971\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.9082 - val_loss: 1.2090\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.4027 - val_loss: 0.6192\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.3239 - val_loss: 0.5774\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.0045 - val_loss: 1.1390\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.2146 - val_loss: 2.4694\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.4191 - val_loss: 0.6334\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.3514 - val_loss: 1.0683\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2561 - val_loss: 0.5145\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.2659 - val_loss: 0.5101\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.2237 - val_loss: 0.3914\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2861 - val_loss: 0.6359\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2246 - val_loss: 0.8115\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2960 - val_loss: 0.4063\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2053 - val_loss: 0.5054\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2909 - val_loss: 0.4926\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.3461 - val_loss: 0.4388\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1841 - val_loss: 0.4095\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.1522 - val_loss: 0.4239\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.1485 - val_loss: 0.4018\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1341 - val_loss: 0.6666\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - ETA: 0s - loss: 0.140 - 2s 179us/step - loss: 0.1403 - val_loss: 0.3926\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1258 - val_loss: 0.5263\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1540 - val_loss: 0.3822\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.1296 - val_loss: 0.3496\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.1217 - val_loss: 0.3932\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1062 - val_loss: 0.3801\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1210 - val_loss: 0.3161\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1080 - val_loss: 0.3919\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1711 - val_loss: 1.5157\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1927 - val_loss: 0.3776\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1508 - val_loss: 0.3347\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1205 - val_loss: 0.4226\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0958 - val_loss: 0.2986\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0987 - val_loss: 0.3109\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0908 - val_loss: 0.2991\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0843 - val_loss: 0.3533\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1056 - val_loss: 0.4154\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1002 - val_loss: 0.2796\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0978 - val_loss: 0.2723\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0865 - val_loss: 0.5652\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0819 - val_loss: 0.3844\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0868 - val_loss: 0.4062\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0781 - val_loss: 0.3778\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1964 - val_loss: 0.5595\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1302 - val_loss: 0.3076\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0963 - val_loss: 0.2866\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0844 - val_loss: 0.3089\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1092 - val_loss: 0.3067\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0963 - val_loss: 0.2695\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1043 - val_loss: 0.3389\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.1162 - val_loss: 0.2740\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0725 - val_loss: 0.2752\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0625 - val_loss: 0.2776\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0651 - val_loss: 0.3574\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0804 - val_loss: 0.3851\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0835 - val_loss: 0.3591\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0641 - val_loss: 0.2715\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0729 - val_loss: 0.3208\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0953 - val_loss: 0.2868\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0863 - val_loss: 0.3165\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0848 - val_loss: 0.2731\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0669 - val_loss: 0.2495\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0903 - val_loss: 2.7277\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0865 - val_loss: 0.3518\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0632 - val_loss: 0.2585\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0566 - val_loss: 0.2739\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0558 - val_loss: 0.3141\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0651 - val_loss: 0.2663\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0604 - val_loss: 0.6667\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0607 - val_loss: 0.3874\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0604 - val_loss: 0.2632\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0646 - val_loss: 0.4041\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0607 - val_loss: 0.3197\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1222 - val_loss: 0.3612\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0656 - val_loss: 0.2457\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0835 - val_loss: 0.4055\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0683 - val_loss: 0.2587\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0474 - val_loss: 0.2619\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0492 - val_loss: 0.2922\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0687 - val_loss: 0.2542\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0493 - val_loss: 0.2512\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0560 - val_loss: 0.3622\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0539 - val_loss: 0.2499\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0570 - val_loss: 0.2750\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0522 - val_loss: 0.2597\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0475 - val_loss: 0.2594\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0490 - val_loss: 0.2787\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0512 - val_loss: 0.2415\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0467 - val_loss: 0.2600\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0467 - val_loss: 0.2729\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0459 - val_loss: 0.3125\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0465 - val_loss: 0.2559\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0472 - val_loss: 0.2862\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0657 - val_loss: 0.5213\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0931 - val_loss: 0.2376\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0483 - val_loss: 0.2785\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0544 - val_loss: 0.4114\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0453 - val_loss: 0.2732\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0486 - val_loss: 0.2340\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0535 - val_loss: 0.2985\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0523 - val_loss: 0.2712\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0518 - val_loss: 0.4967\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0521 - val_loss: 0.3079\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0486 - val_loss: 0.2888\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0414 - val_loss: 0.2568\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0424 - val_loss: 0.4243\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0416 - val_loss: 0.3210\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0491 - val_loss: 0.2821\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0412 - val_loss: 0.2747\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0492 - val_loss: 0.2552\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0463 - val_loss: 0.2690\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0428 - val_loss: 0.2504\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0439 - val_loss: 0.2629\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0418 - val_loss: 0.3485\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0481 - val_loss: 0.2650\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0426 - val_loss: 0.2588\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0437 - val_loss: 0.2410\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0423 - val_loss: 0.3048\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0394 - val_loss: 0.2513\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0414 - val_loss: 0.2613\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0414 - val_loss: 0.3149\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0471 - val_loss: 0.2689\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0357 - val_loss: 0.2881\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0372 - val_loss: 0.2576\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0381 - val_loss: 0.2574\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0390 - val_loss: 0.3017\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0389 - val_loss: 0.2414\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0321 - val_loss: 0.2765\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0344 - val_loss: 0.2527\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0359 - val_loss: 0.2469\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0376 - val_loss: 0.2588\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0358 - val_loss: 0.2598\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0356 - val_loss: 0.2820\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0396 - val_loss: 0.2654\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0356 - val_loss: 0.2856\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0366 - val_loss: 0.2628\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0329 - val_loss: 0.2952\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0381 - val_loss: 0.3075\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0384 - val_loss: 0.2589\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0386 - val_loss: 0.2492\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0415 - val_loss: 0.2777\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0399 - val_loss: 0.2662\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0356 - val_loss: 0.2609\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0436 - val_loss: 0.2598\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0328 - val_loss: 0.2686\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0333 - val_loss: 0.2623\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0328 - val_loss: 0.2583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0341 - val_loss: 0.4034\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0336 - val_loss: 0.2916\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0345 - val_loss: 0.2599\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0407 - val_loss: 0.2462\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0362 - val_loss: 0.3228\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0371 - val_loss: 0.2659\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0366 - val_loss: 0.2696\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0437 - val_loss: 0.2652\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0321 - val_loss: 0.2590\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0315 - val_loss: 0.2952\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0299 - val_loss: 0.2523\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0339 - val_loss: 0.2815\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0327 - val_loss: 0.2846\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0328 - val_loss: 0.3105\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0514 - val_loss: 0.4226\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0392 - val_loss: 0.2441\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - ETA: 0s - loss: 0.033 - 2s 169us/step - loss: 0.0339 - val_loss: 0.3301\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0295 - val_loss: 0.2693\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0285 - val_loss: 0.2707\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0294 - val_loss: 0.2538\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0331 - val_loss: 0.2760\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0331 - val_loss: 0.2562\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0472 - val_loss: 0.2491\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0354 - val_loss: 0.2488\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0345 - val_loss: 0.3145\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0447 - val_loss: 0.2607\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0334 - val_loss: 0.2648\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0326 - val_loss: 0.2655\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0264 - val_loss: 0.2882\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0339 - val_loss: 0.2740\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0307 - val_loss: 0.2719\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0348 - val_loss: 0.2754\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0330 - val_loss: 0.2603\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0331 - val_loss: 0.2857\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0283 - val_loss: 0.2722\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0303 - val_loss: 0.2540\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0362 - val_loss: 0.2627\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0327 - val_loss: 0.2639\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0334 - val_loss: 0.2622\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0287 - val_loss: 0.2523\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0298 - val_loss: 0.2757\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0253 - val_loss: 0.2917\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0267 - val_loss: 0.2815\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0302 - val_loss: 0.2888\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0284 - val_loss: 0.2677\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0293 - val_loss: 0.2652\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0254 - val_loss: 0.2800\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0281 - val_loss: 0.3984\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0284 - val_loss: 0.2690\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0281 - val_loss: 0.2812\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0308 - val_loss: 0.3007\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0288 - val_loss: 0.2621\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0329 - val_loss: 0.3388\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0293 - val_loss: 0.2977\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0295 - val_loss: 0.3113\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0337 - val_loss: 0.2563\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0365 - val_loss: 0.2575\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0263 - val_loss: 0.2558\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0267 - val_loss: 0.2713\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0302 - val_loss: 0.2987\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0258 - val_loss: 0.2762\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0403 - val_loss: 0.2919\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0315 - val_loss: 0.2956\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0326 - val_loss: 0.2677\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0259 - val_loss: 0.2885\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0293 - val_loss: 0.2882\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0375 - val_loss: 0.2662\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0377 - val_loss: 0.2779\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0378 - val_loss: 0.2746\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0375 - val_loss: 0.2823\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0289 - val_loss: 0.2875\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0322 - val_loss: 0.2903\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0279 - val_loss: 0.2725\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0293 - val_loss: 0.3437\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0263 - val_loss: 0.2801\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0334 - val_loss: 0.3413\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0290 - val_loss: 0.3821\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0314 - val_loss: 0.2729\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0251 - val_loss: 0.2754\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0243 - val_loss: 0.2778\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0275 - val_loss: 0.2616\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0336 - val_loss: 0.2770\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0268 - val_loss: 0.2839\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0344 - val_loss: 0.2943\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0248 - val_loss: 0.2747\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0266 - val_loss: 0.2762\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0314 - val_loss: 0.2843\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0260 - val_loss: 0.2807\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0251 - val_loss: 0.2839\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0291 - val_loss: 0.2885\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0266 - val_loss: 0.2795\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0293 - val_loss: 0.2842\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0285 - val_loss: 0.3100\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0229 - val_loss: 0.3301\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0259 - val_loss: 0.2920\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0277 - val_loss: 0.3135\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0299 - val_loss: 0.3234\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.002),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20eb7251860>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con ReLU Lr=0.02')\n",
    "im.legend(('Val_Loss', 'Loss' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Entrene los modelos considerados en b) y c) usando *progressive decay*. Compare y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 1.5455 - val_loss: 0.5402\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.5970 - val_loss: 0.5171\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.5115 - val_loss: 0.4245\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.4323 - val_loss: 0.3179\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3821 - val_loss: 0.2891\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.3275 - val_loss: 0.2630\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.2864 - val_loss: 0.3928\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.2590 - val_loss: 0.2222\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.2275 - val_loss: 0.2091\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.2058 - val_loss: 0.3163\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1832 - val_loss: 0.2084\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1650 - val_loss: 0.1876\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1515 - val_loss: 0.3790\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.1377 - val_loss: 0.1606\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1284 - val_loss: 0.1350\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1159 - val_loss: 0.1846\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1105 - val_loss: 0.1156\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1005 - val_loss: 0.1198\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0982 - val_loss: 0.1193\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0902 - val_loss: 0.1058\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0872 - val_loss: 0.0962\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0816 - val_loss: 0.0869\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0780 - val_loss: 0.1683\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0745 - val_loss: 0.0841\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0720 - val_loss: 0.0893\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0683 - val_loss: 0.0816\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0668 - val_loss: 0.0859\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0633 - val_loss: 0.0851\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0622 - val_loss: 0.0754\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0598 - val_loss: 0.1225\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0603 - val_loss: 0.0733\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0577 - val_loss: 0.1054\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0592 - val_loss: 0.0717\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0545 - val_loss: 0.0670\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0526 - val_loss: 0.0835\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0547 - val_loss: 0.0830\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0513 - val_loss: 0.0843\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0510 - val_loss: 0.0642\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0490 - val_loss: 0.0629\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0481 - val_loss: 0.1213\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0472 - val_loss: 0.0683\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0479 - val_loss: 0.0741ETA: 0s\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0468 - val_loss: 0.0630\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0454 - val_loss: 0.0705\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0448 - val_loss: 0.0732\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0436 - val_loss: 0.0599\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0428 - val_loss: 0.0849\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0425 - val_loss: 0.0588\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0441 - val_loss: 0.0695\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0406 - val_loss: 0.0586\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0407 - val_loss: 0.0622\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0404 - val_loss: 0.0583\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0392 - val_loss: 0.0577\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0396 - val_loss: 0.0558\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0389 - val_loss: 0.1044\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0385 - val_loss: 0.0566\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0377 - val_loss: 0.0576\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0374 - val_loss: 0.0620\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0362 - val_loss: 0.0542\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0359 - val_loss: 0.0629\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0348 - val_loss: 0.0650\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0362 - val_loss: 0.0540\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0356 - val_loss: 0.0525\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0353 - val_loss: 0.0528\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0342 - val_loss: 0.0593\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0341 - val_loss: 0.0516\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0336 - val_loss: 0.0561\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0326 - val_loss: 0.0536\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0334 - val_loss: 0.1185\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0332 - val_loss: 0.0595\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0326 - val_loss: 0.0514\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0326 - val_loss: 0.0544\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0329 - val_loss: 0.0515\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0327 - val_loss: 0.0631\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0320 - val_loss: 0.0658\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0317 - val_loss: 0.0689\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0320 - val_loss: 0.0520\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0309 - val_loss: 0.0657\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0300 - val_loss: 0.0501\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0300 - val_loss: 0.0511\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0307 - val_loss: 0.0566\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0292 - val_loss: 0.0540\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0291 - val_loss: 0.0661\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0289 - val_loss: 0.0530\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0300 - val_loss: 0.0682\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0283 - val_loss: 0.0498\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0296 - val_loss: 0.0478\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0285 - val_loss: 0.0531\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0280 - val_loss: 0.0495\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0282 - val_loss: 0.0520\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0278 - val_loss: 0.0470\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0269 - val_loss: 0.0472\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0278 - val_loss: 0.0527\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0274 - val_loss: 0.0468\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0276 - val_loss: 0.0455\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.0268 - val_loss: 0.0461\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.0272 - val_loss: 0.0476oss: - ETA: 0s - loss\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.0266 - val_loss: 0.0579\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 284us/step - loss: 0.0269 - val_loss: 0.0826\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0265 - val_loss: 0.0469\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.0262 - val_loss: 0.0462\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0259 - val_loss: 0.0458\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0266 - val_loss: 0.0563\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0250 - val_loss: 0.0598\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.0263 - val_loss: 0.0482\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 4s 377us/step - loss: 0.0258 - val_loss: 0.0593\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.0261 - val_loss: 0.0539\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0265 - val_loss: 0.0503\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0256 - val_loss: 0.0498\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0251 - val_loss: 0.0429ETA: 0s\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0255 - val_loss: 0.0494\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0242 - val_loss: 0.0551\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0237 - val_loss: 0.0811\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0248 - val_loss: 0.0438\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0245 - val_loss: 0.0459\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0236 - val_loss: 0.0445\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0242 - val_loss: 0.0543\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0242 - val_loss: 0.0701\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0235 - val_loss: 0.0440\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0243 - val_loss: 0.0484\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0231 - val_loss: 0.0455\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0236 - val_loss: 0.0473\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0242 - val_loss: 0.0436\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0237 - val_loss: 0.0432\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0231 - val_loss: 0.0468\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0234 - val_loss: 0.0470\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0229 - val_loss: 0.0476\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0226 - val_loss: 0.0439\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0232 - val_loss: 0.0412\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0226 - val_loss: 0.0524\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0228 - val_loss: 0.0658\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0234 - val_loss: 0.0488\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0223 - val_loss: 0.0443\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0215 - val_loss: 0.0471\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0218 - val_loss: 0.0429\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.0221 - val_loss: 0.0415\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0217 - val_loss: 0.0438\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0212 - val_loss: 0.0485\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0209 - val_loss: 0.0574\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0211 - val_loss: 0.0475\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0222 - val_loss: 0.0472\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0212 - val_loss: 0.0454\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0216 - val_loss: 0.0428\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0218 - val_loss: 0.0481\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0205 - val_loss: 0.0521\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0212 - val_loss: 0.0462\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0214 - val_loss: 0.0492\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0205 - val_loss: 0.0475\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 0.0203 - val_loss: 0.0480\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0206 - val_loss: 0.0444\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0208 - val_loss: 0.0486\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0201 - val_loss: 0.0430\n",
      "Epoch 153/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0201 - val_loss: 0.0441\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0205 - val_loss: 0.0451\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0199 - val_loss: 0.0448\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0203 - val_loss: 0.0489\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0200 - val_loss: 0.0427\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0196 - val_loss: 0.0418\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0203 - val_loss: 0.0398\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0202 - val_loss: 0.0415\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0197 - val_loss: 0.0391\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0199 - val_loss: 0.0403\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0198 - val_loss: 0.0428\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0195 - val_loss: 0.0402\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0188 - val_loss: 0.0593\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0196 - val_loss: 0.0404\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0206 - val_loss: 0.0411\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0193 - val_loss: 0.0412\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0185 - val_loss: 0.0409\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0193 - val_loss: 0.0412\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0191 - val_loss: 0.0408\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0192 - val_loss: 0.0432\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0193 - val_loss: 0.0559\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0196 - val_loss: 0.0513\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0194 - val_loss: 0.041319\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0187 - val_loss: 0.0394\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0186 - val_loss: 0.0386\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0186 - val_loss: 0.0432\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0183 - val_loss: 0.0401\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0185 - val_loss: 0.0408\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0185 - val_loss: 0.0436\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0185 - val_loss: 0.0423\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0187 - val_loss: 0.0392\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0179 - val_loss: 0.0383\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0185 - val_loss: 0.0397\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0181 - val_loss: 0.0411\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0180 - val_loss: 0.0389\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0180 - val_loss: 0.0383\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0178 - val_loss: 0.0405\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0181 - val_loss: 0.0448\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0178 - val_loss: 0.0379\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0182 - val_loss: 0.0469\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0173 - val_loss: 0.0405\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0173 - val_loss: 0.0768\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0180 - val_loss: 0.0445\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0171 - val_loss: 0.0393\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0175 - val_loss: 0.0430\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0176 - val_loss: 0.0388\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0167 - val_loss: 0.0399\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0171 - val_loss: 0.0382\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0166 - val_loss: 0.0414\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0170 - val_loss: 0.0383\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0167 - val_loss: 0.0466\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0168 - val_loss: 0.0383\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0165 - val_loss: 0.0516\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0167 - val_loss: 0.0388\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0164 - val_loss: 0.0389\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0178 - val_loss: 0.0402\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0173 - val_loss: 0.0398\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0164 - val_loss: 0.0381\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0171 - val_loss: 0.0479\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0170 - val_loss: 0.0469\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0162 - val_loss: 0.0376\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0166 - val_loss: 0.0478\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0162 - val_loss: 0.0404\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0164 - val_loss: 0.0379\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0162 - val_loss: 0.0440\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0166 - val_loss: 0.0370\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0162 - val_loss: 0.0380\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0164 - val_loss: 0.0407\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0163 - val_loss: 0.0400\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0169 - val_loss: 0.0375 ETA: 0s - loss: 0.0\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0159 - val_loss: 0.0400\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0163 - val_loss: 0.0411\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0158 - val_loss: 0.0452\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0163 - val_loss: 0.0372\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0163 - val_loss: 0.0371\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0161 - val_loss: 0.0397\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0158 - val_loss: 0.0424\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0153 - val_loss: 0.0436\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0155 - val_loss: 0.0410\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0157 - val_loss: 0.0393\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0157 - val_loss: 0.0369\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0162 - val_loss: 0.0389\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0154 - val_loss: 0.0555\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0159 - val_loss: 0.0487A: 0s - loss\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0156 - val_loss: 0.0396\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0158 - val_loss: 0.0436\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0158 - val_loss: 0.0387\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0157 - val_loss: 0.0381\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0157 - val_loss: 0.0392\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0159 - val_loss: 0.0396\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0148 - val_loss: 0.0368\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0153 - val_loss: 0.0364\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0148 - val_loss: 0.0379\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0157 - val_loss: 0.0366\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0154 - val_loss: 0.0542\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0153 - val_loss: 0.0372\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0155 - val_loss: 0.0408\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0153 - val_loss: 0.0437\n"
     ]
    }
   ],
   "source": [
    "n_decay = 10\n",
    "lear_decay = np.logspace(-6,0,n_decay)\n",
    "sgd = SGD(lr=0.2, decay=1e-6)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20eb7a5aa58>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Sigmoid and Decay')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 8.6099 - val_loss: 2.8148\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.4197 - val_loss: 3.6697\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.9554 - val_loss: 1.7309\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.6741 - val_loss: 1.6318\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.5668 - val_loss: 1.3138\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.5110 - val_loss: 1.3167\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.5312 - val_loss: 1.2140\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3767 - val_loss: 1.0507\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.3044 - val_loss: 0.9711\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.3088 - val_loss: 0.9241\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2459 - val_loss: 0.8392\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2312 - val_loss: 0.8586\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2456 - val_loss: 0.8839\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2686 - val_loss: 0.7983\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2213 - val_loss: 0.7833\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1778 - val_loss: 0.7309\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.1809 - val_loss: 0.7002\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1519 - val_loss: 0.6978\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.1417 - val_loss: 0.6841\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1564 - val_loss: 0.6522\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.1332 - val_loss: 0.6485\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1452 - val_loss: 0.6401\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1340 - val_loss: 0.6366\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1227 - val_loss: 0.6542\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1175 - val_loss: 0.6526\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1127 - val_loss: 0.6088\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1083 - val_loss: 0.5899\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1090 - val_loss: 0.6034\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.1092 - val_loss: 0.5679\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1048 - val_loss: 0.6472\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1050 - val_loss: 0.5794\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0977 - val_loss: 0.5981\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0999 - val_loss: 0.5345\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0939 - val_loss: 0.5377\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0950 - val_loss: 0.5932\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0925 - val_loss: 0.5196\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0987 - val_loss: 0.5038\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0901 - val_loss: 0.5268\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0848 - val_loss: 0.6216\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0828 - val_loss: 0.5193\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0959 - val_loss: 0.5205\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0793 - val_loss: 0.5116\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0778 - val_loss: 0.9088\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0799 - val_loss: 0.5336\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1310 - val_loss: 0.6722\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1577 - val_loss: 0.5226\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0929 - val_loss: 0.5207\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0767 - val_loss: 0.4876\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0747 - val_loss: 0.4980\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0695 - val_loss: 0.4906\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0664 - val_loss: 0.4891\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0653 - val_loss: 0.5185\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0697 - val_loss: 0.5533\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0635 - val_loss: 0.4745\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0634 - val_loss: 0.4897\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0632 - val_loss: 0.4600\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0613 - val_loss: 0.4482\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0649 - val_loss: 0.4545\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0605 - val_loss: 0.5650\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0576 - val_loss: 0.4451\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0560 - val_loss: 0.4500\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0617 - val_loss: 0.4663\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0582 - val_loss: 0.4829\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0581 - val_loss: 0.4564\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0573 - val_loss: 0.4301\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0546 - val_loss: 0.4656\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0584 - val_loss: 0.4581\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0609 - val_loss: 0.4398\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0543 - val_loss: 0.4633\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0508 - val_loss: 0.4413\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0530 - val_loss: 0.4239\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0501 - val_loss: 0.4769\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0531 - val_loss: 0.4617\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0534 - val_loss: 0.4887\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0475 - val_loss: 0.4325\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0484 - val_loss: 0.4237\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0465 - val_loss: 0.4317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0485 - val_loss: 0.4194\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0593 - val_loss: 0.4402\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0537 - val_loss: 0.4277\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0484 - val_loss: 0.4798\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0473 - val_loss: 0.4182\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0455 - val_loss: 0.4216\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0454 - val_loss: 0.4692\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0484 - val_loss: 0.4268\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0469 - val_loss: 0.4329\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0440 - val_loss: 0.5337\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0428 - val_loss: 0.4457\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0460 - val_loss: 0.4072\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0431 - val_loss: 0.4125\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0438 - val_loss: 0.4427\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0425 - val_loss: 0.4181\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0425 - val_loss: 0.4475\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0393 - val_loss: 0.4072\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0404 - val_loss: 0.4722\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0416 - val_loss: 0.4212\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0398 - val_loss: 0.4139\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0388 - val_loss: 0.4420\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0387 - val_loss: 0.4267\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0409 - val_loss: 0.3997\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0408 - val_loss: 0.4125\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0401 - val_loss: 0.4235\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0443 - val_loss: 0.4482\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0453 - val_loss: 0.4319\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0411 - val_loss: 0.4530\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0381 - val_loss: 0.4108\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0384 - val_loss: 0.4396\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0383 - val_loss: 0.4572\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0376 - val_loss: 0.4101\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0398 - val_loss: 0.4384\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0389 - val_loss: 0.4037\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0394 - val_loss: 0.5612\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0360 - val_loss: 0.4117\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0363 - val_loss: 0.4076\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0360 - val_loss: 0.3963\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - ETA: 0s - loss: 0.0358- - 2s 182us/step - loss: 0.0356 - val_loss: 0.4221\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0339 - val_loss: 0.4283\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0353 - val_loss: 0.4005\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0339 - val_loss: 0.4154\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0340 - val_loss: 0.4045\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0357 - val_loss: 0.4247\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0340 - val_loss: 0.4120\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0333 - val_loss: 0.4335\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0371 - val_loss: 0.4481\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0337 - val_loss: 0.4674\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0391 - val_loss: 0.4347\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0418 - val_loss: 0.4252\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0350 - val_loss: 0.4189\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0347 - val_loss: 0.4160\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0335 - val_loss: 0.4268\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0348 - val_loss: 0.4088\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0348 - val_loss: 0.4164\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0346 - val_loss: 0.4403\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0345 - val_loss: 0.4175\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0344 - val_loss: 0.4293\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0349 - val_loss: 0.4427\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0322 - val_loss: 0.4264\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0317 - val_loss: 0.4256\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0300 - val_loss: 0.4643\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0317 - val_loss: 0.4204\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0299 - val_loss: 0.4339\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0323 - val_loss: 0.4377\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0309 - val_loss: 0.4360\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0292 - val_loss: 0.4417\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0291 - val_loss: 0.4331\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0308 - val_loss: 0.4238\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0302 - val_loss: 0.4429\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0288 - val_loss: 0.4450\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0307 - val_loss: 0.4437\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0343 - val_loss: 0.4368\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0347 - val_loss: 0.4346\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0290 - val_loss: 0.4684\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0284 - val_loss: 0.4894\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0298 - val_loss: 0.4419\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0301 - val_loss: 0.4709\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0292 - val_loss: 0.4630\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0303 - val_loss: 0.4543\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0302 - val_loss: 0.4378\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0281 - val_loss: 0.4377\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0307 - val_loss: 0.4387\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0289 - val_loss: 0.4438\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0280 - val_loss: 0.4601\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0275 - val_loss: 0.4402\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0283 - val_loss: 0.4450\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0285 - val_loss: 0.4502\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0287 - val_loss: 0.4469\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0273 - val_loss: 0.4409\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0276 - val_loss: 0.4352\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0262 - val_loss: 0.4691\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0271 - val_loss: 0.4416\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0278 - val_loss: 0.4680\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0283 - val_loss: 0.4561\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0285 - val_loss: 0.4657\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0306 - val_loss: 0.4530\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0303 - val_loss: 0.4710\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0263 - val_loss: 0.4794\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0355 - val_loss: 0.4743\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0582 - val_loss: 0.4974\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0463 - val_loss: 0.4892\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0275 - val_loss: 0.4815\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0291 - val_loss: 0.4780\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0299 - val_loss: 0.4820\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0275 - val_loss: 0.4694\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0268 - val_loss: 0.4683\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0263 - val_loss: 0.4932\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0254 - val_loss: 0.4852\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0265 - val_loss: 0.4667\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0244 - val_loss: 0.4884\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0253 - val_loss: 0.4634\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0290 - val_loss: 0.4727\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0268 - val_loss: 0.5075\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0264 - val_loss: 0.4858\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0257 - val_loss: 0.4661\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0246 - val_loss: 0.4938\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0270 - val_loss: 0.4774\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0293 - val_loss: 0.4881\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0252 - val_loss: 0.4769\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0257 - val_loss: 0.5557\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0246 - val_loss: 0.4714\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0248 - val_loss: 0.4980\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0276 - val_loss: 0.4848\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0266 - val_loss: 0.4764\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0246 - val_loss: 0.4744\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0241 - val_loss: 0.6427\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0246 - val_loss: 0.4920\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0231 - val_loss: 0.5062\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0239 - val_loss: 0.5026\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0237 - val_loss: 0.4711\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0234 - val_loss: 0.4841\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0237 - val_loss: 0.4816\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0227 - val_loss: 0.4886\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0263 - val_loss: 0.4806\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0233 - val_loss: 0.4949\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0258 - val_loss: 0.4838\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0278 - val_loss: 0.4978\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0254 - val_loss: 0.5243\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0226 - val_loss: 0.4948\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0235 - val_loss: 0.4899\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0236 - val_loss: 0.4834\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0218 - val_loss: 0.4866\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0229 - val_loss: 0.5170\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0225 - val_loss: 0.4997\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0227 - val_loss: 0.5012\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0241 - val_loss: 0.5071\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0234 - val_loss: 0.5818\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0209 - val_loss: 0.4831\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0246 - val_loss: 0.4780\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0248 - val_loss: 0.5036\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0225 - val_loss: 0.4926\n",
      "Epoch 230/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0214 - val_loss: 0.4736\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0230 - val_loss: 0.4887\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0227 - val_loss: 0.5132\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0234 - val_loss: 0.5021\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0225 - val_loss: 0.4832\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0222 - val_loss: 0.5129\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0224 - val_loss: 0.4958\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0218 - val_loss: 0.5151\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0219 - val_loss: 0.4915\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0211 - val_loss: 0.5127\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0212 - val_loss: 0.5106\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0222 - val_loss: 0.4997\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0216 - val_loss: 0.4912\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0231 - val_loss: 0.5217\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0217 - val_loss: 0.5152\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0212 - val_loss: 0.4969\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0210 - val_loss: 0.4959\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0212 - val_loss: 0.5292\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0229 - val_loss: 0.5207\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0203 - val_loss: 0.4997\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0206 - val_loss: 0.5000\n"
     ]
    }
   ],
   "source": [
    "n_decay = 10\n",
    "lear_decay = np.logspace(-6,0,n_decay)\n",
    "sgd = SGD(lr=0.2, decay=1e-6)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed03f4da0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Decay')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Entrene los modelos considerados en b) y c) utilizando SGD en mini-*batches*. Experimente con diferentes tamaños del *batch*. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0188 - val_loss: 0.5019\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0182 - val_loss: 0.5170\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0184 - val_loss: 0.5011\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0185 - val_loss: 0.5025\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0183 - val_loss: 0.5076\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0184 - val_loss: 0.5084\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0180 - val_loss: 0.5084\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0189 - val_loss: 0.5033\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0181 - val_loss: 0.5157\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0180 - val_loss: 0.5064\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0181 - val_loss: 0.5078\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0180 - val_loss: 0.5178\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0181 - val_loss: 0.5013\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0179 - val_loss: 0.5062\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0181 - val_loss: 0.5311\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0180 - val_loss: 0.5075\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0177 - val_loss: 0.5013\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0191 - val_loss: 0.5215\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0183 - val_loss: 0.5207\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0181 - val_loss: 0.5067\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0182 - val_loss: 0.5268\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0186 - val_loss: 0.5092\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0178 - val_loss: 0.5220\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0181 - val_loss: 0.5099\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0181 - val_loss: 0.5074\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0177 - val_loss: 0.5231\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0180 - val_loss: 0.5299\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0178 - val_loss: 0.5239\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0176 - val_loss: 0.5178\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0180 - val_loss: 0.5336\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0180 - val_loss: 0.5160\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0175 - val_loss: 0.5229\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0171 - val_loss: 0.5357\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0173 - val_loss: 0.5182\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0178 - val_loss: 0.5125\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0178 - val_loss: 0.5185\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0175 - val_loss: 0.5246\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0170 - val_loss: 0.5149\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0178 - val_loss: 0.5138\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0173 - val_loss: 0.5151\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0173 - val_loss: 0.5274\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0174 - val_loss: 0.5150\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0177 - val_loss: 0.5196\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0172 - val_loss: 0.5231\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0176 - val_loss: 0.5319\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0170 - val_loss: 0.5355\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0172 - val_loss: 0.5369\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 1s 138us/step - loss: 0.0177 - val_loss: 0.5193\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 1s 138us/step - loss: 0.0175 - val_loss: 0.5197\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0172 - val_loss: 0.5427\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0178 - val_loss: 0.5136\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0168 - val_loss: 0.5446\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 1s 138us/step - loss: 0.0168 - val_loss: 0.5210\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 1s 138us/step - loss: 0.0167 - val_loss: 0.5270\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0168 - val_loss: 0.5416\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0171 - val_loss: 0.5198\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0169 - val_loss: 0.5406\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0169 - val_loss: 0.5223\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0169 - val_loss: 0.5147\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0170 - val_loss: 0.5271\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0170 - val_loss: 0.5288\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0169 - val_loss: 0.5228\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 1s 138us/step - loss: 0.0169 - val_loss: 0.5111\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 1s 138us/step - loss: 0.0166 - val_loss: 0.5247\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0165 - val_loss: 0.5212\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0168 - val_loss: 0.5297\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0174 - val_loss: 0.5290\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0168 - val_loss: 0.5292\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0168 - val_loss: 0.5457\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0169 - val_loss: 0.5269\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0166 - val_loss: 0.5536\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0163 - val_loss: 0.5307\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0169 - val_loss: 0.5295\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0171 - val_loss: 0.5400\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0172 - val_loss: 0.5358\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0166 - val_loss: 0.5344\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0170 - val_loss: 0.5304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0161 - val_loss: 0.5217\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0167 - val_loss: 0.5316\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0160 - val_loss: 0.5319\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0169 - val_loss: 0.5158\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0166 - val_loss: 0.5372\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0163 - val_loss: 0.5213\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0160 - val_loss: 0.5453\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0168 - val_loss: 0.5358\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0165 - val_loss: 0.5477\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0164 - val_loss: 0.5456\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0165 - val_loss: 0.5297\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0168 - val_loss: 0.5423\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0162 - val_loss: 0.5464\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0165 - val_loss: 0.5284\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0163 - val_loss: 0.5539\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0166 - val_loss: 0.5339\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0161 - val_loss: 0.5460\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0164 - val_loss: 0.5585\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0162 - val_loss: 0.5331\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0162 - val_loss: 0.5338\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0157 - val_loss: 0.5463\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0161 - val_loss: 0.5487\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0161 - val_loss: 0.5402\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0160 - val_loss: 0.5551\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0164 - val_loss: 0.5273\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0159 - val_loss: 0.5457\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0167 - val_loss: 0.5472\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0160 - val_loss: 0.5427\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0156 - val_loss: 0.5341\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0160 - val_loss: 0.5456\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0158 - val_loss: 0.5484\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0161 - val_loss: 0.5583\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0160 - val_loss: 0.5553\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0162 - val_loss: 0.5538\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0154 - val_loss: 0.5367\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0155 - val_loss: 0.5333\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0158 - val_loss: 0.5629\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0158 - val_loss: 0.5407\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0153 - val_loss: 0.5487\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0155 - val_loss: 0.5344\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0159 - val_loss: 0.5451\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0156 - val_loss: 0.5458\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0158 - val_loss: 0.5371\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0155 - val_loss: 0.5465\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0160 - val_loss: 0.5432\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0152 - val_loss: 0.5627\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0159 - val_loss: 0.5503\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0159 - val_loss: 0.5468\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0162 - val_loss: 0.5501\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0162 - val_loss: 0.5410\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0153 - val_loss: 0.5458\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0154 - val_loss: 0.5437\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0156 - val_loss: 0.5477\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0159 - val_loss: 0.5462\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0156 - val_loss: 0.5525\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0151 - val_loss: 0.5529\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0153 - val_loss: 0.5501\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0157 - val_loss: 0.5531\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0152 - val_loss: 0.5548\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0154 - val_loss: 0.5419\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0155 - val_loss: 0.5603\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0150 - val_loss: 0.5437\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0148 - val_loss: 0.5469\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0150 - val_loss: 0.5420\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0151 - val_loss: 0.5539\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0153 - val_loss: 0.5565\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0156 - val_loss: 0.5547\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0153 - val_loss: 0.5577\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0153 - val_loss: 0.5414\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0151 - val_loss: 0.5433\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0153 - val_loss: 0.5536\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0154 - val_loss: 0.5706\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0151 - val_loss: 0.5505\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0148 - val_loss: 0.5625\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0153 - val_loss: 0.5559\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0149 - val_loss: 0.5473\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0153 - val_loss: 0.5516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0145 - val_loss: 0.5681\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0152 - val_loss: 0.5543\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0149 - val_loss: 0.5696\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0146 - val_loss: 0.5588\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 1s 138us/step - loss: 0.0152 - val_loss: 0.5647\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0152 - val_loss: 0.5573\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0150 - val_loss: 0.5536\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0148 - val_loss: 0.5545\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0151 - val_loss: 0.5536\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0152 - val_loss: 0.5574\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0147 - val_loss: 0.5552\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0154 - val_loss: 0.5552\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0152 - val_loss: 0.5581\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0162 - val_loss: 0.5689\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0148 - val_loss: 0.5478\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0155 - val_loss: 0.5546\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0151 - val_loss: 0.5774\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0152 - val_loss: 0.5770\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - ETA: 0s - loss: 0.015 - 1s 143us/step - loss: 0.0158 - val_loss: 0.5690\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0153 - val_loss: 0.5667\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0147 - val_loss: 0.5572\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0153 - val_loss: 0.5603\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0155 - val_loss: 0.5591\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0144 - val_loss: 0.5693\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0147 - val_loss: 0.5571\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0142 - val_loss: 0.5610\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0149 - val_loss: 0.5781\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0149 - val_loss: 0.5702\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0149 - val_loss: 0.5598\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0147 - val_loss: 0.5715\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0146 - val_loss: 0.5659\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0145 - val_loss: 0.5577\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0145 - val_loss: 0.5659\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0145 - val_loss: 0.5738\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0144 - val_loss: 0.5685\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0145 - val_loss: 0.5614\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0147 - val_loss: 0.5756\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0146 - val_loss: 0.5612\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0148 - val_loss: 0.5678\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0147 - val_loss: 0.5633\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0149 - val_loss: 0.5735\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0146 - val_loss: 0.5804\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0145 - val_loss: 0.5761\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0144 - val_loss: 0.5838\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0146 - val_loss: 0.5881\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0154 - val_loss: 0.5686\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0147 - val_loss: 0.5608\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0145 - val_loss: 0.5743\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0144 - val_loss: 0.5657\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0147 - val_loss: 0.5624\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0141 - val_loss: 0.5713\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0150 - val_loss: 0.5739\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0143 - val_loss: 0.5687\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0142 - val_loss: 0.5663\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0146 - val_loss: 0.5838\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0147 - val_loss: 0.5827\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0144 - val_loss: 0.5828\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0141 - val_loss: 0.5731\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0141 - val_loss: 0.5825\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0143 - val_loss: 0.5822\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0144 - val_loss: 0.5749\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0147 - val_loss: 0.5761\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0142 - val_loss: 0.5685\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0146 - val_loss: 0.5799\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0146 - val_loss: 0.5858\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0144 - val_loss: 0.5817\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0142 - val_loss: 0.5635\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0139 - val_loss: 0.5923\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0143 - val_loss: 0.5723\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0144 - val_loss: 0.5669\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0142 - val_loss: 0.5651\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0137 - val_loss: 0.5891\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0144 - val_loss: 0.5834\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0143 - val_loss: 0.5795\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0142 - val_loss: 0.5728\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0140 - val_loss: 0.5750\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0140 - val_loss: 0.5910\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0140 - val_loss: 0.5762\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0145 - val_loss: 0.5858\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0137 - val_loss: 0.5973\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0137 - val_loss: 0.5990\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0144 - val_loss: 0.5732\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0141 - val_loss: 0.5845\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0141 - val_loss: 0.5747\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0139 - val_loss: 0.5848\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 1s 141us/step - loss: 0.0138 - val_loss: 0.5827\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0138 - val_loss: 0.5834\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0141 - val_loss: 0.5865\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0142 - val_loss: 0.5760\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0140 - val_loss: 0.5951\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0138 - val_loss: 0.5743\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0138 - val_loss: 0.5829\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0137 - val_loss: 0.5812\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0137 - val_loss: 0.5800\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0140 - val_loss: 0.5890\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0143 - val_loss: 0.5719\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 9.3065 - val_loss: 3.0850\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 1.3714 - val_loss: 1.5869\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.9069 - val_loss: 1.4414\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.5805 - val_loss: 1.1533\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.5038 - val_loss: 0.8699\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.3976 - val_loss: 1.1296\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.5202 - val_loss: 0.8214\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.3940 - val_loss: 2.5244\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2974 - val_loss: 0.6243\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.2584 - val_loss: 0.5571\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.3063 - val_loss: 0.7830\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2257 - val_loss: 0.5110\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.2028 - val_loss: 0.4349\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.1934 - val_loss: 0.4157\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1892 - val_loss: 0.4341\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1764 - val_loss: 0.9479\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2110 - val_loss: 0.4029\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2548 - val_loss: 0.3834\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.1753 - val_loss: 0.4378\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.1437 - val_loss: 0.4471\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.1357 - val_loss: 0.3184\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1378 - val_loss: 0.4540\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1661 - val_loss: 0.3011\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1480 - val_loss: 0.3896\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1716 - val_loss: 0.3680\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1232 - val_loss: 0.3205\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1194 - val_loss: 0.2919\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1167 - val_loss: 0.2889\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1461 - val_loss: 0.3085\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.1554 - val_loss: 0.3161\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.1259 - val_loss: 0.2840\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1055 - val_loss: 0.2730\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1024 - val_loss: 0.2760\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1002 - val_loss: 0.2938\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0922 - val_loss: 0.2954\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0974 - val_loss: 0.2594\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0975 - val_loss: 0.2728\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0894 - val_loss: 0.2652\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0946 - val_loss: 0.2628\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0911 - val_loss: 0.2891\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0914 - val_loss: 0.2760\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0993 - val_loss: 0.2801\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0869 - val_loss: 0.2213\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0827 - val_loss: 0.2146\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0816 - val_loss: 0.2284\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0878 - val_loss: 0.2383\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0834 - val_loss: 0.2862\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1005 - val_loss: 0.2212\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0873 - val_loss: 0.2242\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0812 - val_loss: 0.2268\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0775 - val_loss: 0.2171\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0774 - val_loss: 0.2635\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0813 - val_loss: 0.2073\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0778 - val_loss: 0.3104\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0696 - val_loss: 0.2398\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0671 - val_loss: 0.2320\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0667 - val_loss: 0.2085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0672 - val_loss: 0.2017\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0674 - val_loss: 0.2088\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0639 - val_loss: 0.2006\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0630 - val_loss: 0.1972\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0615 - val_loss: 0.1905\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0616 - val_loss: 0.1922\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0606 - val_loss: 0.1890\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0580 - val_loss: 0.1818\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0605 - val_loss: 0.2298\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0659 - val_loss: 0.1885\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0577 - val_loss: 0.2015\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0700 - val_loss: 0.2174\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0620 - val_loss: 0.1914\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0575 - val_loss: 0.1926\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0576 - val_loss: 0.1787\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0591 - val_loss: 0.1827\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0559 - val_loss: 0.1674\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0536 - val_loss: 0.1775\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0518 - val_loss: 0.1780\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0490 - val_loss: 0.1597\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0548 - val_loss: 0.1758\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0555 - val_loss: 0.1716\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0549 - val_loss: 0.1705\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0517 - val_loss: 0.1744\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0508 - val_loss: 0.1819\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0503 - val_loss: 0.1594\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0503 - val_loss: 0.1733\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0496 - val_loss: 0.2353\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0489 - val_loss: 0.1608\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0470 - val_loss: 0.1557\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0449 - val_loss: 0.1646\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0519 - val_loss: 0.1729\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0470 - val_loss: 0.1597\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0463 - val_loss: 0.1638\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0459 - val_loss: 0.1587\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0446 - val_loss: 0.1588\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0448 - val_loss: 0.1601\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0458 - val_loss: 0.1794\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0452 - val_loss: 0.1545\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0444 - val_loss: 0.1803\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0444 - val_loss: 0.1546\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0440 - val_loss: 0.1621\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0451 - val_loss: 0.1970\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0432 - val_loss: 0.1509\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0424 - val_loss: 0.1614\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0429 - val_loss: 0.1506\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0397 - val_loss: 0.1532\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0400 - val_loss: 0.1493\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0430 - val_loss: 0.1432\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0494 - val_loss: 0.1789\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0419 - val_loss: 0.1502\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0409 - val_loss: 0.1936\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0417 - val_loss: 0.1600\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0439 - val_loss: 0.1592\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0396 - val_loss: 0.1992\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0395 - val_loss: 0.1575\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0425 - val_loss: 0.1669\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0406 - val_loss: 0.1471\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0388 - val_loss: 0.1567\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0402 - val_loss: 0.1564\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0422 - val_loss: 0.1433\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0410 - val_loss: 0.1453\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0390 - val_loss: 0.1427\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0388 - val_loss: 0.1416\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0369 - val_loss: 0.1425\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0378 - val_loss: 0.1835\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0363 - val_loss: 0.1433\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0406 - val_loss: 0.1531\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0377 - val_loss: 0.1599\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0362 - val_loss: 0.1374\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0403 - val_loss: 0.1457\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0368 - val_loss: 0.1431\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0357 - val_loss: 0.1518\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0370 - val_loss: 0.1444\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0354 - val_loss: 0.1722\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0342 - val_loss: 0.1339\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0344 - val_loss: 0.1388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0349 - val_loss: 0.1521\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0367 - val_loss: 0.1318\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0330 - val_loss: 0.1475\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0335 - val_loss: 0.1463\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0350 - val_loss: 0.1475\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0344 - val_loss: 0.1372\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0331 - val_loss: 0.1739\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0311 - val_loss: 0.1383\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0316 - val_loss: 0.1373\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0327 - val_loss: 0.1523\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0313 - val_loss: 0.1440\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0332 - val_loss: 0.1486\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0366 - val_loss: 0.1350\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0321 - val_loss: 0.1338\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0365 - val_loss: 0.1442\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0320 - val_loss: 0.1408\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0317 - val_loss: 0.1433\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0318 - val_loss: 0.1379\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0330 - val_loss: 0.1514\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0338 - val_loss: 0.1444\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0312 - val_loss: 0.1951\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0324 - val_loss: 0.1601\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0320 - val_loss: 0.1408\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0309 - val_loss: 0.1380\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0315 - val_loss: 0.1517\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0341 - val_loss: 0.1514\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0295 - val_loss: 0.1368\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0287 - val_loss: 0.1462\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0298 - val_loss: 0.1490\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0294 - val_loss: 0.1377\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0313 - val_loss: 0.1306\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0331 - val_loss: 0.1407\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0344 - val_loss: 0.1405\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0319 - val_loss: 0.1504\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0322 - val_loss: 0.1325\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0307 - val_loss: 0.1379\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0278 - val_loss: 0.1333\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0283 - val_loss: 0.1385\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0305 - val_loss: 0.1503\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0294 - val_loss: 0.1946\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0296 - val_loss: 0.1386\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0274 - val_loss: 0.1356\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0265 - val_loss: 0.1535\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0285 - val_loss: 0.1370\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0275 - val_loss: 0.2177\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0297 - val_loss: 0.1421\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0270 - val_loss: 0.2165\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0262 - val_loss: 0.1356\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0265 - val_loss: 0.1386\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0268 - val_loss: 0.1390\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0266 - val_loss: 0.1462\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0296 - val_loss: 0.1474\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0264 - val_loss: 0.1325\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0275 - val_loss: 0.1426\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0272 - val_loss: 0.1415\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0258 - val_loss: 0.1362\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0287 - val_loss: 0.1516\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0255 - val_loss: 0.1348\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0265 - val_loss: 0.1527\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0263 - val_loss: 0.1440\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0249 - val_loss: 0.1514\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0257 - val_loss: 0.1407\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0252 - val_loss: 0.1411\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0273 - val_loss: 0.1707\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0265 - val_loss: 0.1513\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0297 - val_loss: 0.1536\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0277 - val_loss: 0.2073\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0401 - val_loss: 0.1412\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0288 - val_loss: 0.1467\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0294 - val_loss: 0.1538\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0268 - val_loss: 0.1437\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0258 - val_loss: 0.1376\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0269 - val_loss: 0.1471\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0257 - val_loss: 0.1481\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0240 - val_loss: 0.1352\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0247 - val_loss: 0.1380\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0243 - val_loss: 0.1366\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0248 - val_loss: 0.1381\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0248 - val_loss: 0.1545\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0236 - val_loss: 0.1437\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0247 - val_loss: 0.1379\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0270 - val_loss: 0.1355\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0241 - val_loss: 0.1781\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0244 - val_loss: 0.1349\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 289us/step - loss: 0.0233 - val_loss: 0.1370\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0253 - val_loss: 0.1407\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0225 - val_loss: 0.1405\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0231 - val_loss: 0.1457\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0234 - val_loss: 0.1409\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0230 - val_loss: 0.1329\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0241 - val_loss: 0.1381\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0234 - val_loss: 0.1377\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0239 - val_loss: 0.1477\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0233 - val_loss: 0.1437\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0257 - val_loss: 0.1359\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0231 - val_loss: 0.1372\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0241 - val_loss: 0.1416\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0240 - val_loss: 0.1380\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0228 - val_loss: 0.1475\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0232 - val_loss: 0.1423\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0235 - val_loss: 0.1397\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0236 - val_loss: 0.1382\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0221 - val_loss: 0.1460\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0234 - val_loss: 0.1496\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0213 - val_loss: 0.1415\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0241 - val_loss: 0.1386\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0234 - val_loss: 0.1585\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0230 - val_loss: 0.1524\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0265 - val_loss: 0.1507\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0236 - val_loss: 0.1422\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0234 - val_loss: 0.1538\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0240 - val_loss: 0.1356\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0230 - val_loss: 0.1351\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0238 - val_loss: 0.1412\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0288 - val_loss: 0.1462\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0222 - val_loss: 0.1485\n"
     ]
    }
   ],
   "source": [
    "n_batches = 21\n",
    "batch_sizes = np.round(np.linspace(1,X_train_scaled.shape[0],n_batches))\n",
    "model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed0a6eb00>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Mini Batch=21')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0191 - val_loss: 0.1456\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0190 - val_loss: 0.1425\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0191 - val_loss: 0.1373\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0194 - val_loss: 0.1386\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0185 - val_loss: 0.1422\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0203 - val_loss: 0.1460\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0191 - val_loss: 0.1383\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0186 - val_loss: 0.1381\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0191 - val_loss: 0.1374\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0189 - val_loss: 0.1375\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0183 - val_loss: 0.1368\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0182 - val_loss: 0.1424\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0188 - val_loss: 0.1372\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0189 - val_loss: 0.1421\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0189 - val_loss: 0.1368\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0178 - val_loss: 0.1425\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0183 - val_loss: 0.1397\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0186 - val_loss: 0.1351\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0184 - val_loss: 0.1353\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0182 - val_loss: 0.1372\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0197 - val_loss: 0.1456\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0180 - val_loss: 0.1401\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0180 - val_loss: 0.1419\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0178 - val_loss: 0.1498\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0179 - val_loss: 0.1390\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0181 - val_loss: 0.1392\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0183 - val_loss: 0.1384\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0187 - val_loss: 0.1364\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0181 - val_loss: 0.1389\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0175 - val_loss: 0.1370\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0183 - val_loss: 0.1419\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0176 - val_loss: 0.1355\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0180 - val_loss: 0.1463\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0181 - val_loss: 0.1359\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0179 - val_loss: 0.1409\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0176 - val_loss: 0.1385\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0181 - val_loss: 0.1386\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0176 - val_loss: 0.1397\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0183 - val_loss: 0.1392\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.0178 - val_loss: 0.1364\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0177 - val_loss: 0.1421\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0181 - val_loss: 0.1425\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0178 - val_loss: 0.1394\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0173 - val_loss: 0.1410\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.0174 - val_loss: 0.1397\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0171 - val_loss: 0.1397\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0168 - val_loss: 0.1441\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0173 - val_loss: 0.1368\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0175 - val_loss: 0.1427\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0173 - val_loss: 0.1469\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0170 - val_loss: 0.1383\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0173 - val_loss: 0.1390\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0173 - val_loss: 0.1450\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0175 - val_loss: 0.1391\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0175 - val_loss: 0.1380\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0177 - val_loss: 0.1344\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0167 - val_loss: 0.1385\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0171 - val_loss: 0.1439\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0176 - val_loss: 0.1417\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0174 - val_loss: 0.1409\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0174 - val_loss: 0.1404\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0164 - val_loss: 0.1394\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0168 - val_loss: 0.1385\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0169 - val_loss: 0.1438\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0165 - val_loss: 0.1384\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0172 - val_loss: 0.1420\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0165 - val_loss: 0.1436\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0168 - val_loss: 0.1386\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0169 - val_loss: 0.1408\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0175 - val_loss: 0.1390\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0170 - val_loss: 0.1398\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0169 - val_loss: 0.1432\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0173 - val_loss: 0.1399\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0169 - val_loss: 0.1431\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0170 - val_loss: 0.1452\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0169 - val_loss: 0.1391\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0168 - val_loss: 0.1460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0171 - val_loss: 0.1405\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0164 - val_loss: 0.1389\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0169 - val_loss: 0.1453\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0166 - val_loss: 0.1411\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0172 - val_loss: 0.1397\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0168 - val_loss: 0.1391\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0167 - val_loss: 0.1464\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0166 - val_loss: 0.1461\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0166 - val_loss: 0.1418\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0173 - val_loss: 0.1439\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0158 - val_loss: 0.1404\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0167 - val_loss: 0.1414\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0165 - val_loss: 0.1395\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0162 - val_loss: 0.1395\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0168 - val_loss: 0.1399\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0162 - val_loss: 0.1413\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0166 - val_loss: 0.1484\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0163 - val_loss: 0.1395\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0165 - val_loss: 0.1437\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0166 - val_loss: 0.1460\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0165 - val_loss: 0.1406\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0163 - val_loss: 0.1385\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0159 - val_loss: 0.1517\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0161 - val_loss: 0.1390\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0165 - val_loss: 0.1394\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0162 - val_loss: 0.1381\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0161 - val_loss: 0.1464\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0163 - val_loss: 0.1436\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0159 - val_loss: 0.1433\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0168 - val_loss: 0.1462\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0163 - val_loss: 0.1432\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0156 - val_loss: 0.1439\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0157 - val_loss: 0.1508\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0156 - val_loss: 0.1420\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0160 - val_loss: 0.1394\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0158 - val_loss: 0.1443\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0165 - val_loss: 0.1457\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0159 - val_loss: 0.1472\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0165 - val_loss: 0.1433\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0159 - val_loss: 0.1435\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0161 - val_loss: 0.1456\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0162 - val_loss: 0.1440\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0155 - val_loss: 0.1452\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0167 - val_loss: 0.1442\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0159 - val_loss: 0.1483\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0164 - val_loss: 0.1458\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0159 - val_loss: 0.1446\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0160 - val_loss: 0.1521\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0155 - val_loss: 0.1403\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0158 - val_loss: 0.1410\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0156 - val_loss: 0.1399\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0159 - val_loss: 0.1417\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0155 - val_loss: 0.1425\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0156 - val_loss: 0.1427\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0161 - val_loss: 0.1502\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0154 - val_loss: 0.1412\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0151 - val_loss: 0.1471\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0158 - val_loss: 0.1448\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0152 - val_loss: 0.1456\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0156 - val_loss: 0.1452\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0160 - val_loss: 0.1447\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0156 - val_loss: 0.1507\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0150 - val_loss: 0.1427\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0156 - val_loss: 0.1546\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0157 - val_loss: 0.1413\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0154 - val_loss: 0.1496\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0153 - val_loss: 0.1452\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0146 - val_loss: 0.1487\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0154 - val_loss: 0.1461\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0154 - val_loss: 0.1563\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0153 - val_loss: 0.1563\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0160 - val_loss: 0.1463\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0155 - val_loss: 0.1452\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0152 - val_loss: 0.1478\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0152 - val_loss: 0.1572\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0149 - val_loss: 0.1442\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0148 - val_loss: 0.1464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0159 - val_loss: 0.1478\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0155 - val_loss: 0.1456\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0154 - val_loss: 0.1456\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0146 - val_loss: 0.1452\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0147 - val_loss: 0.1457\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0151 - val_loss: 0.1440\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0157 - val_loss: 0.1516\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0146 - val_loss: 0.1445\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0146 - val_loss: 0.1415\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0145 - val_loss: 0.1514\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0155 - val_loss: 0.1503\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0150 - val_loss: 0.1462\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0151 - val_loss: 0.1543\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0151 - val_loss: 0.1548\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0150 - val_loss: 0.1457\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0150 - val_loss: 0.1494\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0148 - val_loss: 0.1463\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0146 - val_loss: 0.1514\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0145 - val_loss: 0.1457\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0144 - val_loss: 0.1436\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0151 - val_loss: 0.1484\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0150 - val_loss: 0.1470\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0153 - val_loss: 0.1856\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0154 - val_loss: 0.1451\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0149 - val_loss: 0.1519\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0151 - val_loss: 0.1479\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0151 - val_loss: 0.1492\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0152 - val_loss: 0.1515\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0144 - val_loss: 0.1451\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0150 - val_loss: 0.1530\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0149 - val_loss: 0.1529\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0149 - val_loss: 0.1514\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0153 - val_loss: 0.1486\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0149 - val_loss: 0.1485\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0148 - val_loss: 0.1472\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0154 - val_loss: 0.1486\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0140 - val_loss: 0.1485\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0143 - val_loss: 0.1458\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0146 - val_loss: 0.1506\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0143 - val_loss: 0.1498\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0147 - val_loss: 0.1461\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0143 - val_loss: 0.1564\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0159 - val_loss: 0.1481\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0146 - val_loss: 0.1533\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0142 - val_loss: 0.1604\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0145 - val_loss: 0.1454\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0147 - val_loss: 0.1509\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0139 - val_loss: 0.1505\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0149 - val_loss: 0.1491\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0146 - val_loss: 0.1465\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0147 - val_loss: 0.1458\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0144 - val_loss: 0.1474\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0146 - val_loss: 0.1498\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0145 - val_loss: 0.1486\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0151 - val_loss: 0.1566\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0146 - val_loss: 0.1488\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0143 - val_loss: 0.1494\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0148 - val_loss: 0.1493\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0150 - val_loss: 0.1566\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0143 - val_loss: 0.1568\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0145 - val_loss: 0.1517\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0140 - val_loss: 0.1494\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0139 - val_loss: 0.1546\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0140 - val_loss: 0.1571\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0139 - val_loss: 0.1501\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0137 - val_loss: 0.1654\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0141 - val_loss: 0.1484\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0136 - val_loss: 0.1510\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0143 - val_loss: 0.1526\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0137 - val_loss: 0.1494\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0136 - val_loss: 0.1546\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0136 - val_loss: 0.1545\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0140 - val_loss: 0.1528\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0144 - val_loss: 0.1527\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0138 - val_loss: 0.1523\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0139 - val_loss: 0.1513\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0139 - val_loss: 0.1527\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0141 - val_loss: 0.1544\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0143 - val_loss: 0.1547\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0135 - val_loss: 0.1508\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0138 - val_loss: 0.1538\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0137 - val_loss: 0.1591\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0137 - val_loss: 0.1551\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0134 - val_loss: 0.1495\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0137 - val_loss: 0.1517\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0140 - val_loss: 0.1528\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0143 - val_loss: 0.1512\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0134 - val_loss: 0.1531\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0140 - val_loss: 0.1534\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0136 - val_loss: 0.1598\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0138 - val_loss: 0.1539\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0142 - val_loss: 0.1528\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0135 - val_loss: 0.1526\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0134 - val_loss: 0.1577\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0140 - val_loss: 0.1545\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0136 - val_loss: 0.1541\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 8.7221 - val_loss: 2.7310\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 1.5465 - val_loss: 1.7448\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.9724 - val_loss: 1.4422\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.6170 - val_loss: 1.4786\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.5285 - val_loss: 1.2230\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.4665 - val_loss: 1.1573\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.3808 - val_loss: 1.1377\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.3574 - val_loss: 1.4225\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.3421 - val_loss: 0.9697\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.3223 - val_loss: 0.9182\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.3056 - val_loss: 0.8792\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.2744 - val_loss: 0.9732\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.2353 - val_loss: 0.8371\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.2249 - val_loss: 0.8931\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.2138 - val_loss: 0.7808\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.2468 - val_loss: 0.7606\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.2009 - val_loss: 0.7471\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.1930 - val_loss: 1.1346\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.2519 - val_loss: 0.8594\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.1644 - val_loss: 0.6525\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.1664 - val_loss: 0.7261\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.1612 - val_loss: 0.6893\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.1500 - val_loss: 0.9376\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.1961 - val_loss: 0.7013\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.1554 - val_loss: 0.6650\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.1352 - val_loss: 0.6966\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.1403 - val_loss: 0.6151\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.1290 - val_loss: 0.5376\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.1153 - val_loss: 0.5563\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.1147 - val_loss: 0.5314\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.1192 - val_loss: 0.5390\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.1170 - val_loss: 0.5621\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.1023 - val_loss: 0.5411\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.1061 - val_loss: 0.5164\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.1039 - val_loss: 0.5256\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.1050 - val_loss: 0.5594\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.1036 - val_loss: 0.6045\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0949 - val_loss: 0.5276\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0928 - val_loss: 0.5523\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0938 - val_loss: 0.5275\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.0867 - val_loss: 0.5004\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0889 - val_loss: 0.4982\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0905 - val_loss: 0.5345\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0852 - val_loss: 0.5044\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0848 - val_loss: 0.4778\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0895 - val_loss: 0.4918\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0865 - val_loss: 0.5409\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0795 - val_loss: 0.4886\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0762 - val_loss: 0.5157\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.0734 - val_loss: 0.4838\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0735 - val_loss: 0.4512\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0712 - val_loss: 0.4601\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0785 - val_loss: 0.4576\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0787 - val_loss: 0.4662\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0672 - val_loss: 0.4751\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0662 - val_loss: 0.4674\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0654 - val_loss: 0.5416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.0677 - val_loss: 0.4783\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0645 - val_loss: 0.5054\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0648 - val_loss: 0.5992\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0616 - val_loss: 0.4776\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0630 - val_loss: 0.5555\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0626 - val_loss: 0.4560\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0607 - val_loss: 0.4631\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0555 - val_loss: 0.4774\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0586 - val_loss: 0.4344\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0606 - val_loss: 0.5972\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0639 - val_loss: 0.4679\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0586 - val_loss: 0.6652\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0567 - val_loss: 0.4542\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0547 - val_loss: 0.4611\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0548 - val_loss: 0.4328\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0510 - val_loss: 0.4412\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0553 - val_loss: 0.4809\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0577 - val_loss: 0.4890\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0546 - val_loss: 0.4526\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0518 - val_loss: 0.4707\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0485 - val_loss: 0.4377\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0523 - val_loss: 0.4746\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0489 - val_loss: 0.4802\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0509 - val_loss: 0.4611\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0566 - val_loss: 0.4786\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.0497 - val_loss: 0.4514\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0549 - val_loss: 0.4617\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0490 - val_loss: 0.5288\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0579 - val_loss: 0.4408\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0465 - val_loss: 0.4347\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0496 - val_loss: 0.4524\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0464 - val_loss: 0.4349\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0435 - val_loss: 0.5929\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0553 - val_loss: 0.4621\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0433 - val_loss: 0.4544\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0462 - val_loss: 0.4570\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0470 - val_loss: 0.4674\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0450 - val_loss: 0.4510\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0434 - val_loss: 0.4597\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.0453 - val_loss: 0.4379\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 0.0452 - val_loss: 0.4606\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0476 - val_loss: 0.4635\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0459 - val_loss: 0.4617\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0423 - val_loss: 0.4463\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0397 - val_loss: 0.4629\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0411 - val_loss: 0.4352\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0425 - val_loss: 0.4477\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0402 - val_loss: 0.4610\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0421 - val_loss: 0.5423\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0405 - val_loss: 0.4590\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0388 - val_loss: 0.4600\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0403 - val_loss: 0.4424\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0392 - val_loss: 0.4938\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0392 - val_loss: 0.4480\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0364 - val_loss: 0.4840\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0389 - val_loss: 0.4271\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0362 - val_loss: 0.4494\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0369 - val_loss: 0.4408\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0371 - val_loss: 0.4557\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0376 - val_loss: 0.4474\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0374 - val_loss: 0.4479\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0390 - val_loss: 0.4596\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0367 - val_loss: 0.5348\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0353 - val_loss: 0.4521\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.0339 - val_loss: 0.4531\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0359 - val_loss: 0.4668\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0354 - val_loss: 0.4572\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0355 - val_loss: 0.4689\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0349 - val_loss: 0.4751\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0399 - val_loss: 0.4524\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.0484 - val_loss: 0.4796\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0391 - val_loss: 0.5212\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0368 - val_loss: 0.4618\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0354 - val_loss: 0.4652\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0363 - val_loss: 0.4703\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0353 - val_loss: 0.4480\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0326 - val_loss: 0.4774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0340 - val_loss: 0.4905\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0332 - val_loss: 0.5660\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0363 - val_loss: 0.4631\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0335 - val_loss: 0.4640\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0410 - val_loss: 0.5062\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0401 - val_loss: 0.5354\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0372 - val_loss: 0.4802\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0337 - val_loss: 0.4827\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0326 - val_loss: 0.4809\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.0351 - val_loss: 0.4947\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0351 - val_loss: 0.4901\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0339 - val_loss: 0.4937\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0308 - val_loss: 0.4703\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0287 - val_loss: 0.5108\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0311 - val_loss: 0.4795\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0287 - val_loss: 0.4658\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0297 - val_loss: 0.4615\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0312 - val_loss: 0.4869\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0293 - val_loss: 0.4719\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0295 - val_loss: 0.4715\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0287 - val_loss: 0.5442\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0309 - val_loss: 0.4576\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0290 - val_loss: 0.4891\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0296 - val_loss: 0.4581\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0283 - val_loss: 0.4533\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0276 - val_loss: 0.4885\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0285 - val_loss: 0.4848\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0286 - val_loss: 0.4811\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0304 - val_loss: 0.4595\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0299 - val_loss: 0.4700\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0269 - val_loss: 0.4673\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0297 - val_loss: 0.5046\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0272 - val_loss: 0.4555\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0279 - val_loss: 0.4720\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0288 - val_loss: 0.4751\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0274 - val_loss: 0.4842\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 0.0282 - val_loss: 0.5155\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0272 - val_loss: 0.5078\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0266 - val_loss: 0.4682\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0265 - val_loss: 0.4694\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0281 - val_loss: 0.4701\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0273 - val_loss: 0.4576\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0264 - val_loss: 0.4635\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0275 - val_loss: 0.5077\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0268 - val_loss: 0.4667\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0258 - val_loss: 0.4778\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0265 - val_loss: 0.4805\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0263 - val_loss: 0.4703\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0265 - val_loss: 0.4645\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0256 - val_loss: 0.4847\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0266 - val_loss: 0.4944\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0264 - val_loss: 0.4657\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0265 - val_loss: 0.4845\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0254 - val_loss: 0.4893\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0249 - val_loss: 0.4666\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0247 - val_loss: 0.4742\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.0242 - val_loss: 0.4823\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.0240 - val_loss: 0.4732\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0245 - val_loss: 0.5286\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0244 - val_loss: 0.4607\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0250 - val_loss: 0.5364\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0248 - val_loss: 0.4877\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0261 - val_loss: 0.4678\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0241 - val_loss: 0.4864\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0251 - val_loss: 0.4859\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0230 - val_loss: 0.5134\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.0238 - val_loss: 0.4948\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0251 - val_loss: 0.4854\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.0237 - val_loss: 0.4858\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0244 - val_loss: 0.5014\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0235 - val_loss: 0.4834\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0234 - val_loss: 0.4806\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0239 - val_loss: 0.5059\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0229 - val_loss: 0.4829\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0238 - val_loss: 0.4920\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0234 - val_loss: 0.5181\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0222 - val_loss: 0.5056\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0233 - val_loss: 0.4990\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0231 - val_loss: 0.4940\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.0232 - val_loss: 0.4832\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0220 - val_loss: 0.4840\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0234 - val_loss: 0.4917\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0228 - val_loss: 0.4927\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0231 - val_loss: 0.5044\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0220 - val_loss: 0.5151\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0227 - val_loss: 0.5108\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0227 - val_loss: 0.5058\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0219 - val_loss: 0.4993\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0223 - val_loss: 0.5117\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0220 - val_loss: 0.4887\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 289us/step - loss: 0.0232 - val_loss: 0.5171\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0256 - val_loss: 0.5299\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0230 - val_loss: 0.5001\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0234 - val_loss: 0.4893\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0229 - val_loss: 0.5029\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.0209 - val_loss: 0.5389\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.0218 - val_loss: 0.5258\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0224 - val_loss: 0.5231\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0211 - val_loss: 0.5037\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0216 - val_loss: 0.5006\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0274 - val_loss: 0.5000\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0221 - val_loss: 0.5356\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0220 - val_loss: 0.5291\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0219 - val_loss: 0.5187\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.0216 - val_loss: 0.5199\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0206 - val_loss: 0.5147\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0207 - val_loss: 0.5106\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.0216 - val_loss: 0.5216\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0203 - val_loss: 0.6335\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0279 - val_loss: 0.5137\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0217 - val_loss: 0.5246\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0219 - val_loss: 0.5049\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0212 - val_loss: 0.4969\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0224 - val_loss: 0.5087\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.0214 - val_loss: 0.4977\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0238 - val_loss: 0.4904\n"
     ]
    }
   ],
   "source": [
    "n_batches = 10\n",
    "batch_sizes = np.round(np.linspace(1,X_train_scaled.shape[0],n_batches))\n",
    "model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed106f208>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Mini Batch=10')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 0.0184 - val_loss: 0.5190\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.0176 - val_loss: 0.4986\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0174 - val_loss: 0.5138\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0179 - val_loss: 0.5129\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0175 - val_loss: 0.5209\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0171 - val_loss: 0.5089\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0181 - val_loss: 0.5062\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 4s 380us/step - loss: 0.0174 - val_loss: 0.5292\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.0176 - val_loss: 0.5009\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.0172 - val_loss: 0.5182\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0171 - val_loss: 0.5107\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0176 - val_loss: 0.5191\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0168 - val_loss: 0.5260\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0171 - val_loss: 0.5052\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0174 - val_loss: 0.5328\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0178 - val_loss: 0.5000\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0172 - val_loss: 0.5069\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0173 - val_loss: 0.5133\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0169 - val_loss: 0.5323\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0169 - val_loss: 0.5148\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0172 - val_loss: 0.5167\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0172 - val_loss: 0.5084\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0173 - val_loss: 0.5110\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0168 - val_loss: 0.5103\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0170 - val_loss: 0.5140\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0167 - val_loss: 0.5114\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0172 - val_loss: 0.5298\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0168 - val_loss: 0.5187\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0176 - val_loss: 0.5275\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0168 - val_loss: 0.5206\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0168 - val_loss: 0.5105\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0167 - val_loss: 0.5134\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0170 - val_loss: 0.5265\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0167 - val_loss: 0.5124\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0172 - val_loss: 0.5169\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0172 - val_loss: 0.5178\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0166 - val_loss: 0.5165\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0167 - val_loss: 0.5163\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.0166 - val_loss: 0.5139\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.0168 - val_loss: 0.5336\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0167 - val_loss: 0.5164\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0167 - val_loss: 0.5249\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0165 - val_loss: 0.5219\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0163 - val_loss: 0.5301\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0165 - val_loss: 0.5211\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0168 - val_loss: 0.5285\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0166 - val_loss: 0.5301\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0162 - val_loss: 0.5218\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0161 - val_loss: 0.5204\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0166 - val_loss: 0.5265\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0163 - val_loss: 0.5330\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0165 - val_loss: 0.5297\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.0165 - val_loss: 0.5230\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0162 - val_loss: 0.5268\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.0165 - val_loss: 0.5209\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0162 - val_loss: 0.5304\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0161 - val_loss: 0.5256\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0167 - val_loss: 0.5180\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0163 - val_loss: 0.5118\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0158 - val_loss: 0.5281\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.0156 - val_loss: 0.5506\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0162 - val_loss: 0.5573\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0160 - val_loss: 0.5262\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0161 - val_loss: 0.5288\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0160 - val_loss: 0.5216\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0164 - val_loss: 0.5078\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0164 - val_loss: 0.5202\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0159 - val_loss: 0.5138\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0161 - val_loss: 0.5372\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0163 - val_loss: 0.5204\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0160 - val_loss: 0.5281\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0162 - val_loss: 0.5127\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0158 - val_loss: 0.5154\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0156 - val_loss: 0.5282\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0159 - val_loss: 0.5184\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0163 - val_loss: 0.5203\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0160 - val_loss: 0.5411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0159 - val_loss: 0.5292\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0152 - val_loss: 0.5230\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0158 - val_loss: 0.5258\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0155 - val_loss: 0.5383\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0155 - val_loss: 0.5197\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0154 - val_loss: 0.5266\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0163 - val_loss: 0.5464\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0157 - val_loss: 0.5211\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0155 - val_loss: 0.5357\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0154 - val_loss: 0.5395\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0155 - val_loss: 0.5319\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0157 - val_loss: 0.5250\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0158 - val_loss: 0.5310\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0156 - val_loss: 0.5337\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0155 - val_loss: 0.5293\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0153 - val_loss: 0.5194\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0154 - val_loss: 0.5338\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0155 - val_loss: 0.5217\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0156 - val_loss: 0.5427\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0155 - val_loss: 0.5331\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.0156 - val_loss: 0.5315\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0154 - val_loss: 0.5268\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 0.0159 - val_loss: 0.5302\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0157 - val_loss: 0.5259\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0156 - val_loss: 0.5333\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0155 - val_loss: 0.5272\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0160 - val_loss: 0.5213\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0159 - val_loss: 0.5282\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0154 - val_loss: 0.5352\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0158 - val_loss: 0.5311\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.0153 - val_loss: 0.5262\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0156 - val_loss: 0.5404\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0153 - val_loss: 0.5373\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0152 - val_loss: 0.5284\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0160 - val_loss: 0.5183\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0159 - val_loss: 0.5307\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0152 - val_loss: 0.5480\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0156 - val_loss: 0.5343\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0156 - val_loss: 0.5307\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0151 - val_loss: 0.5342\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0150 - val_loss: 0.5310\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0156 - val_loss: 0.5321\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0151 - val_loss: 0.5402\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0150 - val_loss: 0.5216\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0148 - val_loss: 0.5481\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0154 - val_loss: 0.5335\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0148 - val_loss: 0.5385\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0149 - val_loss: 0.5403\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0146 - val_loss: 0.5403\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0151 - val_loss: 0.5306\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0150 - val_loss: 0.5390\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0153 - val_loss: 0.5418\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0150 - val_loss: 0.5425\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0150 - val_loss: 0.5417\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0149 - val_loss: 0.5360\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0142 - val_loss: 0.5295\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0149 - val_loss: 0.5334\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0148 - val_loss: 0.5370\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0147 - val_loss: 0.5368\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0146 - val_loss: 0.5506\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0151 - val_loss: 0.5305\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0143 - val_loss: 0.5446\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0145 - val_loss: 0.5494\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0150 - val_loss: 0.5494\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0149 - val_loss: 0.5379\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0142 - val_loss: 0.5238\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0146 - val_loss: 0.5373\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0148 - val_loss: 0.5359\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0148 - val_loss: 0.5416\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.0143 - val_loss: 0.5274\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0147 - val_loss: 0.5486\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0140 - val_loss: 0.5372\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0148 - val_loss: 0.5598\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0143 - val_loss: 0.5307\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0145 - val_loss: 0.5288\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0141 - val_loss: 0.5528\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0145 - val_loss: 0.5498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.0146 - val_loss: 0.5292\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0143 - val_loss: 0.5531\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.0145 - val_loss: 0.5396\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 0.0143 - val_loss: 0.5387\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0146 - val_loss: 0.5508\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0144 - val_loss: 0.5365\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0143 - val_loss: 0.5390\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0144 - val_loss: 0.5588\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0141 - val_loss: 0.5309\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0141 - val_loss: 0.5548\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0143 - val_loss: 0.5455\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0147 - val_loss: 0.5613\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0145 - val_loss: 0.5464\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0137 - val_loss: 0.5418\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0144 - val_loss: 0.5425\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0141 - val_loss: 0.5441\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0141 - val_loss: 0.5399\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0143 - val_loss: 0.5321\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0142 - val_loss: 0.5580\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0145 - val_loss: 0.5407\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0137 - val_loss: 0.5434\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0140 - val_loss: 0.5459\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.0141 - val_loss: 0.5444\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0144 - val_loss: 0.5483\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0141 - val_loss: 0.5413\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0138 - val_loss: 0.5395\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0137 - val_loss: 0.5400\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0138 - val_loss: 0.5458\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0143 - val_loss: 0.5688 2\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0141 - val_loss: 0.5487\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.0135 - val_loss: 0.5358\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0137 - val_loss: 0.5403\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0141 - val_loss: 0.5297\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.0138 - val_loss: 0.5363\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0137 - val_loss: 0.5523\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0138 - val_loss: 0.5683\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0143 - val_loss: 0.5411\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0139 - val_loss: 0.5542\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0140 - val_loss: 0.5488\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0141 - val_loss: 0.5445\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0139 - val_loss: 0.5665\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0138 - val_loss: 0.5530\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0140 - val_loss: 0.5460\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0143 - val_loss: 0.5362\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0140 - val_loss: 0.5621\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0136 - val_loss: 0.5467\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0137 - val_loss: 0.5392\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0137 - val_loss: 0.5496\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0135 - val_loss: 0.5430\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0138 - val_loss: 0.5478\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0138 - val_loss: 0.5462\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0136 - val_loss: 0.5489\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0136 - val_loss: 0.5581\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0139 - val_loss: 0.5550\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0138 - val_loss: 0.5476\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0139 - val_loss: 0.5445\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0135 - val_loss: 0.5533\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0141 - val_loss: 0.5616\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.0137 - val_loss: 0.5468\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0134 - val_loss: 0.5449\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0137 - val_loss: 0.5455\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0138 - val_loss: 0.5464\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0137 - val_loss: 0.5663\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0135 - val_loss: 0.5535\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0135 - val_loss: 0.5713\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0137 - val_loss: 0.5476 0s - lo\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0133 - val_loss: 0.5441\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0137 - val_loss: 0.5489\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0136 - val_loss: 0.5507\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0136 - val_loss: 0.5793\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0132 - val_loss: 0.5647\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0138 - val_loss: 0.5535\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0137 - val_loss: 0.5507\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0130 - val_loss: 0.5620\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0137 - val_loss: 0.5412\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0130 - val_loss: 0.5597\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0136 - val_loss: 0.5573\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0131 - val_loss: 0.5474\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0133 - val_loss: 0.5586\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0135 - val_loss: 0.5467\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0132 - val_loss: 0.5463\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0132 - val_loss: 0.5576\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0138 - val_loss: 0.5536\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0138 - val_loss: 0.5576\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0128 - val_loss: 0.5509\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0132 - val_loss: 0.5491\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0133 - val_loss: 0.5473\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0130 - val_loss: 0.5555\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0135 - val_loss: 0.5492\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0130 - val_loss: 0.5636\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0127 - val_loss: 0.5576\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0128 - val_loss: 0.5591\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0131 - val_loss: 0.5626\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0132 - val_loss: 0.5617\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0133 - val_loss: 0.5612\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0139 - val_loss: 0.5651\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 6s 581us/step - loss: 9.7435 - val_loss: 2.7231\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 4s 445us/step - loss: 1.3949 - val_loss: 1.7645\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.8441 - val_loss: 1.5976\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 5s 503us/step - loss: 0.6511 - val_loss: 1.6193\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 4s 441us/step - loss: 0.5592 - val_loss: 1.1969\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 422us/step - loss: 0.5184 - val_loss: 1.1330\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.5296 - val_loss: 1.2349\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 4s 424us/step - loss: 0.4768 - val_loss: 1.1330\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 4s 430us/step - loss: 0.3154 - val_loss: 0.8953\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.2894 - val_loss: 0.8807\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 4s 426us/step - loss: 0.2637 - val_loss: 0.8421\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 0.2640 - val_loss: 1.0390\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 4s 433us/step - loss: 0.3424 - val_loss: 0.7575\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 4s 402us/step - loss: 0.2453 - val_loss: 0.7397\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.2379 - val_loss: 0.6880\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 4s 428us/step - loss: 0.2187 - val_loss: 0.6974\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 4s 421us/step - loss: 0.2738 - val_loss: 0.6823\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.1807 - val_loss: 0.6389\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.2449 - val_loss: 0.6996\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 4s 436us/step - loss: 0.1980 - val_loss: 0.5878\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.1577 - val_loss: 0.5490\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.1461 - val_loss: 0.5821\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 4s 424us/step - loss: 0.1400 - val_loss: 0.5525\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 4s 426us/step - loss: 0.1349 - val_loss: 0.5252\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.1288 - val_loss: 0.6781\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.1246 - val_loss: 0.4771\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: 0.1291 - val_loss: 0.4576\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.1138 - val_loss: 0.5147\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 0.1195 - val_loss: 0.5817\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 4s 422us/step - loss: 0.1129 - val_loss: 0.4810\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 4s 430us/step - loss: 0.1095 - val_loss: 0.4646\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 4s 448us/step - loss: 0.1177 - val_loss: 0.4325\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.1052 - val_loss: 0.4195\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 4s 426us/step - loss: 0.1042 - val_loss: 0.4896\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.0998 - val_loss: 0.5095\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: 0.1025 - val_loss: 0.4368\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: 0.0965 - val_loss: 0.3850\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 4s 403us/step - loss: 0.1048 - val_loss: 0.4986\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: 0.0950 - val_loss: 0.3830\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 0.0923 - val_loss: 0.4414\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 0.0893 - val_loss: 0.3781\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 0.0884 - val_loss: 0.4606\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 0.0833 - val_loss: 0.3484\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 0.0854 - val_loss: 0.3744\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 4s 394us/step - loss: 0.0815 - val_loss: 0.4039\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 4s 430us/step - loss: 0.0830 - val_loss: 0.3529\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: 0.0791 - val_loss: 0.3523\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0776 - val_loss: 0.3939\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0871 - val_loss: 0.3423\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0752 - val_loss: 0.3439\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0854 - val_loss: 0.3418\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.0721 - val_loss: 0.3955\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0700 - val_loss: 0.3486\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0715 - val_loss: 0.3998\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0728 - val_loss: 0.3270\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0689 - val_loss: 0.3502\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0690 - val_loss: 0.3329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0633 - val_loss: 0.3234\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0682 - val_loss: 0.3253\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0633 - val_loss: 0.3150\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0650 - val_loss: 0.3256\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0648 - val_loss: 0.3181\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0609 - val_loss: 0.3197\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0626 - val_loss: 0.3221\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0759 - val_loss: 0.3346\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0658 - val_loss: 0.3015\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0602 - val_loss: 0.3453\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0579 - val_loss: 0.3340\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0572 - val_loss: 0.3161\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0612 - val_loss: 0.3140\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0577 - val_loss: 0.3123\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0565 - val_loss: 0.3030\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0540 - val_loss: 0.2978\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0588 - val_loss: 0.3133\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0553 - val_loss: 0.3277\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0538 - val_loss: 0.3193\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.0558 - val_loss: 0.2926\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0582 - val_loss: 0.3018\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0515 - val_loss: 0.2979\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0509 - val_loss: 0.3148\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0518 - val_loss: 0.3532\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0519 - val_loss: 0.2890\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0551 - val_loss: 0.2841\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0502 - val_loss: 0.3047\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0472 - val_loss: 0.2976\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0933 - val_loss: 0.3556\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0874 - val_loss: 0.2834\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0548 - val_loss: 0.2812\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0565 - val_loss: 0.3284\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0567 - val_loss: 0.3174\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0642 - val_loss: 0.2930\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0483 - val_loss: 0.3039\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0468 - val_loss: 0.2850\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0537 - val_loss: 0.2798\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0986 - val_loss: 0.3093\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0743 - val_loss: 0.2903\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0485 - val_loss: 0.3011\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0467 - val_loss: 0.2933\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0460 - val_loss: 0.3082\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0459 - val_loss: 0.2981\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0425 - val_loss: 0.2836\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0442 - val_loss: 0.2748\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0429 - val_loss: 0.2813\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0409 - val_loss: 0.2847\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0434 - val_loss: 0.2803\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0470 - val_loss: 0.3020\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0416 - val_loss: 0.2746\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0411 - val_loss: 0.2757\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0444 - val_loss: 0.2690\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0448 - val_loss: 0.3562\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0418 - val_loss: 0.3036\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0381 - val_loss: 0.3003\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0391 - val_loss: 0.2868\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0431 - val_loss: 0.3643\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0424 - val_loss: 0.3292\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0450 - val_loss: 0.2790\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0397 - val_loss: 0.3096\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0396 - val_loss: 0.3044\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0372 - val_loss: 0.2877\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0390 - val_loss: 0.2792\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0379 - val_loss: 0.2825\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0383 - val_loss: 0.3093\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0359 - val_loss: 0.3136\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0423 - val_loss: 0.2807\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0373 - val_loss: 0.2784\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0345 - val_loss: 0.2784\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0344 - val_loss: 0.3250\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0361 - val_loss: 0.2730\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0366 - val_loss: 0.3247\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0360 - val_loss: 0.3044\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0369 - val_loss: 0.2747\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0338 - val_loss: 0.2829\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0347 - val_loss: 0.3115\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0343 - val_loss: 0.2738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0344 - val_loss: 0.2797\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0329 - val_loss: 0.3088\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0341 - val_loss: 0.2900\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0323 - val_loss: 0.2866\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0348 - val_loss: 0.2836\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0341 - val_loss: 0.2800\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0328 - val_loss: 0.3525\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0360 - val_loss: 0.3250\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0327 - val_loss: 0.2974\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0344 - val_loss: 0.2991\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0342 - val_loss: 0.2770\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0327 - val_loss: 0.2986\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.0319 - val_loss: 0.2803\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0312 - val_loss: 0.2840\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0311 - val_loss: 0.3095\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0379 - val_loss: 0.3067\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0377 - val_loss: 0.2981\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0322 - val_loss: 0.3054\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0335 - val_loss: 0.2845\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0372 - val_loss: 0.3335\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0306 - val_loss: 0.2973\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0319 - val_loss: 0.2886\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0302 - val_loss: 0.3472\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0313 - val_loss: 0.3320\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0320 - val_loss: 0.2758\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0317 - val_loss: 0.3093\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0297 - val_loss: 0.2846\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0320 - val_loss: 0.3035\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0306 - val_loss: 0.2947\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0299 - val_loss: 0.3479\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0294 - val_loss: 0.3048\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0297 - val_loss: 0.3385\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0301 - val_loss: 0.2808\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0313 - val_loss: 0.3150\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0311 - val_loss: 0.2923\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0290 - val_loss: 0.2917\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0283 - val_loss: 0.2898\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0330 - val_loss: 0.2891\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0312 - val_loss: 0.2892\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0297 - val_loss: 0.3091\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0326 - val_loss: 0.3023\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0285 - val_loss: 0.2992\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0277 - val_loss: 0.2881\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0273 - val_loss: 0.2980\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0267 - val_loss: 0.2912\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0262 - val_loss: 0.3234\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0270 - val_loss: 0.2878\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0270 - val_loss: 0.2851\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0314 - val_loss: 0.3332\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0271 - val_loss: 0.2988\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0299 - val_loss: 0.2938\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0264 - val_loss: 0.3646\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0266 - val_loss: 0.3152\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0256 - val_loss: 0.3008\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0269 - val_loss: 0.3124\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0269 - val_loss: 0.2982\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0258 - val_loss: 0.2959\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0257 - val_loss: 0.3070\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0314 - val_loss: 0.2961\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0310 - val_loss: 0.3259\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0294 - val_loss: 0.3089\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0262 - val_loss: 0.3172\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0258 - val_loss: 0.3110\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0283 - val_loss: 0.3678\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0255 - val_loss: 0.2980\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0253 - val_loss: 0.3116\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0244 - val_loss: 0.3170\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0385 - val_loss: 0.3142\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0302 - val_loss: 0.2970\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0253 - val_loss: 0.3651\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0251 - val_loss: 0.3113\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0256 - val_loss: 0.3123\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0260 - val_loss: 0.3075\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0250 - val_loss: 0.2959\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0249 - val_loss: 0.3205\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0266 - val_loss: 0.3105\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0238 - val_loss: 0.3146\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0240 - val_loss: 0.3118\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0256 - val_loss: 0.3051\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0255 - val_loss: 0.3069\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0245 - val_loss: 0.3299\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0239 - val_loss: 0.3082\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0233 - val_loss: 0.3271\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0267 - val_loss: 0.3207\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0254 - val_loss: 0.3011\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0243 - val_loss: 0.3042\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0230 - val_loss: 0.2958\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0268 - val_loss: 0.3055\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0260 - val_loss: 0.4727\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0245 - val_loss: 0.3121\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0227 - val_loss: 0.3014\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0229 - val_loss: 0.3105\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0228 - val_loss: 0.3012\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0227 - val_loss: 0.3187\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0242 - val_loss: 0.3304\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0236 - val_loss: 0.3087\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0227 - val_loss: 0.3105\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0249 - val_loss: 0.3108\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0226 - val_loss: 0.3114\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0218 - val_loss: 0.3024\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0228 - val_loss: 0.3113\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0246 - val_loss: 0.3168\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0235 - val_loss: 0.3091\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0232 - val_loss: 0.3226\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0236 - val_loss: 0.3157\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0223 - val_loss: 0.3047\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0220 - val_loss: 0.3027\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0222 - val_loss: 0.3314\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0229 - val_loss: 0.3177\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0260 - val_loss: 0.3318\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0218 - val_loss: 0.3295\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0238 - val_loss: 0.3280\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0238 - val_loss: 0.3160\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0207 - val_loss: 0.3195\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0227 - val_loss: 0.3399\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0218 - val_loss: 0.3074\n"
     ]
    }
   ],
   "source": [
    "n_batches = 10\n",
    "batch_sizes = np.round(np.linspace(1,X_train_scaled.shape[0],n_batches))\n",
    "model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed1713f98>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Mini Batch=10')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 7s 704us/step - loss: 8.9151 - val_loss: 3.3800\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 1.5959 - val_loss: 1.4206\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.8059 - val_loss: 0.8264\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.4917 - val_loss: 0.5069\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 4s 396us/step - loss: 0.3576 - val_loss: 0.3857\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 395us/step - loss: 0.2846 - val_loss: 0.3162\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 4s 389us/step - loss: 0.2359 - val_loss: 0.2658\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.2016 - val_loss: 0.2344ETA - ETA: 0s  - ETA: 0s - loss: 0.20\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: 0.1795 - val_loss: 0.2135\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: 0.1639 - val_loss: 0.2006\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.1479 - val_loss: 0.1812\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.1380 - val_loss: 0.1726\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.1302 - val_loss: 0.1640\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.1211 - val_loss: 0.1547\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.1148 - val_loss: 0.1481\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.1096 - val_loss: 0.1428\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.1037 - val_loss: 0.1346\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.0985 - val_loss: 0.1323\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.0946 - val_loss: 0.1264\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.0905 - val_loss: 0.1228\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 0.0873 - val_loss: 0.1180\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.0843 - val_loss: 0.1168\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.0813 - val_loss: 0.1110\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 4s 383us/step - loss: 0.0777 - val_loss: 0.1097\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 4s 405us/step - loss: 0.0757 - val_loss: 0.1056\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 4s 389us/step - loss: 0.0738 - val_loss: 0.1067\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 0.0720 - val_loss: 0.1042\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 4s 390us/step - loss: 0.0700 - val_loss: 0.1033\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 0.0684 - val_loss: 0.1005\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 4s 389us/step - loss: 0.0673 - val_loss: 0.0981\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.0656 - val_loss: 0.0988\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 4s 400us/step - loss: 0.0644 - val_loss: 0.0990\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 4s 401us/step - loss: 0.0633 - val_loss: 0.0958\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 4s 389us/step - loss: 0.0622 - val_loss: 0.0936\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.0609 - val_loss: 0.0930\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.0600 - val_loss: 0.0933\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.0592 - val_loss: 0.0917\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: 0.0583 - val_loss: 0.0913\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 4s 395us/step - loss: 0.0571 - val_loss: 0.0903\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0563 - val_loss: 0.0888\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 0.0553 - val_loss: 0.0877\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 4s 397us/step - loss: 0.0548 - val_loss: 0.0895\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 0.0539 - val_loss: 0.0879\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 4s 359us/step - loss: 0.0533 - val_loss: 0.0882\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.0526 - val_loss: 0.0883\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 0.0519 - val_loss: 0.0857\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 4s 448us/step - loss: 0.0513 - val_loss: 0.0844\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 4s 441us/step - loss: 0.0508 - val_loss: 0.0846\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 4s 437us/step - loss: 0.0502 - val_loss: 0.0848\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.0495 - val_loss: 0.0841\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 4s 432us/step - loss: 0.0490 - val_loss: 0.0827\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0483 - val_loss: 0.0827\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 4s 424us/step - loss: 0.0480 - val_loss: 0.0825\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 4s 418us/step - loss: 0.0474 - val_loss: 0.0812\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.0469 - val_loss: 0.0814\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 4s 421us/step - loss: 0.0464 - val_loss: 0.0817\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 4s 441us/step - loss: 0.0461 - val_loss: 0.0797\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.0455 - val_loss: 0.0797\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.0453 - val_loss: 0.0791\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0448 - val_loss: 0.0787\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0445 - val_loss: 0.0802\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0439 - val_loss: 0.0782\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0436 - val_loss: 0.0776\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0432 - val_loss: 0.0766\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0429 - val_loss: 0.0770\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.0427 - val_loss: 0.0774\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0421 - val_loss: 0.0764\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0419 - val_loss: 0.0762\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0413 - val_loss: 0.0762\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0412 - val_loss: 0.0754\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0410 - val_loss: 0.0764\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 4s 405us/step - loss: 0.0405 - val_loss: 0.0752\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0403 - val_loss: 0.0741\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0399 - val_loss: 0.0745\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0397 - val_loss: 0.0740\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.0395 - val_loss: 0.0755\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0390 - val_loss: 0.0728\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0388 - val_loss: 0.0740\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0385 - val_loss: 0.0725\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.0383 - val_loss: 0.0729\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0379 - val_loss: 0.0725\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0377 - val_loss: 0.0726\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0375 - val_loss: 0.0729\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0373 - val_loss: 0.0715\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 0.0370 - val_loss: 0.0712\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0367 - val_loss: 0.0722l - \n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0365 - val_loss: 0.0711\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0364 - val_loss: 0.0707\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0361 - val_loss: 0.0704\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0359 - val_loss: 0.0707\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0357 - val_loss: 0.0695\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0354 - val_loss: 0.0706\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0353 - val_loss: 0.0693\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 285us/step - loss: 0.0350 - val_loss: 0.0696\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.0349 - val_loss: 0.0690\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0347 - val_loss: 0.0702\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0345 - val_loss: 0.0684\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0343 - val_loss: 0.0682\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0340 - val_loss: 0.0683\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0339 - val_loss: 0.0681\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0337 - val_loss: 0.0678\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0335 - val_loss: 0.0676\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0333 - val_loss: 0.0671\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0332 - val_loss: 0.0672\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0330 - val_loss: 0.0669\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0329 - val_loss: 0.0668\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0326 - val_loss: 0.0665\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0325 - val_loss: 0.0660\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0324 - val_loss: 0.0665\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0322 - val_loss: 0.0667\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0320 - val_loss: 0.0669\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0319 - val_loss: 0.0674\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0316 - val_loss: 0.0659\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0314 - val_loss: 0.0657\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.0314 - val_loss: 0.0656\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.0312 - val_loss: 0.0655\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.0311 - val_loss: 0.0659\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.0310 - val_loss: 0.0642\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.0308 - val_loss: 0.0649\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.0307 - val_loss: 0.0649\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0304 - val_loss: 0.0647\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0303 - val_loss: 0.0649\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0303 - val_loss: 0.0649\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0301 - val_loss: 0.0637\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.0299 - val_loss: 0.0633\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 0.0298 - val_loss: 0.0636\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0297 - val_loss: 0.0628\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0296 - val_loss: 0.0645\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0295 - val_loss: 0.0637\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0292 - val_loss: 0.0646\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0293 - val_loss: 0.0629\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0290 - val_loss: 0.0637\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0289 - val_loss: 0.0628\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0288 - val_loss: 0.0624\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0288 - val_loss: 0.0635\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0286 - val_loss: 0.0625\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0284 - val_loss: 0.0627\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0284 - val_loss: 0.0620\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0283 - val_loss: 0.0627\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0281 - val_loss: 0.0624\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0280 - val_loss: 0.0620\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0279 - val_loss: 0.0624\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0278 - val_loss: 0.0614\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0278 - val_loss: 0.0615\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0276 - val_loss: 0.0621\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0276 - val_loss: 0.0616\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0274 - val_loss: 0.0628\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0273 - val_loss: 0.0617\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0272 - val_loss: 0.0611\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.0272 - val_loss: 0.0613\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0270 - val_loss: 0.0620\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.0269 - val_loss: 0.0604\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0268 - val_loss: 0.0612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0267 - val_loss: 0.0617\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.0266 - val_loss: 0.0615\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0265 - val_loss: 0.0608\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0264 - val_loss: 0.0606\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0263 - val_loss: 0.0608\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0262 - val_loss: 0.0607\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0261 - val_loss: 0.0603\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.0260 - val_loss: 0.0601\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0260 - val_loss: 0.0605\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0259 - val_loss: 0.0601\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.0257 - val_loss: 0.0598\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0256 - val_loss: 0.0596\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0255 - val_loss: 0.0599\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0255 - val_loss: 0.0608\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0253 - val_loss: 0.0596\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.0253 - val_loss: 0.0600\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0253 - val_loss: 0.0591\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0252 - val_loss: 0.0593\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0251 - val_loss: 0.0591\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0249 - val_loss: 0.0592\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0249 - val_loss: 0.0587\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0246 - val_loss: 0.0588\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0246 - val_loss: 0.0592\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0246 - val_loss: 0.0588\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0245 - val_loss: 0.0590\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0244 - val_loss: 0.0587\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0244 - val_loss: 0.0587\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0242 - val_loss: 0.0595\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0242 - val_loss: 0.0586\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0241 - val_loss: 0.0581\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0240 - val_loss: 0.0578\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0239 - val_loss: 0.0586\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0238 - val_loss: 0.0574\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0238 - val_loss: 0.0583\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0238 - val_loss: 0.0573\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0236 - val_loss: 0.0579\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0236 - val_loss: 0.0573\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0235 - val_loss: 0.0579\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0234 - val_loss: 0.0583\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0234 - val_loss: 0.0576\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0233 - val_loss: 0.0578\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0234 - val_loss: 0.0578\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0231 - val_loss: 0.0572\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0232 - val_loss: 0.0574\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0231 - val_loss: 0.0571\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0230 - val_loss: 0.0572\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0229 - val_loss: 0.0577\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0228 - val_loss: 0.0570\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0228 - val_loss: 0.0570\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0227 - val_loss: 0.0573\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0226 - val_loss: 0.0568\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0226 - val_loss: 0.0572\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0225 - val_loss: 0.0564\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0224 - val_loss: 0.0561\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0224 - val_loss: 0.0562\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0223 - val_loss: 0.0563\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0224 - val_loss: 0.0563\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0222 - val_loss: 0.0561\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0221 - val_loss: 0.0559\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0221 - val_loss: 0.0565\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0220 - val_loss: 0.0564\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0219 - val_loss: 0.0563\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0219 - val_loss: 0.0558\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0219 - val_loss: 0.0556\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0217 - val_loss: 0.0558\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 4s 400us/step - loss: 0.0217 - val_loss: 0.0555\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 4s 390us/step - loss: 0.0216 - val_loss: 0.0557\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 4s 403us/step - loss: 0.0216 - val_loss: 0.0557\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.0215 - val_loss: 0.0551\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: 0.0215 - val_loss: 0.0555\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: 0.0214 - val_loss: 0.0554\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.0214 - val_loss: 0.0556\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 4s 446us/step - loss: 0.0213 - val_loss: 0.0550\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 4s 433us/step - loss: 0.0212 - val_loss: 0.0555\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0212 - val_loss: 0.0553\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 4s 436us/step - loss: 0.0211 - val_loss: 0.0551\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 4s 445us/step - loss: 0.0211 - val_loss: 0.0554\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 4s 443us/step - loss: 0.0211 - val_loss: 0.0548\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 4s 421us/step - loss: 0.0210 - val_loss: 0.0549 0s - loss: \n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 4s 451us/step - loss: 0.0209 - val_loss: 0.0549\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0208 - val_loss: 0.0555\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 5s 473us/step - loss: 0.0207 - val_loss: 0.0546\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0207 - val_loss: 0.0556\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.0208 - val_loss: 0.0550\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.0207 - val_loss: 0.0549\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0207 - val_loss: 0.0546\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.0206 - val_loss: 0.0545\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0205 - val_loss: 0.0543\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0205 - val_loss: 0.0546\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.0204 - val_loss: 0.0546\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 0.0203 - val_loss: 0.0544\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.0203 - val_loss: 0.0545\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0202 - val_loss: 0.0545\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0202 - val_loss: 0.0541\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0202 - val_loss: 0.0540\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.0202 - val_loss: 0.0539\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 4s 437us/step - loss: 0.0201 - val_loss: 0.0540\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "moptimizer = Adagrad(lr=0.01)\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Entrene los modelos obtenidos en b) y c) utilizando estrategias modernas para adaptar la tasa de aprendizaje. Compare los desempeños de adagrad, adadelta, RMSprop y adam. ¿Se observa en algún caso un mejor resultado final? ¿Se observa en algún caso una mayor velocidad de convergencia sobre el dataset de entrenamiento? ¿Sobre el dataset de validación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed0a229e8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Adagrad')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 404us/step - loss: 125.7632 - val_loss: 134.9639\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 112.2877 - val_loss: 120.4968\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 99.4564 - val_loss: 106.3188\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 86.9450 - val_loss: 92.8456\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 74.9932 - val_loss: 80.1790\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 63.8525 - val_loss: 68.2374\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 53.6008 - val_loss: 57.2280\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 44.3202 - val_loss: 47.3013\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 36.0867 - val_loss: 38.5816\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 28.9516 - val_loss: 30.9469\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 22.8630 - val_loss: 24.5235\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 17.8275 - val_loss: 19.1843\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 13.7702 - val_loss: 14.8955\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 10.5669 - val_loss: 11.5213\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 8.1140 - val_loss: 8.9337\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 6.3011 - val_loss: 7.0348\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 5.0051 - val_loss: 5.6735\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 4.0834 - val_loss: 4.6916\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 3.4109 - val_loss: 3.9787\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 2.9112 - val_loss: 3.4444\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 2.5326 - val_loss: 3.0288\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 2.2397 - val_loss: 2.6989\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 2.0076 - val_loss: 2.4342\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 1.8235 - val_loss: 2.2117\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 1.6629 - val_loss: 2.0220\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 1.5248 - val_loss: 1.8411\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 1.3972 - val_loss: 1.6846\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 1.2893 - val_loss: 1.5490\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 1.1938 - val_loss: 1.4270\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 1.1108 - val_loss: 1.3195\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 1.0356 - val_loss: 1.2233\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.9703 - val_loss: 1.1373\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.9105 - val_loss: 1.0546\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.8570 - val_loss: 0.9887\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.8120 - val_loss: 0.9285\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.7704 - val_loss: 0.8730\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.7319 - val_loss: 0.8244\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.6967 - val_loss: 0.7765\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.6629 - val_loss: 0.7284\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.6322 - val_loss: 0.6895\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.6052 - val_loss: 0.6554\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.5801 - val_loss: 0.6240\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.5573 - val_loss: 0.5924\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.5360 - val_loss: 0.5653\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.5169 - val_loss: 0.5417\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.4989 - val_loss: 0.5177\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.4817 - val_loss: 0.4953\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.4660 - val_loss: 0.4753\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.4514 - val_loss: 0.4569\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.4374 - val_loss: 0.4391\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.4245 - val_loss: 0.4270\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.4130 - val_loss: 0.4103\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.4019 - val_loss: 0.3976\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.3917 - val_loss: 0.3848\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.3822 - val_loss: 0.3742\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.3737 - val_loss: 0.3655\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.3649 - val_loss: 0.3553\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.3573 - val_loss: 0.3477\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.3499 - val_loss: 0.3394\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.3429 - val_loss: 0.3319\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.3365 - val_loss: 0.3248\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.3301 - val_loss: 0.3176\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.3239 - val_loss: 0.3101\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.3183 - val_loss: 0.3052\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.3129 - val_loss: 0.2992\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.3074 - val_loss: 0.2932\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.3027 - val_loss: 0.2886\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.2980 - val_loss: 0.2834\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.2933 - val_loss: 0.2783\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.2890 - val_loss: 0.2743\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.2846 - val_loss: 0.2705\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.2805 - val_loss: 0.2653\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.2764 - val_loss: 0.2603\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.2723 - val_loss: 0.2574\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.2684 - val_loss: 0.2540\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.2645 - val_loss: 0.2493\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.2611 - val_loss: 0.2465\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.2576 - val_loss: 0.2437\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.2546 - val_loss: 0.2407\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.2510 - val_loss: 0.2369\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.2481 - val_loss: 0.2344\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.2451 - val_loss: 0.2310\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.2420 - val_loss: 0.2278\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.2393 - val_loss: 0.2254\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.2366 - val_loss: 0.2222\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.2340 - val_loss: 0.2195\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.2313 - val_loss: 0.2180\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.2289 - val_loss: 0.2159\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.2261 - val_loss: 0.2143\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.2240 - val_loss: 0.2115\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.2218 - val_loss: 0.2088\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.2196 - val_loss: 0.2064\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.2173 - val_loss: 0.2046\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.2151 - val_loss: 0.2028\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.2129 - val_loss: 0.2004\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.2105 - val_loss: 0.1996\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.2086 - val_loss: 0.1964\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.2066 - val_loss: 0.1953\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.2048 - val_loss: 0.1940\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.2028 - val_loss: 0.1913\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.2011 - val_loss: 0.1902\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1991 - val_loss: 0.1892\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.1975 - val_loss: 0.1871\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.1955 - val_loss: 0.1853\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.1938 - val_loss: 0.1841\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1922 - val_loss: 0.1835\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1907 - val_loss: 0.1824\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.1891 - val_loss: 0.1802\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.1877 - val_loss: 0.1794\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.1860 - val_loss: 0.1784\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.1845 - val_loss: 0.1765\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.1829 - val_loss: 0.1757\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.1816 - val_loss: 0.1736\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.1802 - val_loss: 0.1731\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.1787 - val_loss: 0.1719\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.1773 - val_loss: 0.1708\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.1761 - val_loss: 0.1691\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.1746 - val_loss: 0.1686\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.1734 - val_loss: 0.1675\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.1722 - val_loss: 0.1666\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.1709 - val_loss: 0.1653\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.1698 - val_loss: 0.1641\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.1687 - val_loss: 0.1634\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.1674 - val_loss: 0.1624\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.1662 - val_loss: 0.1617\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.1651 - val_loss: 0.1614\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.1640 - val_loss: 0.1599\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.1629 - val_loss: 0.1590\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.1619 - val_loss: 0.1576\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.1605 - val_loss: 0.1579\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.1595 - val_loss: 0.1567\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.1586 - val_loss: 0.1554\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.1575 - val_loss: 0.1545\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 0.1565 - val_loss: 0.1537\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.1555 - val_loss: 0.1531\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.1546 - val_loss: 0.1526\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.1537 - val_loss: 0.1514\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1527 - val_loss: 0.1507\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.1518 - val_loss: 0.1497\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.1509 - val_loss: 0.1492\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.1499 - val_loss: 0.1481\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.1490 - val_loss: 0.1479\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.1480 - val_loss: 0.1468\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.1471 - val_loss: 0.1468\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.1463 - val_loss: 0.1457\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.1455 - val_loss: 0.1452\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.1446 - val_loss: 0.1443\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.1438 - val_loss: 0.1431\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.1428 - val_loss: 0.1435\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.1421 - val_loss: 0.1425\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.1414 - val_loss: 0.1413\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.1404 - val_loss: 0.1403oss:  - ETA: 0s - loss: 0.1\n",
      "Epoch 153/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.1397 - val_loss: 0.1402\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.1390 - val_loss: 0.1396\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 0.1382 - val_loss: 0.1390\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.1374 - val_loss: 0.1387\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1366 - val_loss: 0.1384\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.1360 - val_loss: 0.1380\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.1351 - val_loss: 0.1372\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.1345 - val_loss: 0.1364\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.1338 - val_loss: 0.1356\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.1331 - val_loss: 0.1352\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.1324 - val_loss: 0.1343\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.1318 - val_loss: 0.1336\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.1311 - val_loss: 0.1333\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.1304 - val_loss: 0.1330\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.1297 - val_loss: 0.1322\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.1292 - val_loss: 0.1317\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.1285 - val_loss: 0.1313\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.1279 - val_loss: 0.1309\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.1272 - val_loss: 0.1299\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.1266 - val_loss: 0.1295\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.1260 - val_loss: 0.1291\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.1253 - val_loss: 0.1286\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.1249 - val_loss: 0.1284\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.1242 - val_loss: 0.1277\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.1236 - val_loss: 0.1274\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.1230 - val_loss: 0.1270\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.1225 - val_loss: 0.1266\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.1219 - val_loss: 0.1264\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.1213 - val_loss: 0.1262\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.1208 - val_loss: 0.1258\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.1203 - val_loss: 0.1248\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.1197 - val_loss: 0.1247\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.1191 - val_loss: 0.1243\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.1185 - val_loss: 0.1237\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.1181 - val_loss: 0.1232\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1175 - val_loss: 0.1231\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.1170 - val_loss: 0.1223\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.1164 - val_loss: 0.1218\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.1159 - val_loss: 0.1216\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.1155 - val_loss: 0.1215\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.1150 - val_loss: 0.1207\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.1144 - val_loss: 0.1204\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.1140 - val_loss: 0.1202\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.1136 - val_loss: 0.1196\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.1131 - val_loss: 0.1192\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.1126 - val_loss: 0.1189\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.1121 - val_loss: 0.1185\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.1116 - val_loss: 0.1178\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.1111 - val_loss: 0.1178\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.1107 - val_loss: 0.1171\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.1102 - val_loss: 0.1167\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.1097 - val_loss: 0.1164\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.1092 - val_loss: 0.1168\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.1088 - val_loss: 0.1155\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.1084 - val_loss: 0.1152\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.1079 - val_loss: 0.1151\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.1076 - val_loss: 0.1147\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.1070 - val_loss: 0.1149\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.1067 - val_loss: 0.1142\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.1063 - val_loss: 0.1134\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.1058 - val_loss: 0.1131\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.1054 - val_loss: 0.1128\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.1051 - val_loss: 0.1127\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.1046 - val_loss: 0.1124\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.1042 - val_loss: 0.1119\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.1038 - val_loss: 0.1119\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.1035 - val_loss: 0.1115\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.1030 - val_loss: 0.1113\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.1026 - val_loss: 0.1109\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.1023 - val_loss: 0.1103\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.1019 - val_loss: 0.1102\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.1015 - val_loss: 0.1096\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.1011 - val_loss: 0.1094\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.1007 - val_loss: 0.1092\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.1004 - val_loss: 0.1087\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.1000 - val_loss: 0.1086\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.0996 - val_loss: 0.1082\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.0993 - val_loss: 0.1079\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0989 - val_loss: 0.1075\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0986 - val_loss: 0.1076\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0982 - val_loss: 0.1072\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0978 - val_loss: 0.1065\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 285us/step - loss: 0.0975 - val_loss: 0.1066\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.0972 - val_loss: 0.1061\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.0968 - val_loss: 0.1060\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0965 - val_loss: 0.1056\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0961 - val_loss: 0.1053\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.0958 - val_loss: 0.1053\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.0955 - val_loss: 0.1050\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0952 - val_loss: 0.1046\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0948 - val_loss: 0.1042\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0945 - val_loss: 0.1040\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0942 - val_loss: 0.1041\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0939 - val_loss: 0.1038\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0936 - val_loss: 0.1034\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0932 - val_loss: 0.1032\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0929 - val_loss: 0.1028\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0926 - val_loss: 0.1029\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "moptimizer = Adadelta(lr=0.01)\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20eb5785438>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Adadelta')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 3.2626 - val_loss: 0.9879\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.7712 - val_loss: 0.3843\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.6224 - val_loss: 0.4103\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.5638 - val_loss: 0.3372\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.5506 - val_loss: 0.7248\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.4798 - val_loss: 0.2431\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.4673 - val_loss: 0.2959\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.4581 - val_loss: 0.2777\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.4385 - val_loss: 0.6839\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.4286 - val_loss: 1.1514\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.4057 - val_loss: 0.4034\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.4202 - val_loss: 0.2306\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.4203 - val_loss: 0.3606\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.4136 - val_loss: 0.6199\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.4043 - val_loss: 0.3444\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.3722 - val_loss: 1.1098\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.3843 - val_loss: 0.6886\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.3766 - val_loss: 0.2120\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.3741 - val_loss: 0.2581\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.3836 - val_loss: 0.3072\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.3688 - val_loss: 0.3811\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.3514 - val_loss: 0.4630\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.3611 - val_loss: 0.4882\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.3645 - val_loss: 0.5744\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.3597 - val_loss: 0.6285\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.3442 - val_loss: 0.9400\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.3514 - val_loss: 0.4583\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.3509 - val_loss: 0.3381\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.3300 - val_loss: 0.3407\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.3457 - val_loss: 0.2639\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.3374 - val_loss: 0.2164\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.3302 - val_loss: 0.1994\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.3251 - val_loss: 0.8915\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.3280 - val_loss: 0.7984\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.3208 - val_loss: 0.3494\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.3230 - val_loss: 0.1840\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.3300 - val_loss: 0.3209\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.3270 - val_loss: 0.6542\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.3090 - val_loss: 0.3406\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.3226 - val_loss: 1.4529\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.3042 - val_loss: 0.2924\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.3200 - val_loss: 0.5129\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.3164 - val_loss: 0.2099\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.3298 - val_loss: 0.4488\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.3208 - val_loss: 0.3503\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.3190 - val_loss: 0.3458\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.3273 - val_loss: 0.3528\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.3048 - val_loss: 0.3156\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.3066 - val_loss: 0.2436\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.3209 - val_loss: 0.4028\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.3069 - val_loss: 0.1738\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.3068 - val_loss: 0.2307\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.3090 - val_loss: 0.5495\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.3019 - val_loss: 0.2744\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.3013 - val_loss: 0.2755\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2971 - val_loss: 0.1705\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2813 - val_loss: 0.3781\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2991 - val_loss: 0.3995\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.3110 - val_loss: 0.5224\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2910 - val_loss: 0.5183\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2923 - val_loss: 0.4928\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.3067 - val_loss: 0.3197\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2940 - val_loss: 0.3668\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2908 - val_loss: 0.1742\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2806 - val_loss: 0.2928\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2848 - val_loss: 0.2909\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2841 - val_loss: 0.1797\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2763 - val_loss: 0.8457\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2858 - val_loss: 0.7419\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2875 - val_loss: 1.1703\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2948 - val_loss: 0.2306\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2884 - val_loss: 0.4328\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2919 - val_loss: 0.8125\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2960 - val_loss: 0.5599\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2796 - val_loss: 0.1981\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2845 - val_loss: 0.1809\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2731 - val_loss: 0.5015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2956 - val_loss: 0.2536\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2698 - val_loss: 0.1877\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2709 - val_loss: 0.2878\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2872 - val_loss: 0.2116\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2776 - val_loss: 0.3702\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2788 - val_loss: 0.1604\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2892 - val_loss: 0.2120\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2799 - val_loss: 0.1550\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2799 - val_loss: 0.5875\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2679 - val_loss: 0.2375\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2887 - val_loss: 0.1770\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2647 - val_loss: 0.1533\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.2731 - val_loss: 0.4135\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.2661 - val_loss: 0.3163\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2787 - val_loss: 0.5929\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.2751 - val_loss: 0.1784\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2678 - val_loss: 0.5080\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.2707 - val_loss: 0.1841\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.2901 - val_loss: 0.3166\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.2651 - val_loss: 0.6049\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.2620 - val_loss: 0.1380\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.2780 - val_loss: 0.2927\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2791 - val_loss: 0.2196\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2646 - val_loss: 0.2029\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.2786 - val_loss: 0.2678\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2773 - val_loss: 0.3939\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2834 - val_loss: 0.1763\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2796 - val_loss: 1.4377\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2684 - val_loss: 0.2822\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2781 - val_loss: 0.1708\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2690 - val_loss: 0.1580\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.2738 - val_loss: 0.4136\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.2784 - val_loss: 0.3262\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2872 - val_loss: 0.1597\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2793 - val_loss: 0.1432\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.2770 - val_loss: 0.2433\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2666 - val_loss: 0.6028\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2779 - val_loss: 0.6891\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2698 - val_loss: 0.8098\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.2687 - val_loss: 0.4143\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.2702 - val_loss: 0.6917\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2717 - val_loss: 1.2455\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2608 - val_loss: 0.1473\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.2620 - val_loss: 0.8034\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.2575 - val_loss: 0.9474\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.2582 - val_loss: 0.1186\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2564 - val_loss: 0.2146\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.2599 - val_loss: 0.3211\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2636 - val_loss: 0.3491\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.2494 - val_loss: 0.1299\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2526 - val_loss: 0.1741\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2645 - val_loss: 0.1591\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2486 - val_loss: 0.1861\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.2563 - val_loss: 0.3652\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2533 - val_loss: 0.1882\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.2584 - val_loss: 0.3695\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2436 - val_loss: 0.3654\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2654 - val_loss: 0.1726\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2454 - val_loss: 0.1526\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2494 - val_loss: 0.2568\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.2719 - val_loss: 0.1315\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2368 - val_loss: 0.2081\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.2444 - val_loss: 0.3121\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.2531 - val_loss: 0.1995\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.2348 - val_loss: 0.3680\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2523 - val_loss: 0.2177\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.2377 - val_loss: 0.1939\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.2409 - val_loss: 0.1365\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.2411 - val_loss: 0.1304\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.2399 - val_loss: 0.2314\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2445 - val_loss: 0.1687\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.2387 - val_loss: 0.1855\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.2534 - val_loss: 0.1547\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.2555 - val_loss: 0.1835\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.2509 - val_loss: 0.1447\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.2430 - val_loss: 0.2200\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.2306 - val_loss: 0.1239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.2373 - val_loss: 0.5218\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2397 - val_loss: 0.1785\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.2470 - val_loss: 0.2222\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.2497 - val_loss: 0.1314\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.2450 - val_loss: 0.1299\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.2527 - val_loss: 0.1942\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.2556 - val_loss: 0.2103\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.2482 - val_loss: 0.1689\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.2498 - val_loss: 0.1393\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.2353 - val_loss: 0.2523\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2355 - val_loss: 0.5517\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.2601 - val_loss: 0.1184\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.2488 - val_loss: 0.1275\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.2470 - val_loss: 0.1231\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.2289 - val_loss: 0.2677\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.2418 - val_loss: 0.6293\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.2401 - val_loss: 0.1403\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.2347 - val_loss: 0.3457\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.2436 - val_loss: 0.1669\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2536 - val_loss: 0.1465\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.2389 - val_loss: 0.1507\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.2459 - val_loss: 0.1424\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2418 - val_loss: 0.1616\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2451 - val_loss: 0.1775\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2448 - val_loss: 0.1983\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2402 - val_loss: 0.4084\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2435 - val_loss: 0.2435\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2421 - val_loss: 0.1557\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.2440 - val_loss: 0.1449\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.2377 - val_loss: 0.1952\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.2468 - val_loss: 0.1442\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.2463 - val_loss: 0.3279\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.2473 - val_loss: 0.1275\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.2390 - val_loss: 0.1726\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2317 - val_loss: 0.6266\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.2282 - val_loss: 0.1712\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.2461 - val_loss: 0.1453\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2422 - val_loss: 0.1708\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2296 - val_loss: 0.4586\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2479 - val_loss: 0.2176\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2517 - val_loss: 0.1310\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2387 - val_loss: 0.5801\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2372 - val_loss: 0.7073\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2434 - val_loss: 0.2452\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2424 - val_loss: 0.4626\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2323 - val_loss: 0.1991\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2449 - val_loss: 0.1239\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2290 - val_loss: 0.1245\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2343 - val_loss: 0.1265\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2274 - val_loss: 0.2468\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2425 - val_loss: 0.4003\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2369 - val_loss: 0.4616\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2353 - val_loss: 0.1326\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.2389 - val_loss: 0.1470\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2344 - val_loss: 0.1168\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2374 - val_loss: 0.2495\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2323 - val_loss: 0.1967\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2351 - val_loss: 0.2013\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2321 - val_loss: 0.1923\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.2244 - val_loss: 0.1672\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2286 - val_loss: 0.1665\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2375 - val_loss: 0.3729\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2405 - val_loss: 0.4666\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2410 - val_loss: 0.1509\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2391 - val_loss: 0.2923\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2356 - val_loss: 0.4867\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2226 - val_loss: 0.6314\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.2447 - val_loss: 0.1906\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2367 - val_loss: 0.2535\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.2345 - val_loss: 0.1301\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2346 - val_loss: 0.1219\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2276 - val_loss: 0.1217\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2368 - val_loss: 0.3629\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2360 - val_loss: 0.7609\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2406 - val_loss: 0.2646\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2240 - val_loss: 0.1345\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.2309 - val_loss: 0.2128\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2207 - val_loss: 0.1455\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2400 - val_loss: 0.1723\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.2228 - val_loss: 0.1044\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2293 - val_loss: 0.1702\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2238 - val_loss: 0.1951\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.2392 - val_loss: 0.1485\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2188 - val_loss: 0.2847\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2355 - val_loss: 0.1558\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2318 - val_loss: 0.3304\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2256 - val_loss: 0.1426\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2252 - val_loss: 0.1214\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2369 - val_loss: 0.1193\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2212 - val_loss: 0.2057\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2314 - val_loss: 0.1900\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2321 - val_loss: 0.4154\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2258 - val_loss: 0.3866\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2392 - val_loss: 0.2667\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.2184 - val_loss: 0.1876\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2299 - val_loss: 0.2667\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "moptimizer = RMSprop(lr=0.01)\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed2c4a4e0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and RMSprop')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 3.8499 - val_loss: 0.2792\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2102 - val_loss: 0.2032\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.1835 - val_loss: 0.1787\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.1758 - val_loss: 0.1811\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.1476 - val_loss: 0.1516\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.1479 - val_loss: 0.1594\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.1525 - val_loss: 0.1665\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.1473 - val_loss: 0.2066\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.1546 - val_loss: 0.1711\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.1781 - val_loss: 0.2551\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.1805 - val_loss: 0.2533\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.1831 - val_loss: 0.1872\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.1833 - val_loss: 0.2410\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.1977 - val_loss: 0.1794\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.1973 - val_loss: 0.2194\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.2110 - val_loss: 0.2584\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.2285 - val_loss: 0.2238\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.2298 - val_loss: 0.2844\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.2389 - val_loss: 0.2290\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.2200 - val_loss: 0.2497\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2447 - val_loss: 0.2564\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.2064 - val_loss: 0.2599\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.1971 - val_loss: 0.2534\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.2143 - val_loss: 0.1990\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.1914 - val_loss: 0.1970\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.1906 - val_loss: 0.1850\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.1917 - val_loss: 0.2474\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.2059 - val_loss: 0.2598\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.2342 - val_loss: 0.2152\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.1971 - val_loss: 0.2197\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.2141 - val_loss: 0.2217\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.2092 - val_loss: 0.2210\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.1951 - val_loss: 0.2312\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.2176 - val_loss: 0.1780\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.1820 - val_loss: 0.2618\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.1743 - val_loss: 0.1783\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.1791 - val_loss: 0.2185\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.1684 - val_loss: 0.1927\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.1706 - val_loss: 0.2860\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.2305 - val_loss: 0.2186\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.2041 - val_loss: 0.1944\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 0.1971 - val_loss: 0.2653\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.2052 - val_loss: 0.1822\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.1754 - val_loss: 0.2278\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.1974 - val_loss: 0.2077\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 0.1996 - val_loss: 0.2279\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.2043 - val_loss: 0.1802\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.1738 - val_loss: 0.1933\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.1570 - val_loss: 0.2074\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.1815 - val_loss: 0.2389\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.1848 - val_loss: 0.1893\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.1628 - val_loss: 0.2642\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.1888 - val_loss: 0.2061\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.2028 - val_loss: 0.2445\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.1769 - val_loss: 0.2251\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.1995 - val_loss: 0.2887\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 0.2171 - val_loss: 0.3127\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.1780 - val_loss: 0.1827\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.1793 - val_loss: 0.1944\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.1675 - val_loss: 0.2142\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.1643 - val_loss: 0.1741\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1729 - val_loss: 0.1887\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1572 - val_loss: 0.1783\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1552 - val_loss: 0.1672\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1426 - val_loss: 0.1810\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.1645 - val_loss: 0.2112\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1579 - val_loss: 0.1894\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1493 - val_loss: 0.1717\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.1703 - val_loss: 0.1947\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.2044 - val_loss: 0.2263\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1899 - val_loss: 0.1647\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.1521 - val_loss: 0.1767\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.1571 - val_loss: 0.3025\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.1812 - val_loss: 0.1833\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.1774 - val_loss: 0.1714\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.1595 - val_loss: 0.2161\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1599 - val_loss: 0.1818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.1492 - val_loss: 0.1728\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.1463 - val_loss: 0.1545\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1459 - val_loss: 0.1824\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.1448 - val_loss: 0.1513\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.1709 - val_loss: 0.1930\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.1615 - val_loss: 0.1520\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1496 - val_loss: 0.1833\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1409 - val_loss: 0.1517\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.1558 - val_loss: 0.1507\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.1363 - val_loss: 0.1737\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.1422 - val_loss: 0.1389\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.1122 - val_loss: 0.1443\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.1335 - val_loss: 0.1956\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.1438 - val_loss: 0.1575\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1585 - val_loss: 0.2504\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.1401 - val_loss: 0.1607\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.1466 - val_loss: 0.2086\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.1470 - val_loss: 0.1431\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.1604 - val_loss: 0.1954\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1591 - val_loss: 0.1731\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.1580 - val_loss: 0.1728\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.1332 - val_loss: 0.1437\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1542 - val_loss: 0.1536\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1283 - val_loss: 0.1784\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.1399 - val_loss: 0.1905\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.1747 - val_loss: 0.1985\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.1536 - val_loss: 0.1830\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.1717 - val_loss: 0.1730\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.1525 - val_loss: 0.1814\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.1785 - val_loss: 0.1903\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1960 - val_loss: 0.2243\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.2044 - val_loss: 0.1943\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1666 - val_loss: 0.2100\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.1664 - val_loss: 0.1881\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1573 - val_loss: 0.2424\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1547 - val_loss: 0.1729\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1676 - val_loss: 0.1970\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1574 - val_loss: 0.1956\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1756 - val_loss: 0.1784\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.1650 - val_loss: 0.2909\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1600 - val_loss: 0.2309\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1724 - val_loss: 0.1966\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.2077 - val_loss: 0.1678\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1791 - val_loss: 0.2505\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.1629 - val_loss: 0.1524\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.1332 - val_loss: 0.1815\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.1485 - val_loss: 0.1431\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.1548 - val_loss: 0.2202\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.1563 - val_loss: 0.1673\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 285us/step - loss: 0.1508 - val_loss: 0.2149\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.1644 - val_loss: 0.1702\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.1508 - val_loss: 0.1873\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.1406 - val_loss: 0.1571\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.1303 - val_loss: 0.1283\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.1355 - val_loss: 0.1555\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.1540 - val_loss: 0.1382\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.1579 - val_loss: 0.1852\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.1406 - val_loss: 0.1851\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.1560 - val_loss: 0.1857\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.1469 - val_loss: 0.1607\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.1433 - val_loss: 0.1446\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.1236 - val_loss: 0.1712\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.1462 - val_loss: 0.1487\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.1501 - val_loss: 0.1890\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.1586 - val_loss: 0.1959\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.1554 - val_loss: 0.1849\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1646 - val_loss: 0.2649\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.1731 - val_loss: 0.1454\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1468 - val_loss: 0.2039\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1593 - val_loss: 0.1855\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1323 - val_loss: 0.1516\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1316 - val_loss: 0.1406\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.1384 - val_loss: 0.1325\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1542 - val_loss: 0.1380\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1502 - val_loss: 0.1783\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.1605 - val_loss: 0.1424\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1462 - val_loss: 0.1663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.1612 - val_loss: 0.1682\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1669 - val_loss: 0.1854\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.1538 - val_loss: 0.1803\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.1383 - val_loss: 0.1484\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.1494 - val_loss: 0.1660\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.1444 - val_loss: 0.1384\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1397 - val_loss: 0.1559\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.1240 - val_loss: 0.1232\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.1182 - val_loss: 0.2152\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.1397 - val_loss: 0.1480\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1415 - val_loss: 0.1664\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1241 - val_loss: 0.1443\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1325 - val_loss: 0.1703\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1347 - val_loss: 0.1741\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.1264 - val_loss: 0.3036\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1332 - val_loss: 0.1413\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.1249 - val_loss: 0.1354\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.1120 - val_loss: 0.1264\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.1233 - val_loss: 0.1612\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.1515 - val_loss: 0.1671\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.1374 - val_loss: 0.2057\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1302 - val_loss: 0.1411\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.1394 - val_loss: 0.1536\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.1293 - val_loss: 0.1636\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.1292 - val_loss: 0.1137\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.1230 - val_loss: 0.1344\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.1197 - val_loss: 0.1601\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.1146 - val_loss: 0.1176\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1137 - val_loss: 0.1422\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1204 - val_loss: 0.1429\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.1394 - val_loss: 0.1727\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.1253 - val_loss: 0.1349\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.1240 - val_loss: 0.1313\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1165 - val_loss: 0.1046\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.1183 - val_loss: 0.1143\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1134 - val_loss: 0.1132\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.1293 - val_loss: 0.1279\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.1318 - val_loss: 0.1532\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.1386 - val_loss: 0.1647\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.1281 - val_loss: 0.1245\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.1264 - val_loss: 0.2265\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 0.1286 - val_loss: 0.1292\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.1200 - val_loss: 0.1248\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1139 - val_loss: 0.1232\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.1130 - val_loss: 0.1727\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.1279 - val_loss: 0.1568\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1116 - val_loss: 0.1184\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.1395 - val_loss: 0.2323\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1266 - val_loss: 0.1284\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.1260 - val_loss: 0.1722\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.1266 - val_loss: 0.1686\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.1560 - val_loss: 0.1321\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.1398 - val_loss: 0.1917\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.1343 - val_loss: 0.1598\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.1268 - val_loss: 0.1473\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.1360 - val_loss: 0.1218\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1306 - val_loss: 0.1348\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.1209 - val_loss: 0.1842\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.1251 - val_loss: 0.1488\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1419 - val_loss: 0.1614\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.1316 - val_loss: 0.1551\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1238 - val_loss: 0.1624\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.1342 - val_loss: 0.1708\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.1478 - val_loss: 0.1966\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.1360 - val_loss: 0.1329ETA: 0s\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.1229 - val_loss: 0.1514\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.1220 - val_loss: 0.1389\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1149 - val_loss: 0.1320\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.1304 - val_loss: 0.1423\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1398 - val_loss: 0.1259\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.1274 - val_loss: 0.1248\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.1338 - val_loss: 0.1540\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.1364 - val_loss: 0.1519\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1333 - val_loss: 0.1794\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.1463 - val_loss: 0.1656\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.1376 - val_loss: 0.1647\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.1291 - val_loss: 0.1536\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1199 - val_loss: 0.1413\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.1307 - val_loss: 0.1641\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.1106 - val_loss: 0.1255\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.1051 - val_loss: 0.1388\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1220 - val_loss: 0.1721\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1295 - val_loss: 0.1828\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1587 - val_loss: 0.2008\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1438 - val_loss: 0.1694\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.1397 - val_loss: 0.1537\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.1479 - val_loss: 0.1493\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - ETA: 0s - loss: 0.171 - 2s 218us/step - loss: 0.1707 - val_loss: 0.1444\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.1310 - val_loss: 0.1717\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.1237 - val_loss: 0.1422\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.1194 - val_loss: 0.1190\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1092 - val_loss: 0.1189\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.1058 - val_loss: 0.1245\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.1074 - val_loss: 0.1333\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.1218 - val_loss: 0.1330\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.1272 - val_loss: 0.1731A: 1s -  - ETA: 0s - los\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "moptimizer = Adam(lr=0.01)\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed4238940>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Adam')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 18.1601 - val_loss: 9.5480\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 5.1231 - val_loss: 5.7848\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 3.1306 - val_loss: 4.6532\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 2.3565 - val_loss: 3.7023\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 1.8822 - val_loss: 3.2866\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 1.5836 - val_loss: 3.0405\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 1.3663 - val_loss: 2.5880\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 1.1845 - val_loss: 2.4979\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 1.0717 - val_loss: 2.3786\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.9769 - val_loss: 2.1544\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.8907 - val_loss: 2.1550\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.8475 - val_loss: 2.1306\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.8084 - val_loss: 1.9671\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.7554 - val_loss: 1.9493\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.7340 - val_loss: 1.8810\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.6972 - val_loss: 1.8095\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.6668 - val_loss: 1.7590\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.6408 - val_loss: 1.7377\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.6249 - val_loss: 1.7202\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.6014 - val_loss: 1.6737\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.5842 - val_loss: 1.6783\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.5677 - val_loss: 1.6382\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.5538 - val_loss: 1.5851\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.5389 - val_loss: 1.5626\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.5238 - val_loss: 1.5455\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.5150 - val_loss: 1.5370\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.5020 - val_loss: 1.5299\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.4946 - val_loss: 1.4785\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.4840 - val_loss: 1.4724\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.4782 - val_loss: 1.4468\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.4630 - val_loss: 1.4132\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.4550 - val_loss: 1.4453\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.4428 - val_loss: 1.4092\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.4387 - val_loss: 1.3777\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.4288 - val_loss: 1.3927\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.4224 - val_loss: 1.3625\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.4178 - val_loss: 1.3361\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.4099 - val_loss: 1.3353\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.4024 - val_loss: 1.3290\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3982 - val_loss: 1.3043\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.3920 - val_loss: 1.2871\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.3864 - val_loss: 1.2949\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.3807 - val_loss: 1.2972\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3755 - val_loss: 1.2760\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.3682 - val_loss: 1.2727\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.3655 - val_loss: 1.2329\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.3604 - val_loss: 1.2211\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.3549 - val_loss: 1.2339\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.3511 - val_loss: 1.1977\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.3471 - val_loss: 1.2037\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.3433 - val_loss: 1.1951\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.3398 - val_loss: 1.1952\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.3342 - val_loss: 1.1651\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.3316 - val_loss: 1.1780\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.3270 - val_loss: 1.1674\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.3236 - val_loss: 1.1706\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3201 - val_loss: 1.1465\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.3157 - val_loss: 1.1480\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.3145 - val_loss: 1.1477\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.3102 - val_loss: 1.1344\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.3061 - val_loss: 1.1458\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3037 - val_loss: 1.1193\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.3015 - val_loss: 1.1039\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2967 - val_loss: 1.1270\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.2942 - val_loss: 1.1195\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2919 - val_loss: 1.0999\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.2876 - val_loss: 1.0825\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2867 - val_loss: 1.0819\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2831 - val_loss: 1.0716\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.2812 - val_loss: 1.0642\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2782 - val_loss: 1.0709\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.2754 - val_loss: 1.0553\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2735 - val_loss: 1.0501\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2707 - val_loss: 1.0507\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2678 - val_loss: 1.0405\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2668 - val_loss: 1.0420\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2643 - val_loss: 1.0317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2607 - val_loss: 1.0467\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2597 - val_loss: 1.0180\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2584 - val_loss: 1.0140\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2562 - val_loss: 1.0114\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2540 - val_loss: 1.0008\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2523 - val_loss: 1.0130\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.2504 - val_loss: 0.9971\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.2486 - val_loss: 0.9909\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.2465 - val_loss: 1.0046\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.2434 - val_loss: 0.9744\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.2423 - val_loss: 0.9955\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2406 - val_loss: 0.9849\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2394 - val_loss: 0.9921\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2374 - val_loss: 0.9768\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2356 - val_loss: 0.9580\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.2343 - val_loss: 0.9653\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2324 - val_loss: 0.9658\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2306 - val_loss: 0.9497\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2284 - val_loss: 0.9610\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2268 - val_loss: 0.9588\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2256 - val_loss: 0.9343\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2245 - val_loss: 0.9401\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2225 - val_loss: 0.9390\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2211 - val_loss: 0.9503\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2197 - val_loss: 0.9152\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2185 - val_loss: 0.9205\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2175 - val_loss: 0.9190\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2151 - val_loss: 0.9102\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2141 - val_loss: 0.9256\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2128 - val_loss: 0.9106\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2108 - val_loss: 0.9119\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2104 - val_loss: 0.8967\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2081 - val_loss: 0.9125\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2073 - val_loss: 0.8966\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2060 - val_loss: 0.8911\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2051 - val_loss: 0.8878\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2034 - val_loss: 0.8802\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2022 - val_loss: 0.8915\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2011 - val_loss: 0.8734\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2002 - val_loss: 0.8874\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1988 - val_loss: 0.8861\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1974 - val_loss: 0.8800\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1962 - val_loss: 0.8997\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1950 - val_loss: 0.8635\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1938 - val_loss: 0.8782\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1932 - val_loss: 0.8545\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1921 - val_loss: 0.8639\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1913 - val_loss: 0.8532\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1898 - val_loss: 0.8606\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1887 - val_loss: 0.8571\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1880 - val_loss: 0.8457\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1875 - val_loss: 0.8479\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1863 - val_loss: 0.8438\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1850 - val_loss: 0.8544\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1842 - val_loss: 0.8465\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1834 - val_loss: 0.8395\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1819 - val_loss: 0.8555\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1815 - val_loss: 0.8308\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1803 - val_loss: 0.8150\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1796 - val_loss: 0.8210\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1789 - val_loss: 0.8263\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1778 - val_loss: 0.8277\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1767 - val_loss: 0.8150\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1760 - val_loss: 0.8217\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1749 - val_loss: 0.8136\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1742 - val_loss: 0.8138\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1732 - val_loss: 0.7986\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1728 - val_loss: 0.8081\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1720 - val_loss: 0.8044\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1710 - val_loss: 0.7999\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1704 - val_loss: 0.7968\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1693 - val_loss: 0.8091\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1689 - val_loss: 0.7978\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1679 - val_loss: 0.7915\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1667 - val_loss: 0.7886\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1664 - val_loss: 0.7888\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1658 - val_loss: 0.7881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1646 - val_loss: 0.7821\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1641 - val_loss: 0.7850\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1631 - val_loss: 0.7793\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1631 - val_loss: 0.7806\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1618 - val_loss: 0.7712\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1615 - val_loss: 0.7797\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1608 - val_loss: 0.7687\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1601 - val_loss: 0.7652\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1593 - val_loss: 0.7726\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1584 - val_loss: 0.7666\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1580 - val_loss: 0.7730\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1573 - val_loss: 0.7728\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1567 - val_loss: 0.7561\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1560 - val_loss: 0.7581\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1551 - val_loss: 0.7670\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1549 - val_loss: 0.7581\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1541 - val_loss: 0.7612\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1537 - val_loss: 0.7573\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1531 - val_loss: 0.7537\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1525 - val_loss: 0.7505\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1522 - val_loss: 0.7468\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1512 - val_loss: 0.7397\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1507 - val_loss: 0.7448\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1501 - val_loss: 0.7481\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1498 - val_loss: 0.7376\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1489 - val_loss: 0.7365\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1484 - val_loss: 0.7380\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1480 - val_loss: 0.7421\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1469 - val_loss: 0.7301\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1465 - val_loss: 0.7348\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1463 - val_loss: 0.7426\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1456 - val_loss: 0.7345\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1452 - val_loss: 0.7363\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1449 - val_loss: 0.7311\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1443 - val_loss: 0.7298\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1438 - val_loss: 0.7270\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1432 - val_loss: 0.7242\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1428 - val_loss: 0.7276\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1421 - val_loss: 0.7214\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1414 - val_loss: 0.7240\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1412 - val_loss: 0.7149\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1407 - val_loss: 0.7209\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1402 - val_loss: 0.7223\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1398 - val_loss: 0.7219\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1394 - val_loss: 0.7155\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1386 - val_loss: 0.7260\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1383 - val_loss: 0.7208\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1379 - val_loss: 0.7079\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1373 - val_loss: 0.7118\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.1367 - val_loss: 0.7139\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1361 - val_loss: 0.7048\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1360 - val_loss: 0.7047\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1354 - val_loss: 0.7103\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1355 - val_loss: 0.6999\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1348 - val_loss: 0.7041\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1342 - val_loss: 0.7049\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1338 - val_loss: 0.6920\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1335 - val_loss: 0.7001\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1328 - val_loss: 0.6999\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1324 - val_loss: 0.6917\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1322 - val_loss: 0.6870\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1317 - val_loss: 0.7019\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1312 - val_loss: 0.7035\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1309 - val_loss: 0.6880\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1304 - val_loss: 0.6868\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1299 - val_loss: 0.6932\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1297 - val_loss: 0.6848\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1292 - val_loss: 0.6887\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1289 - val_loss: 0.6849\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1284 - val_loss: 0.6911\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1277 - val_loss: 0.6857\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1277 - val_loss: 0.6849\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1272 - val_loss: 0.6754\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1269 - val_loss: 0.6817\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1265 - val_loss: 0.6768\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1262 - val_loss: 0.6752\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1256 - val_loss: 0.6844\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1253 - val_loss: 0.6759\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1249 - val_loss: 0.6696\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1248 - val_loss: 0.6769\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1244 - val_loss: 0.6768\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1241 - val_loss: 0.6754\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1233 - val_loss: 0.6780\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1236 - val_loss: 0.6698\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1229 - val_loss: 0.6717\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1227 - val_loss: 0.6705\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1222 - val_loss: 0.6721\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1219 - val_loss: 0.6827\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1218 - val_loss: 0.6686\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1213 - val_loss: 0.6653\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1207 - val_loss: 0.6635\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1207 - val_loss: 0.6661\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1202 - val_loss: 0.6666\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1199 - val_loss: 0.6603\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1196 - val_loss: 0.6548\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1194 - val_loss: 0.6641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed44f69e8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "moptimizer = Adagrad(lr=0.001)\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Adagrad')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 11.3394 - val_loss: 2.9798\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 1.2172 - val_loss: 1.9777\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.8435 - val_loss: 1.3647\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.5243 - val_loss: 1.5739\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.5142 - val_loss: 1.0598\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.2942 - val_loss: 0.9322\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.3921 - val_loss: 2.0044\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.4203 - val_loss: 1.3477\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.2480 - val_loss: 0.7763\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.2798 - val_loss: 0.6734\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1793 - val_loss: 0.6529\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1390 - val_loss: 0.4859\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.4017 - val_loss: 0.7417\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.2909 - val_loss: 0.7353\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.2831 - val_loss: 0.6568\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.2023 - val_loss: 0.6788\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1498 - val_loss: 0.4865\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.2776 - val_loss: 0.5166\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1976 - val_loss: 0.5727\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1608 - val_loss: 0.6974\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.2235 - val_loss: 0.7275\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.5889 - val_loss: 0.4881\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1872 - val_loss: 0.6439\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.1581 - val_loss: 0.7511\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1727 - val_loss: 0.5463\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1437 - val_loss: 0.4880\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.1644 - val_loss: 0.5071\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1148 - val_loss: 0.4591\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1474 - val_loss: 0.6168\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1367 - val_loss: 0.4978\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.2834 - val_loss: 0.5396\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1897 - val_loss: 0.5133\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1137 - val_loss: 0.4963\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0768 - val_loss: 0.5062\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0943 - val_loss: 0.5624\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1126 - val_loss: 0.4534\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1492 - val_loss: 0.5519\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0970 - val_loss: 0.4997\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1190 - val_loss: 0.5299\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.1311 - val_loss: 0.5408\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1874 - val_loss: 0.5321\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0784 - val_loss: 0.5007\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0674 - val_loss: 0.4984\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1482 - val_loss: 0.5567\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.1724 - val_loss: 0.5199\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0798 - val_loss: 0.5382\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0779 - val_loss: 0.5276\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0877 - val_loss: 0.5544\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0665 - val_loss: 0.5251\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0707 - val_loss: 0.5614\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0768 - val_loss: 0.5148\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0888 - val_loss: 0.5811\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0993 - val_loss: 0.4735\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.1285 - val_loss: 0.5761\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1770 - val_loss: 0.7435\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.2501 - val_loss: 0.6599\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0867 - val_loss: 0.6590\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0740 - val_loss: 0.6088\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0576 - val_loss: 0.5167\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0598 - val_loss: 0.5324\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0580 - val_loss: 0.5977\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1129 - val_loss: 0.5711\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0952 - val_loss: 0.5302\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0777 - val_loss: 0.5810\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0571 - val_loss: 0.5142\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0507 - val_loss: 0.5785\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0657 - val_loss: 0.5516\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0676 - val_loss: 0.5544\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0641 - val_loss: 0.7187\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0775 - val_loss: 0.5957\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0661 - val_loss: 0.5997\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1912 - val_loss: 0.6646\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0830 - val_loss: 0.6638\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0668 - val_loss: 0.5905\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0590 - val_loss: 0.5388\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0527 - val_loss: 0.5781\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0738 - val_loss: 0.6423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0803 - val_loss: 0.6717\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0935 - val_loss: 0.5581\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0740 - val_loss: 0.6943\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0779 - val_loss: 0.6250\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0504 - val_loss: 0.6211\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0494 - val_loss: 0.5824\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0462 - val_loss: 0.6117\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.1127 - val_loss: 0.6239\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0641 - val_loss: 0.5662\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0642 - val_loss: 0.5775\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0620 - val_loss: 0.6219\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0576 - val_loss: 0.6043\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0486 - val_loss: 0.6018\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0603 - val_loss: 0.7435\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0671 - val_loss: 0.6528\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0590 - val_loss: 0.6617\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0613 - val_loss: 0.8423\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1995 - val_loss: 0.6866\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0778 - val_loss: 0.7143\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0608 - val_loss: 0.6306\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0542 - val_loss: 0.6130\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0580 - val_loss: 0.7049\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0651 - val_loss: 0.6907\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0553 - val_loss: 0.5754\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0478 - val_loss: 0.6030\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0738 - val_loss: 0.6670\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0860 - val_loss: 0.6532\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0400 - val_loss: 0.6019\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0517 - val_loss: 0.5906\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0537 - val_loss: 0.6218\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0440 - val_loss: 0.5866\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0477 - val_loss: 0.6828\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0398 - val_loss: 0.6185\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0514 - val_loss: 0.6162\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0564 - val_loss: 0.8372\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1770 - val_loss: 0.6855\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0643 - val_loss: 0.6504\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0562 - val_loss: 0.6498\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0471 - val_loss: 0.7119\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0452 - val_loss: 0.7105\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0491 - val_loss: 0.6512\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0798 - val_loss: 0.5990\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0535 - val_loss: 0.6440\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0422 - val_loss: 0.7343\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0361 - val_loss: 0.6576\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0534 - val_loss: 0.6497\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0963 - val_loss: 0.6373\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0601 - val_loss: 0.6982\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0468 - val_loss: 0.6979\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0479 - val_loss: 0.7470\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0428 - val_loss: 0.6645\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0418 - val_loss: 0.6516\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0398 - val_loss: 0.5829\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0560 - val_loss: 0.6996\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0417 - val_loss: 0.7437\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0972 - val_loss: 0.5998\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1936 - val_loss: 0.6565\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0723 - val_loss: 0.5778\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0369 - val_loss: 0.6777\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0493 - val_loss: 0.6253\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0518 - val_loss: 0.6233\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0502 - val_loss: 0.5953\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0342 - val_loss: 0.6120\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0412 - val_loss: 0.6314\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0382 - val_loss: 0.5839\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0303 - val_loss: 0.6625\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0763 - val_loss: 0.6534\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0381 - val_loss: 0.7847\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0542 - val_loss: 0.6731\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0852 - val_loss: 0.8343\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0521 - val_loss: 0.6977\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0471 - val_loss: 0.6630\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0366 - val_loss: 0.6385\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0337 - val_loss: 0.7476\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0510 - val_loss: 0.6235\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0381 - val_loss: 0.6745\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0559 - val_loss: 0.6785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0382 - val_loss: 0.6898\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0418 - val_loss: 0.6961\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0702 - val_loss: 0.9619\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0793 - val_loss: 0.6781\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0432 - val_loss: 0.6662\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0566 - val_loss: 0.5800\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0612 - val_loss: 0.6027\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0340 - val_loss: 0.7865\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0350 - val_loss: 0.6430\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0436 - val_loss: 0.6252\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0527 - val_loss: 0.6514\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0418 - val_loss: 0.7328\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0465 - val_loss: 0.7193\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0687 - val_loss: 0.7093\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0587 - val_loss: 0.6835\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0486 - val_loss: 0.7255\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0554 - val_loss: 0.6414\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0532 - val_loss: 0.7383\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0397 - val_loss: 0.7465\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0370 - val_loss: 0.6352\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0365 - val_loss: 0.6770\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0486 - val_loss: 0.7037\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0429 - val_loss: 0.6656\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0597 - val_loss: 0.6775\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0339 - val_loss: 0.7270\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0393 - val_loss: 0.6962\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0284 - val_loss: 0.7470\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0578 - val_loss: 0.7016\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0804 - val_loss: 0.7630\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0488 - val_loss: 0.7337\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0589 - val_loss: 0.7591\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0337 - val_loss: 0.7008\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0366 - val_loss: 0.7513\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0263 - val_loss: 0.6758\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0396 - val_loss: 0.7845\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0432 - val_loss: 0.6952\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0385 - val_loss: 0.7779\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0511 - val_loss: 0.8608\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0684 - val_loss: 0.7060\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0368 - val_loss: 0.8769\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0466 - val_loss: 0.7818\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0508 - val_loss: 0.6978\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0358 - val_loss: 0.7594\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0446 - val_loss: 0.7511\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0381 - val_loss: 0.7284\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0315 - val_loss: 0.7597\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0397 - val_loss: 0.7095\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0530 - val_loss: 0.7163\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0440 - val_loss: 0.6425\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0523 - val_loss: 0.7355\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0282 - val_loss: 0.7023\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0496 - val_loss: 0.8192\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0358 - val_loss: 0.7908\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0587 - val_loss: 0.7205\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0354 - val_loss: 0.7263\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0346 - val_loss: 0.8038\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0337 - val_loss: 0.7543\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - ETA: 0s - loss: 0.025 - 2s 200us/step - loss: 0.0254 - val_loss: 0.7692\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0527 - val_loss: 0.7276\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0694 - val_loss: 0.7545\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0921 - val_loss: 0.8250\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0545 - val_loss: 0.6887\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0391 - val_loss: 0.7215\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0287 - val_loss: 0.6973\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0253 - val_loss: 0.7447\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0217 - val_loss: 0.7645\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0250 - val_loss: 0.7645\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0364 - val_loss: 0.7681\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0315 - val_loss: 0.6971\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0390 - val_loss: 0.7321\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0659 - val_loss: 0.7669\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0376 - val_loss: 0.6907\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0277 - val_loss: 0.7709\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0333 - val_loss: 0.7473\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0375 - val_loss: 0.7477\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0390 - val_loss: 0.7179\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0432 - val_loss: 0.7557\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0900 - val_loss: 0.7510\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0372 - val_loss: 0.7559\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0257 - val_loss: 0.7458\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0321 - val_loss: 0.7834\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0265 - val_loss: 0.7596\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0255 - val_loss: 0.7602\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0316 - val_loss: 0.7652\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0422 - val_loss: 0.7842\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0332 - val_loss: 0.7647\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0528 - val_loss: 0.7566\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0557 - val_loss: 0.7128\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0285 - val_loss: 0.7253\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0239 - val_loss: 0.7444\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0324 - val_loss: 0.7167\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0418 - val_loss: 0.8999\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0491 - val_loss: 0.7129\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0346 - val_loss: 0.7525\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0234 - val_loss: 0.9370\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0479 - val_loss: 0.7499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed3bade10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "moptimizer = Adam(lr=0.001)\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Adam')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 130.0567 - val_loss: 142.2640\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 121.9955 - val_loss: 132.6670\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 114.2655 - val_loss: 123.5611\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 106.8309 - val_loss: 115.0601\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 99.7511 - val_loss: 107.1168\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 92.9813 - val_loss: 99.6626\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 86.4513 - val_loss: 92.6110\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 80.0689 - val_loss: 85.8689\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 73.7802 - val_loss: 79.3434\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 67.4920 - val_loss: 72.8739\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 61.1564 - val_loss: 66.5038\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 54.8208 - val_loss: 60.1228\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 48.6185 - val_loss: 53.9826\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 42.8558 - val_loss: 48.2997\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 37.8631 - val_loss: 43.3938\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 33.8634 - val_loss: 39.4690\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 30.7562 - val_loss: 36.2739\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 28.2793 - val_loss: 33.6369\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 26.2782 - val_loss: 31.4465\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 24.5593 - val_loss: 29.5340\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 23.0674 - val_loss: 27.9130\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 21.7760 - val_loss: 26.4586\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 20.5769 - val_loss: 25.1280\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 19.5133 - val_loss: 23.9797\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 18.5634 - val_loss: 22.9263\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 17.7002 - val_loss: 21.9543\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 16.8896 - val_loss: 21.0543\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 16.1624 - val_loss: 20.2722\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 15.4881 - val_loss: 19.5032\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 14.8726 - val_loss: 18.8265\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 14.3030 - val_loss: 18.1871\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 13.7957 - val_loss: 17.6280\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 13.3270 - val_loss: 17.0873\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 12.8978 - val_loss: 16.6141\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 12.5131 - val_loss: 16.1739\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 12.1589 - val_loss: 15.7535\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 11.8215 - val_loss: 15.3678\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 11.5249 - val_loss: 15.0376\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 11.2496 - val_loss: 14.7173\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 11.0026 - val_loss: 14.4317\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 10.7656 - val_loss: 14.1562\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 10.5530 - val_loss: 13.9122\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 10.3521 - val_loss: 13.6713\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 10.1612 - val_loss: 13.4554\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 9.9852 - val_loss: 13.2411\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 9.8174 - val_loss: 13.0458\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 9.6623 - val_loss: 12.8612\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 9.5134 - val_loss: 12.6833\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 9.3672 - val_loss: 12.5059\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 9.2274 - val_loss: 12.3395\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 9.0933 - val_loss: 12.1842\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 8.9707 - val_loss: 12.0316\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 8.8419 - val_loss: 11.8822\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 8.7205 - val_loss: 11.7294\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 8.5965 - val_loss: 11.5969\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 8.4847 - val_loss: 11.4524\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 8.3658 - val_loss: 11.3114\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 8.2518 - val_loss: 11.1735\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 8.1429 - val_loss: 11.0407\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 8.0364 - val_loss: 10.9082\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 7.9273 - val_loss: 10.7788\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 7.8204 - val_loss: 10.6495\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 7.7102 - val_loss: 10.5202\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 7.6091 - val_loss: 10.3932\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 7.5036 - val_loss: 10.2783\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 7.4037 - val_loss: 10.1528\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 7.3058 - val_loss: 10.0388\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 7.2134 - val_loss: 9.9274\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 7.1212 - val_loss: 9.8139\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 7.0258 - val_loss: 9.6969\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 6.9334 - val_loss: 9.5859\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 6.8396 - val_loss: 9.4798\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 6.7538 - val_loss: 9.3813\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 6.6678 - val_loss: 9.2652\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 6.5780 - val_loss: 9.1594\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 6.4937 - val_loss: 9.0675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 6.4175 - val_loss: 8.9748\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 6.3380 - val_loss: 8.8761\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 6.2571 - val_loss: 8.7740\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 6.1690 - val_loss: 8.6730\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 6.0911 - val_loss: 8.5673\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 6.0030 - val_loss: 8.4740\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 5.9242 - val_loss: 8.3740\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 5.8442 - val_loss: 8.2739\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 5.7655 - val_loss: 8.1823\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 5.6931 - val_loss: 8.0862\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 5.6156 - val_loss: 8.0026\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 5.5425 - val_loss: 7.9146\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 5.4681 - val_loss: 7.8171\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 5.3921 - val_loss: 7.7303\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 5.3233 - val_loss: 7.6462\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 5.2566 - val_loss: 7.5684\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 5.1928 - val_loss: 7.4870\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 5.1219 - val_loss: 7.4000\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 5.0544 - val_loss: 7.3170\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 4.9908 - val_loss: 7.2404\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 4.9273 - val_loss: 7.1610\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 4.8633 - val_loss: 7.0816\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 4.8010 - val_loss: 7.0133\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 4.7418 - val_loss: 6.9432\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 4.6893 - val_loss: 6.8728\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 4.6243 - val_loss: 6.7985\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 4.5654 - val_loss: 6.7224\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 4.5057 - val_loss: 6.6503\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 4.4500 - val_loss: 6.5809\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 4.3935 - val_loss: 6.5208\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 4.3445 - val_loss: 6.4543\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 4.2907 - val_loss: 6.3906\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 4.2454 - val_loss: 6.3291\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 4.1898 - val_loss: 6.2707\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 4.1449 - val_loss: 6.2124\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 4.0917 - val_loss: 6.1562\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 4.0536 - val_loss: 6.1011\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 4.0008 - val_loss: 6.0380\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.9522 - val_loss: 5.9843\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.9126 - val_loss: 5.9294\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 3.8658 - val_loss: 5.8785\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.8250 - val_loss: 5.8233\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 3.7832 - val_loss: 5.7734\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.7429 - val_loss: 5.7228\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.7029 - val_loss: 5.6678\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.6613 - val_loss: 5.6147\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 3.6195 - val_loss: 5.5701\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 3.5859 - val_loss: 5.5260\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 3.5456 - val_loss: 5.4783\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 3.5041 - val_loss: 5.4338\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.4718 - val_loss: 5.3943\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.4347 - val_loss: 5.3568\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.4022 - val_loss: 5.3093\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 3.3674 - val_loss: 5.2651\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 3.3348 - val_loss: 5.2357\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.3073 - val_loss: 5.1984\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.2752 - val_loss: 5.1632\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.2464 - val_loss: 5.1230\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.2129 - val_loss: 5.0852\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.1846 - val_loss: 5.0485\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.1543 - val_loss: 5.0125\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 3.1261 - val_loss: 4.9795\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.0989 - val_loss: 4.9435\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 3.0753 - val_loss: 4.9156\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 3.0472 - val_loss: 4.8840\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.0310 - val_loss: 4.8618\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 3.0000 - val_loss: 4.8325\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.9748 - val_loss: 4.7997\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 2.9497 - val_loss: 4.7694\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.9288 - val_loss: 4.7423\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.9040 - val_loss: 4.7169\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.8886 - val_loss: 4.6996\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.8680 - val_loss: 4.6667\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 2.8447 - val_loss: 4.6412\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.8232 - val_loss: 4.6127\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.8024 - val_loss: 4.5849\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.7829 - val_loss: 4.5649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.7655 - val_loss: 4.5388\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.7400 - val_loss: 4.5161\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.7241 - val_loss: 4.4909\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.7013 - val_loss: 4.4743\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 2.6995 - val_loss: 4.4598\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 2.6718 - val_loss: 4.4341\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 2.6525 - val_loss: 4.4045\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 2.6364 - val_loss: 4.3825\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.6153 - val_loss: 4.3594\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.5971 - val_loss: 4.3396\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.5868 - val_loss: 4.3204\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.5652 - val_loss: 4.2929\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.5491 - val_loss: 4.2719\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.5305 - val_loss: 4.2541\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.5158 - val_loss: 4.2320\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.5015 - val_loss: 4.2102\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 2.4862 - val_loss: 4.1903\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.4719 - val_loss: 4.1711\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.4562 - val_loss: 4.1498\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.4420 - val_loss: 4.1316\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.4274 - val_loss: 4.1129\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.4124 - val_loss: 4.0954\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.3987 - val_loss: 4.0730\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.3850 - val_loss: 4.0550\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.3750 - val_loss: 4.0386\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.3580 - val_loss: 4.0215\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.3471 - val_loss: 4.0053\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.3371 - val_loss: 3.9924\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.3237 - val_loss: 3.9720\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.3088 - val_loss: 3.9553\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.2950 - val_loss: 3.9368\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.2894 - val_loss: 3.9273\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 2.2742 - val_loss: 3.9052\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 2.2600 - val_loss: 3.8878\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.2484 - val_loss: 3.8712\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.2385 - val_loss: 3.8557\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.2276 - val_loss: 3.8418\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.2151 - val_loss: 3.8237\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.2014 - val_loss: 3.8091\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.1919 - val_loss: 3.7991\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 2.1833 - val_loss: 3.7801\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.1786 - val_loss: 3.7685\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.1631 - val_loss: 3.7539\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.1503 - val_loss: 3.7380\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.1412 - val_loss: 3.7262\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.1288 - val_loss: 3.7109\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.1189 - val_loss: 3.6990\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.1081 - val_loss: 3.6850\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.0986 - val_loss: 3.6689\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 2.0883 - val_loss: 3.6538\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.0759 - val_loss: 3.6405\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.0715 - val_loss: 3.6294\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.0592 - val_loss: 3.6137\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.0477 - val_loss: 3.5961\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.0387 - val_loss: 3.5852\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.0282 - val_loss: 3.5762\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.0293 - val_loss: 3.5709\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 2.0121 - val_loss: 3.5618\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.0100 - val_loss: 3.5486\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 2.0027 - val_loss: 3.5396\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 1.9907 - val_loss: 3.5219\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 1.9793 - val_loss: 3.5124\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.9691 - val_loss: 3.4988\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.9680 - val_loss: 3.4874\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 1.9521 - val_loss: 3.4698\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.9413 - val_loss: 3.4558\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.9320 - val_loss: 3.4438\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 1.9242 - val_loss: 3.4424\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.9169 - val_loss: 3.4292\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.9106 - val_loss: 3.4164\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.9019 - val_loss: 3.3976\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 1.8896 - val_loss: 3.3856\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 1.8833 - val_loss: 3.3756\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.8739 - val_loss: 3.3636\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.8692 - val_loss: 3.3519\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.8576 - val_loss: 3.3419\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.8536 - val_loss: 3.3316\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 1.8417 - val_loss: 3.3225\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.8360 - val_loss: 3.3121\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.8295 - val_loss: 3.3012\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 1.8207 - val_loss: 3.2826\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 1.8108 - val_loss: 3.2669\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.8002 - val_loss: 3.2568\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.7957 - val_loss: 3.2481\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.7879 - val_loss: 3.2367\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 1.7804 - val_loss: 3.2293\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 1.7707 - val_loss: 3.2243\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 1.7659 - val_loss: 3.2094\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 1.7608 - val_loss: 3.1985\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.7513 - val_loss: 3.1842\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.7431 - val_loss: 3.1725\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.7363 - val_loss: 3.1596\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 1.7254 - val_loss: 3.1495\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.7194 - val_loss: 3.1383\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.7130 - val_loss: 3.1258\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 1.7049 - val_loss: 3.1204\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 1.6984 - val_loss: 3.1107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed037e128>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "moptimizer = Adadelta(lr=0.001)\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Adadelta')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 8.7542 - val_loss: 8.3734\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 2.5027 - val_loss: 3.9383\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 1.9350 - val_loss: 2.5119\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 1.6189 - val_loss: 2.6741\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 1.3321 - val_loss: 3.9061\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 1.0616 - val_loss: 2.1869\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 1.0527 - val_loss: 1.3533\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.8868 - val_loss: 1.3892\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.8769 - val_loss: 1.9765\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.7056 - val_loss: 1.2253\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.7379 - val_loss: 1.8708\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.6845 - val_loss: 0.8822\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.6523 - val_loss: 1.0213\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.6075 - val_loss: 1.4030\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.6302 - val_loss: 1.6044\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.5585 - val_loss: 1.3092\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.5263 - val_loss: 1.1573\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.5385 - val_loss: 1.3227\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.5349 - val_loss: 1.0994\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.5308 - val_loss: 0.7535\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.4700 - val_loss: 0.7735\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.4871 - val_loss: 1.6479\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.4959 - val_loss: 1.3751\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.4505 - val_loss: 1.4179\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.4118 - val_loss: 1.2848\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.4102 - val_loss: 1.6152\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.4166 - val_loss: 1.0447\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.4288 - val_loss: 1.8256\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3805 - val_loss: 1.5899\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.4272 - val_loss: 1.4020\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.3776 - val_loss: 1.1896\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3832 - val_loss: 1.0598\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3656 - val_loss: 2.8302\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3454 - val_loss: 1.1475\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3926 - val_loss: 0.8884\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.3358 - val_loss: 1.0637\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3282 - val_loss: 1.0837\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.3590 - val_loss: 0.9197\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3363 - val_loss: 1.0044\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3113 - val_loss: 1.1075\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3030 - val_loss: 0.6695\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2983 - val_loss: 1.2422\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2925 - val_loss: 1.3699\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2952 - val_loss: 1.3379\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.3784 - val_loss: 0.9133\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2950 - val_loss: 0.9313\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2798 - val_loss: 0.8105\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2807 - val_loss: 1.1861\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2706 - val_loss: 1.2021\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2779 - val_loss: 1.3300\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2716 - val_loss: 1.3247\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2815 - val_loss: 1.9113\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2745 - val_loss: 1.1579\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2802 - val_loss: 1.1607\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2379 - val_loss: 0.8856\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2657 - val_loss: 1.0985\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2432 - val_loss: 1.3508\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2213 - val_loss: 0.7371\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2623 - val_loss: 1.1669\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2339 - val_loss: 0.7817\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2165 - val_loss: 1.1082\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2279 - val_loss: 0.9753\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2504 - val_loss: 1.1094\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2889 - val_loss: 0.9074\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2794 - val_loss: 0.8189\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2299 - val_loss: 1.0950\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2334 - val_loss: 1.0107\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2476 - val_loss: 1.7938\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2214 - val_loss: 1.4974\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2096 - val_loss: 1.0704\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2173 - val_loss: 1.3709\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2345 - val_loss: 0.9413\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2107 - val_loss: 1.0857\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2076 - val_loss: 1.1845\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2247 - val_loss: 1.1834\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2422 - val_loss: 1.1285\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2313 - val_loss: 0.9983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2553 - val_loss: 1.4524\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2585 - val_loss: 0.9060\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1969 - val_loss: 1.6204\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2228 - val_loss: 0.8934\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1939 - val_loss: 0.7951\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2027 - val_loss: 1.2752\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2050 - val_loss: 0.9156\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2172 - val_loss: 1.0943\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2247 - val_loss: 1.0071\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2174 - val_loss: 1.8872\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2131 - val_loss: 0.9340\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2085 - val_loss: 1.2110\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1982 - val_loss: 1.3657\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2098 - val_loss: 0.7497\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2046 - val_loss: 0.8444\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1812 - val_loss: 0.8684\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1872 - val_loss: 0.9761\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1871 - val_loss: 0.9000\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2228 - val_loss: 1.1471\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1926 - val_loss: 1.5204\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2315 - val_loss: 1.1401\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1982 - val_loss: 1.3058\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1817 - val_loss: 0.8477\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1968 - val_loss: 0.8920\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1926 - val_loss: 0.7672\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2511 - val_loss: 0.9805\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1928 - val_loss: 0.9952\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1616 - val_loss: 1.2018\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1974 - val_loss: 1.0075\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1999 - val_loss: 0.7117\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1781 - val_loss: 1.1557\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2342 - val_loss: 0.9019\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1755 - val_loss: 1.1220\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1709 - val_loss: 1.7785\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2085 - val_loss: 0.6857\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1737 - val_loss: 1.1269\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1958 - val_loss: 1.1021\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1800 - val_loss: 1.0369\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1833 - val_loss: 0.9141\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2200 - val_loss: 1.2277\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1834 - val_loss: 0.5526\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1543 - val_loss: 1.0468\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1688 - val_loss: 1.7661\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1673 - val_loss: 0.9896\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1656 - val_loss: 1.0182\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1977 - val_loss: 0.6608\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1641 - val_loss: 0.8614\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2060 - val_loss: 0.8689\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1428 - val_loss: 1.8675\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2351 - val_loss: 1.0488\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1607 - val_loss: 1.1023\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1672 - val_loss: 1.0648\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2048 - val_loss: 1.2091\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1760 - val_loss: 0.5896\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1745 - val_loss: 1.1432\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1777 - val_loss: 1.3441\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1679 - val_loss: 1.5090\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1652 - val_loss: 1.1408\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1404 - val_loss: 0.8012\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.2325 - val_loss: 1.5304\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1453 - val_loss: 0.6278\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1515 - val_loss: 1.0835\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1506 - val_loss: 1.1487\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1868 - val_loss: 0.8584\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1564 - val_loss: 1.1081\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1282 - val_loss: 0.9351\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1504 - val_loss: 1.0842\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2121 - val_loss: 1.1432\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1488 - val_loss: 1.1266\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1395 - val_loss: 1.6958\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1977 - val_loss: 0.6526\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1558 - val_loss: 1.4122\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1680 - val_loss: 0.8576\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1543 - val_loss: 1.2520\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1492 - val_loss: 1.4919\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1930 - val_loss: 0.8385\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.1603 - val_loss: 1.1564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1720 - val_loss: 1.0188\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1607 - val_loss: 1.2343\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1397 - val_loss: 1.4990\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1289 - val_loss: 1.4268\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1981 - val_loss: 1.3983\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1637 - val_loss: 1.3849\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1803 - val_loss: 1.2167\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1217 - val_loss: 1.4726\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1701 - val_loss: 1.4112\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1659 - val_loss: 1.2703\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1806 - val_loss: 1.4270\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1568 - val_loss: 1.3920\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1374 - val_loss: 1.4518\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.1596 - val_loss: 1.0239\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1455 - val_loss: 1.5475\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1389 - val_loss: 1.4298\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1829 - val_loss: 0.8782\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1480 - val_loss: 1.5433\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1419 - val_loss: 1.4392\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1508 - val_loss: 2.7909\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1322 - val_loss: 0.8074\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1417 - val_loss: 1.4140\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1407 - val_loss: 1.3424\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1429 - val_loss: 1.9360\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1160 - val_loss: 0.9735\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1266 - val_loss: 2.7766\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1446 - val_loss: 1.5096\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1403 - val_loss: 1.3276\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1696 - val_loss: 1.7289\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1293 - val_loss: 1.0536\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1656 - val_loss: 1.5159\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1206 - val_loss: 1.3905\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1384 - val_loss: 0.9831\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1111 - val_loss: 2.5515\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1730 - val_loss: 0.9389\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1554 - val_loss: 1.0085\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1315 - val_loss: 1.2739\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1220 - val_loss: 1.2593\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1252 - val_loss: 1.2191\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1904 - val_loss: 1.6087\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1622 - val_loss: 1.1514\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1861 - val_loss: 0.7917\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1269 - val_loss: 1.9413\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1292 - val_loss: 1.6315\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1204 - val_loss: 1.5875\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1312 - val_loss: 0.9727\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1524 - val_loss: 1.1004\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1182 - val_loss: 0.8962\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1434 - val_loss: 2.3625\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1953 - val_loss: 1.1137\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1527 - val_loss: 1.5607\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1204 - val_loss: 1.8413\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1477 - val_loss: 1.7932\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1398 - val_loss: 3.3640\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1388 - val_loss: 1.3302\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1356 - val_loss: 1.0432\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1193 - val_loss: 1.2808\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1751 - val_loss: 0.9134\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1317 - val_loss: 2.3830\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1310 - val_loss: 1.5956\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1241 - val_loss: 1.6508\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1313 - val_loss: 1.2033\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1157 - val_loss: 1.7783\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1051 - val_loss: 1.6730\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1241 - val_loss: 1.3789\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1220 - val_loss: 2.0878\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1129 - val_loss: 2.6711\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1626 - val_loss: 1.3772\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1225 - val_loss: 2.4162\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1782 - val_loss: 1.6303\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1202 - val_loss: 1.2504\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1246 - val_loss: 2.3463\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1351 - val_loss: 1.1822\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1149 - val_loss: 0.3476\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1381 - val_loss: 1.7433\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1282 - val_loss: 1.4195\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1374 - val_loss: 1.2168\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1368 - val_loss: 0.8277\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1432 - val_loss: 1.7106\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1451 - val_loss: 1.6982\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1142 - val_loss: 1.3254\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1392 - val_loss: 2.5376\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1168 - val_loss: 1.2334\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1150 - val_loss: 2.3302\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1182 - val_loss: 1.7822\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1019 - val_loss: 1.1116\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1428 - val_loss: 1.6927\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1253 - val_loss: 2.1920\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1355 - val_loss: 1.3702\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1179 - val_loss: 1.5457\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1283 - val_loss: 1.1155\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1055 - val_loss: 1.5408\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1192 - val_loss: 0.7465\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.1314 - val_loss: 1.3610\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1192 - val_loss: 1.8017\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1231 - val_loss: 1.5694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20ed5b99e48>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "moptimizer = RMSprop(lr=0.001)\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and RMSprop')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Entrene los modelos obtenidos en b) y c) utilizando regularizadores $l_1$ y $l_2$ (*weight decay*). Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente. Además evalúe el efecto de regularizar solo la primera capa *vs* la segunda, comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesus\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, input_dim=1275, kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\Jesus\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 128.6565 - val_loss: 137.8254\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 115.4532 - val_loss: 123.2377\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 102.8024 - val_loss: 109.6455\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 90.4809 - val_loss: 96.2608\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 78.6479 - val_loss: 83.6343\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 67.4833 - val_loss: 71.6930\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 57.1794 - val_loss: 60.6502\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 47.8716 - val_loss: 50.8557\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 39.5925 - val_loss: 42.0357\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 32.3671 - val_loss: 34.3230\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 26.2065 - val_loss: 27.8191\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 21.0462 - val_loss: 22.3216\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 16.8356 - val_loss: 17.8911\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 13.5126 - val_loss: 14.4311\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 10.9780 - val_loss: 11.7622\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 9.0744 - val_loss: 9.7467\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 7.6857 - val_loss: 8.2725\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 6.6748 - val_loss: 7.1981\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 5.9191 - val_loss: 6.3870\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 5.3332 - val_loss: 5.7569\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 4.8611 - val_loss: 5.2396\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 4.4638 - val_loss: 4.7995\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 4.1229 - val_loss: 4.4100\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 3.8267 - val_loss: 4.0823\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 3.5715 - val_loss: 3.7914\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 3.3450 - val_loss: 3.5401\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 3.1423 - val_loss: 3.3037\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 2.9652 - val_loss: 3.1056\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 2.8060 - val_loss: 2.9279\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 2.6614 - val_loss: 2.7639\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 2.5285 - val_loss: 2.6099\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 2.4089 - val_loss: 2.4728\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 2.3031 - val_loss: 2.3497\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 2.2080 - val_loss: 2.2453\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 2.1205 - val_loss: 2.1460\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 2.0400 - val_loss: 2.0558\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 1.9664 - val_loss: 1.9770\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 1.8973 - val_loss: 1.8989\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 1.8321 - val_loss: 1.8265\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 1.7702 - val_loss: 1.7569\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 1.7137 - val_loss: 1.6954\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 1.6607 - val_loss: 1.6395\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 1.6112 - val_loss: 1.5894\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 1.5653 - val_loss: 1.5399\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 1.5216 - val_loss: 1.4934\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 1.4808 - val_loss: 1.4486\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 1.4404 - val_loss: 1.4101\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 1.4029 - val_loss: 1.3673\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 1.3678 - val_loss: 1.3258\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 1.3341 - val_loss: 1.2921\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 1.3028 - val_loss: 1.2570\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 1.2725 - val_loss: 1.2255\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 1.2438 - val_loss: 1.1979\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.2171 - val_loss: 1.1693\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 1.1914 - val_loss: 1.1425\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 1.1673 - val_loss: 1.1184\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 1.1440 - val_loss: 1.0940\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 1.1216 - val_loss: 1.0719\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 1.1003 - val_loss: 1.0474\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 1.0810 - val_loss: 1.0286\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 1.0615 - val_loss: 1.0091\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 1.0437 - val_loss: 0.9900\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 1.0258 - val_loss: 0.9731\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 1.0093 - val_loss: 0.9600\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.9932 - val_loss: 0.9403\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.9772 - val_loss: 0.9280\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.9623 - val_loss: 0.9105\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.9479 - val_loss: 0.8969\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.9337 - val_loss: 0.8828\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.9207 - val_loss: 0.8701\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.9077 - val_loss: 0.8588\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.8955 - val_loss: 0.8468\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.8840 - val_loss: 0.8323\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.8722 - val_loss: 0.8257\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.8608 - val_loss: 0.8113\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.8493 - val_loss: 0.8015\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.8388 - val_loss: 0.7904\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.8280 - val_loss: 0.7796\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.8187 - val_loss: 0.7708\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.8096 - val_loss: 0.7620\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.8007 - val_loss: 0.7538\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.7921 - val_loss: 0.7481\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.7841 - val_loss: 0.7369\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.7760 - val_loss: 0.7304\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.7683 - val_loss: 0.7217\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.7608 - val_loss: 0.7205\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.7535 - val_loss: 0.7084\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.7461 - val_loss: 0.7044\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.7393 - val_loss: 0.6946\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.7325 - val_loss: 0.6873\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.7259 - val_loss: 0.6827\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.7195 - val_loss: 0.6749\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.7136 - val_loss: 0.6716\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.7080 - val_loss: 0.6668\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.7015 - val_loss: 0.6604\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.6962 - val_loss: 0.6537\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.6912 - val_loss: 0.6505\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.6860 - val_loss: 0.6439\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.6810 - val_loss: 0.6421\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.6761 - val_loss: 0.6337\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.6713 - val_loss: 0.6301\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.6663 - val_loss: 0.6254\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.6618 - val_loss: 0.6212\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.6572 - val_loss: 0.6152\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.6530 - val_loss: 0.6109\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.6488 - val_loss: 0.6087\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.6450 - val_loss: 0.6070\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.6413 - val_loss: 0.6009\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.6370 - val_loss: 0.5980\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.6336 - val_loss: 0.5935\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.6299 - val_loss: 0.5903\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.6267 - val_loss: 0.5888\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.6232 - val_loss: 0.5880\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.6201 - val_loss: 0.5819\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.6169 - val_loss: 0.5775\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.6138 - val_loss: 0.5767\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.6101 - val_loss: 0.5715\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.6076 - val_loss: 0.5690\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.6042 - val_loss: 0.5671\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.6020 - val_loss: 0.5642\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.5992 - val_loss: 0.5627\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.5966 - val_loss: 0.5573\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5940 - val_loss: 0.5560\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.5915 - val_loss: 0.5558\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.5892 - val_loss: 0.5508\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.5868 - val_loss: 0.5496\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5848 - val_loss: 0.5478\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.5825 - val_loss: 0.5448\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.5803 - val_loss: 0.5438\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5782 - val_loss: 0.5414\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5758 - val_loss: 0.5397\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5740 - val_loss: 0.5382\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5720 - val_loss: 0.5380\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5701 - val_loss: 0.5342\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5682 - val_loss: 0.5331\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.5662 - val_loss: 0.5316\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.5646 - val_loss: 0.5315\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.5627 - val_loss: 0.5266\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.5610 - val_loss: 0.5258\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.5593 - val_loss: 0.5246\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.5578 - val_loss: 0.5229\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.5557 - val_loss: 0.5266\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.5543 - val_loss: 0.5208\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.5528 - val_loss: 0.5185\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5513 - val_loss: 0.5185\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5498 - val_loss: 0.5154\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.5482 - val_loss: 0.5134\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.5470 - val_loss: 0.5134\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5455 - val_loss: 0.5119\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.5444 - val_loss: 0.5105\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5431 - val_loss: 0.5090\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5415 - val_loss: 0.5094\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.5402 - val_loss: 0.5074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5387 - val_loss: 0.5078\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5377 - val_loss: 0.5048\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5367 - val_loss: 0.5030\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.5356 - val_loss: 0.5034\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5340 - val_loss: 0.5023\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.5331 - val_loss: 0.5001\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5319 - val_loss: 0.4993\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5307 - val_loss: 0.4987\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5300 - val_loss: 0.4980\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.5291 - val_loss: 0.4973\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5280 - val_loss: 0.4954\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5268 - val_loss: 0.4954\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5259 - val_loss: 0.4947\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5248 - val_loss: 0.4958\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.5238 - val_loss: 0.4933\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5229 - val_loss: 0.4910\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5218 - val_loss: 0.4908\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5210 - val_loss: 0.4895\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.5197 - val_loss: 0.4906\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.5190 - val_loss: 0.4888\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.5178 - val_loss: 0.4877\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5168 - val_loss: 0.4852\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5158 - val_loss: 0.4844\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5153 - val_loss: 0.4846\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5144 - val_loss: 0.4834\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5137 - val_loss: 0.4829\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.5128 - val_loss: 0.4835\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5120 - val_loss: 0.4820\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5113 - val_loss: 0.4801\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.5104 - val_loss: 0.4815\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.5100 - val_loss: 0.4784\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5093 - val_loss: 0.4795\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.5082 - val_loss: 0.4785\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5075 - val_loss: 0.4788\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5069 - val_loss: 0.4772\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.5059 - val_loss: 0.4773\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.5049 - val_loss: 0.4785\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5044 - val_loss: 0.4734\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5039 - val_loss: 0.4739\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.5028 - val_loss: 0.4728\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.5024 - val_loss: 0.4731\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.5015 - val_loss: 0.4703\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.5009 - val_loss: 0.4704\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.5002 - val_loss: 0.4704\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.4992 - val_loss: 0.4695\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.4985 - val_loss: 0.4688\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4978 - val_loss: 0.4681\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4971 - val_loss: 0.4681\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.4963 - val_loss: 0.4663\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.4960 - val_loss: 0.4657\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4951 - val_loss: 0.4650\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4945 - val_loss: 0.4643\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.4941 - val_loss: 0.4662\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.4935 - val_loss: 0.4641\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.4928 - val_loss: 0.4633\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.4918 - val_loss: 0.4627\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4918 - val_loss: 0.4635\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4913 - val_loss: 0.4632\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4909 - val_loss: 0.4630\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4901 - val_loss: 0.4626\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4896 - val_loss: 0.4622\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.4890 - val_loss: 0.4612\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4889 - val_loss: 0.4613\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.4883 - val_loss: 0.4605\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.4878 - val_loss: 0.4606\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.4873 - val_loss: 0.4596\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.4871 - val_loss: 0.4602\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.4864 - val_loss: 0.4599\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4861 - val_loss: 0.4595\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4856 - val_loss: 0.4617\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4851 - val_loss: 0.4577\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4846 - val_loss: 0.4581\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4843 - val_loss: 0.4571\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.4839 - val_loss: 0.4566\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4836 - val_loss: 0.4583\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.4832 - val_loss: 0.4553\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4829 - val_loss: 0.4563\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4826 - val_loss: 0.4564\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4821 - val_loss: 0.4550\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4817 - val_loss: 0.4547\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.4812 - val_loss: 0.4542\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.4807 - val_loss: 0.4537\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.4805 - val_loss: 0.4534\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.4800 - val_loss: 0.4538\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.4795 - val_loss: 0.4553\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.4792 - val_loss: 0.4533\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.4788 - val_loss: 0.4549\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.4786 - val_loss: 0.4524\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.4782 - val_loss: 0.4525\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.4778 - val_loss: 0.4507\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.4774 - val_loss: 0.4506\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.4770 - val_loss: 0.4519\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.4766 - val_loss: 0.4527\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.4765 - val_loss: 0.4513\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.4761 - val_loss: 0.4503\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.4755 - val_loss: 0.4504\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.4754 - val_loss: 0.4507\n"
     ]
    }
   ],
   "source": [
    "#ESTE FUE NUESTRO MEJOR MODELO SIMOIDE\n",
    "model = Sequential()\n",
    "...#la regularization se debe incorporar a cada capa separadamente\n",
    "moptimizer = Adadelta(lr=0.01)\n",
    "idim=X_train_scaled.shape[1]\n",
    "model.add(Dense(256,input_dim=idim,kernel_initializer='uniform',W_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x250c2e77208>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and RMSprop')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesus\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, input_dim=1275, kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\Jesus\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 104.2506 - val_loss: 85.8319\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 49.4471 - val_loss: 34.5167\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 20.1722 - val_loss: 20.0374\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 14.5905 - val_loss: 17.3198\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 13.0146 - val_loss: 15.8342\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 11.9563 - val_loss: 14.6388\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 11.0474 - val_loss: 13.6000\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 10.1983 - val_loss: 12.5981\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 9.4518 - val_loss: 11.8436\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 8.7847 - val_loss: 11.0179\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 8.1639 - val_loss: 10.3802\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 7.6340 - val_loss: 9.8575\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 7.1550 - val_loss: 9.4081\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 6.7672 - val_loss: 8.8739\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 6.4206 - val_loss: 8.6466\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 6.1324 - val_loss: 8.3188\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 5.9024 - val_loss: 8.0410\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 5.6710 - val_loss: 7.7304\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 5.4712 - val_loss: 7.6727\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 285us/step - loss: 5.3458 - val_loss: 7.4103\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 5.2238 - val_loss: 7.2296\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 5.0659 - val_loss: 7.2226\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 4.9915 - val_loss: 6.9681\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 289us/step - loss: 4.8657 - val_loss: 6.8218\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 4.7643 - val_loss: 6.6660\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 4.6766 - val_loss: 6.5811\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 4.6219 - val_loss: 6.4676\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 4.5381 - val_loss: 6.3498\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 4.4604 - val_loss: 6.2629\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 4.3948 - val_loss: 6.2053\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 4.3329 - val_loss: 6.0966\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 4.2632 - val_loss: 6.0446\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 4.2277 - val_loss: 5.9039\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 4.1515 - val_loss: 5.7731\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 4.0899 - val_loss: 5.7972\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 4.0385 - val_loss: 5.8060\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 4.0263 - val_loss: 5.6583\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 3.9630 - val_loss: 5.6144\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 3.9140 - val_loss: 5.4729\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 3.8712 - val_loss: 5.4431\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 3.8348 - val_loss: 5.4130\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 3.7998 - val_loss: 5.3400\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 3.7491 - val_loss: 5.2225\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 3.7157 - val_loss: 5.2271\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 3.6931 - val_loss: 5.1990\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 3.6572 - val_loss: 5.1165\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 3.6239 - val_loss: 5.0680\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 3.5833 - val_loss: 5.0631\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 3.5593 - val_loss: 5.0776\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 3.5477 - val_loss: 4.9243\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 3.4982 - val_loss: 4.8756\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 3.4632 - val_loss: 4.8693\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 3.4505 - val_loss: 4.8262\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 3.4145 - val_loss: 4.8065\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 3.4055 - val_loss: 4.7676\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 3.3694 - val_loss: 4.7229\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 3.3324 - val_loss: 4.6858\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 3.3283 - val_loss: 4.6377\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 3.2905 - val_loss: 4.5948\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 3.2639 - val_loss: 4.5747\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 3.2277 - val_loss: 4.5564\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 3.2078 - val_loss: 4.5551\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 3.1960 - val_loss: 4.4750\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 3.1609 - val_loss: 4.3984\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 3.1355 - val_loss: 4.4184\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 3.1267 - val_loss: 4.3727\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 3.1010 - val_loss: 4.3639\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 3.0844 - val_loss: 4.3410\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 3.0516 - val_loss: 4.2929\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 3.0343 - val_loss: 4.2580\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 3.0126 - val_loss: 4.2474\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 2.9894 - val_loss: 4.1984\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 2.9703 - val_loss: 4.1615\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 2.9608 - val_loss: 4.1379\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 2.9279 - val_loss: 4.1964\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 2.9149 - val_loss: 4.0707\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 2.8885 - val_loss: 4.0848\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 2.8803 - val_loss: 4.0529\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 2.8556 - val_loss: 3.9979\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 2.8394 - val_loss: 3.9887\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 2.8242 - val_loss: 3.9624\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 2.8048 - val_loss: 3.9290\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 2.7905 - val_loss: 3.9162\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 2.7627 - val_loss: 3.8961\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 2.7560 - val_loss: 3.8686\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 2.7368 - val_loss: 3.8350\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 2.7227 - val_loss: 3.8518\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 2.7058 - val_loss: 3.7978\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 2.6850 - val_loss: 3.8225\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 2.6714 - val_loss: 3.7765\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 2.6494 - val_loss: 3.7315\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 2.6377 - val_loss: 3.7192\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 2.6189 - val_loss: 3.7109\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 2.6094 - val_loss: 3.7019\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 2.5925 - val_loss: 3.6629\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 2.5745 - val_loss: 3.6404\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 2.5618 - val_loss: 3.6377\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 2.5483 - val_loss: 3.6344\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 2.5291 - val_loss: 3.5836\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 2.5155 - val_loss: 3.5689\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 2.5010 - val_loss: 3.5882\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 2.4921 - val_loss: 3.5600\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 2.4755 - val_loss: 3.5190\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 2.4658 - val_loss: 3.5000\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 2.4471 - val_loss: 3.5140\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 2.4388 - val_loss: 3.4941\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 2.4230 - val_loss: 3.4621\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 2.4104 - val_loss: 3.4370\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 2.3968 - val_loss: 3.4378\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 2.3835 - val_loss: 3.4193\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 2.3693 - val_loss: 3.4109\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 2.3559 - val_loss: 3.3749\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 2.3432 - val_loss: 3.3432\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 2.3308 - val_loss: 3.3476\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 2.3190 - val_loss: 3.3202\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 2.3068 - val_loss: 3.3169\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 2.2970 - val_loss: 3.3035\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 2.2819 - val_loss: 3.2844\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 2.2749 - val_loss: 3.2836\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 2.2586 - val_loss: 3.2624\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 2.2465 - val_loss: 3.2372\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 2.2352 - val_loss: 3.2393\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 2.2231 - val_loss: 3.2091\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 2.2182 - val_loss: 3.2086\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 2.1980 - val_loss: 3.2008\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 2.1857 - val_loss: 3.1601\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 2.1769 - val_loss: 3.1609\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 2.1650 - val_loss: 3.1457\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 2.1539 - val_loss: 3.1271\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 2.1415 - val_loss: 3.1022\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 2.1317 - val_loss: 3.1322\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 2.1199 - val_loss: 3.1259\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 2.1086 - val_loss: 3.1079\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 2.0969 - val_loss: 3.0681\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 2.0872 - val_loss: 3.0613\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 2.0733 - val_loss: 3.0520\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 2.0661 - val_loss: 3.0330\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 2.0543 - val_loss: 3.0040\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 2.0427 - val_loss: 3.0054\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 2.0346 - val_loss: 2.9807\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 2.0243 - val_loss: 2.9903\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 2.0139 - val_loss: 3.0382\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 2.0058 - val_loss: 2.9715\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 1.9936 - val_loss: 2.9612\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.9849 - val_loss: 2.9597\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 1.9748 - val_loss: 2.9141\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.9652 - val_loss: 2.9094\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 1.9547 - val_loss: 2.9008\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 1.9415 - val_loss: 2.8824\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 1.9331 - val_loss: 2.8934\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 1.9235 - val_loss: 2.8542\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 1.9145 - val_loss: 2.8632\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 1.9051 - val_loss: 2.8479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.8934 - val_loss: 2.8221\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 1.8852 - val_loss: 2.8277\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.8763 - val_loss: 2.7950\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 1.8654 - val_loss: 2.8206\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 1.8552 - val_loss: 2.7998\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 1.8463 - val_loss: 2.7821\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.8382 - val_loss: 2.7761\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 1.8271 - val_loss: 2.7573\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 1.8219 - val_loss: 2.7413\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 1.8114 - val_loss: 2.7447\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 1.8015 - val_loss: 2.7317\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.7933 - val_loss: 2.7239\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 1.7831 - val_loss: 2.7096\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.7759 - val_loss: 2.6961\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 1.7640 - val_loss: 2.6792\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 1.7547 - val_loss: 2.6576\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.7469 - val_loss: 2.6637\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.7373 - val_loss: 2.6369\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.7303 - val_loss: 2.6413\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 1.7196 - val_loss: 2.6168\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.7126 - val_loss: 2.6191\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.7015 - val_loss: 2.6314\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.6952 - val_loss: 2.6289\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.6851 - val_loss: 2.6034\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.6773 - val_loss: 2.5886\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 1.6696 - val_loss: 2.5660\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 1.6598 - val_loss: 2.5651\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 1.6511 - val_loss: 2.5672\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 1.6425 - val_loss: 2.5392\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.6349 - val_loss: 2.5324\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 1.6260 - val_loss: 2.5233\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.6192 - val_loss: 2.5210\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.6091 - val_loss: 2.4940\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.6026 - val_loss: 2.5113\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 1.5938 - val_loss: 2.5140\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 1.5868 - val_loss: 2.4816\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.5792 - val_loss: 2.4596\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.5728 - val_loss: 2.4439\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 1.5624 - val_loss: 2.4618\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 1.5554 - val_loss: 2.4484\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 1.5469 - val_loss: 2.4252\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.5397 - val_loss: 2.4374\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.5309 - val_loss: 2.4539\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.5256 - val_loss: 2.4274\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.5165 - val_loss: 2.3972\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 1.5095 - val_loss: 2.4018\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.5012 - val_loss: 2.3945\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.4936 - val_loss: 2.3707\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 1.4869 - val_loss: 2.3545\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 1.4794 - val_loss: 2.3690\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 1.4720 - val_loss: 2.3580\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 1.4648 - val_loss: 2.3315\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 1.4559 - val_loss: 2.3275\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.4494 - val_loss: 2.3232\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.4425 - val_loss: 2.3174\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 1.4366 - val_loss: 2.3098\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 1.4277 - val_loss: 2.3102\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.4220 - val_loss: 2.3003\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.4151 - val_loss: 2.2857\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 1.4089 - val_loss: 2.2794\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 1.3994 - val_loss: 2.2487\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.3934 - val_loss: 2.2478\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.3854 - val_loss: 2.2495\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 1.3796 - val_loss: 2.2382\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 1.3719 - val_loss: 2.2348\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 1.3653 - val_loss: 2.2306\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.3587 - val_loss: 2.2154\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.3523 - val_loss: 2.2349\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.3462 - val_loss: 2.2131\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.3385 - val_loss: 2.1696\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 1.3326 - val_loss: 2.1849\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 1.3263 - val_loss: 2.1778\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 1.3204 - val_loss: 2.1598\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 284us/step - loss: 1.3132 - val_loss: 2.1702\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 1.3080 - val_loss: 2.1475\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 1.3013 - val_loss: 2.1332\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.2970 - val_loss: 2.1264\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 1.2895 - val_loss: 2.1267\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 1.2834 - val_loss: 2.1607\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 1.2778 - val_loss: 2.1065\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 1.2713 - val_loss: 2.1192\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 1.2651 - val_loss: 2.1007\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 1.2593 - val_loss: 2.0924\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 1.2523 - val_loss: 2.0984\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.2468 - val_loss: 2.0747\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 1.2405 - val_loss: 2.0730\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 1.2345 - val_loss: 2.0662\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 1.2283 - val_loss: 2.0492\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.2221 - val_loss: 2.0598\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 1.2178 - val_loss: 2.0416\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 1.2105 - val_loss: 2.0514\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 1.2053 - val_loss: 2.0203\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 1.1982 - val_loss: 2.0184\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 1.1936 - val_loss: 1.9903\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.1886 - val_loss: 2.0130\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 1.1814 - val_loss: 2.0195\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 1.1755 - val_loss: 2.0005\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "...#la regularization se debe incorporar a cada capa separadamente\n",
    "moptimizer = Adadelta(lr=0.01)\n",
    "idim=X_train_scaled.shape[1]\n",
    "model.add(Dense(256,input_dim=idim,kernel_initializer='uniform',W_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x250c42a9be0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and RMSprop')\n",
    "im.legend(('Val_Loss', 'Loss' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesus\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, input_dim=1275, kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\Jesus\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: 135.9281 - val_loss: 144.6806\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 122.9703 - val_loss: 130.6010\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 110.3914 - val_loss: 116.7616\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 98.0696 - val_loss: 103.3450\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 86.2056 - val_loss: 90.5922\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 74.9731 - val_loss: 78.7375\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 64.5441 - val_loss: 67.5327\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 54.9591 - val_loss: 57.3332\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 46.3486 - val_loss: 48.1113\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 38.7492 - val_loss: 40.0955\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 32.1992 - val_loss: 33.2357\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 26.6719 - val_loss: 27.4406\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 22.0993 - val_loss: 22.6707\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 18.3632 - val_loss: 18.7748\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 15.3920 - val_loss: 15.7203\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 13.0645 - val_loss: 13.3346\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 11.2577 - val_loss: 11.4588\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 9.8189 - val_loss: 9.9570\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 8.6380 - val_loss: 8.7218\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 7.6166 - val_loss: 7.6363\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 6.7257 - val_loss: 6.7129\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 5.9361 - val_loss: 5.8859\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 5.2421 - val_loss: 5.1726\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 4.6531 - val_loss: 4.5771\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 289us/step - loss: 4.1448 - val_loss: 4.0476\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 3.7215 - val_loss: 3.6204\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 3.3662 - val_loss: 3.2703\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 3.0679 - val_loss: 2.9593\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 2.8080 - val_loss: 2.6961\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 2.5910 - val_loss: 2.4719\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 289us/step - loss: 2.4012 - val_loss: 2.2833\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 2.2387 - val_loss: 2.1241\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 2.0959 - val_loss: 1.9726\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 1.9596 - val_loss: 1.8195\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 1.8379 - val_loss: 1.7156\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 1.7400 - val_loss: 1.6219\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 1.6511 - val_loss: 1.5219\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 1.5703 - val_loss: 1.4415\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 1.4983 - val_loss: 1.3763\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 1.4323 - val_loss: 1.3081\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 1.3732 - val_loss: 1.2534\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 1.3206 - val_loss: 1.2014\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 1.2743 - val_loss: 1.1640\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 1.2340 - val_loss: 1.1243\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 1.1991 - val_loss: 1.0888\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 1.1704 - val_loss: 1.0612\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 1.1433 - val_loss: 1.0440\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 1.1193 - val_loss: 1.0105\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 1.0970 - val_loss: 0.9884\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 1.0804 - val_loss: 0.9756\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 1.0647 - val_loss: 0.9586\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 1.0510 - val_loss: 0.9448\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 1.0381 - val_loss: 0.9329\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 1.0263 - val_loss: 0.9248\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 1.0164 - val_loss: 0.9157\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 1.0077 - val_loss: 0.9081\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.9993 - val_loss: 0.9040\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.9925 - val_loss: 0.8967\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.9850 - val_loss: 0.8851\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.9790 - val_loss: 0.8836\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.9734 - val_loss: 0.8770\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.9683 - val_loss: 0.8684\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.9623 - val_loss: 0.8643\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.9578 - val_loss: 0.8596\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.9540 - val_loss: 0.8631\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.9497 - val_loss: 0.8551\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.9451 - val_loss: 0.8594\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.9415 - val_loss: 0.8451\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.9385 - val_loss: 0.8425\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.9349 - val_loss: 0.8442\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.9312 - val_loss: 0.8483\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.9283 - val_loss: 0.8311\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.9257 - val_loss: 0.8307\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.9223 - val_loss: 0.8263\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.9196 - val_loss: 0.8283\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.9171 - val_loss: 0.8215\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.9154 - val_loss: 0.8225\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.9126 - val_loss: 0.8245\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 289us/step - loss: 0.9110 - val_loss: 0.8194\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.9089 - val_loss: 0.8151\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.9069 - val_loss: 0.8113\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.9052 - val_loss: 0.8125\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.9031 - val_loss: 0.8144\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.9015 - val_loss: 0.8096\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 0.9002 - val_loss: 0.8092\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.8983 - val_loss: 0.8044\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.8966 - val_loss: 0.8043\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.8950 - val_loss: 0.8104\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.8936 - val_loss: 0.8016\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.8927 - val_loss: 0.8040\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.8911 - val_loss: 0.8001\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.8901 - val_loss: 0.8013\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.8889 - val_loss: 0.8003\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 0.8873 - val_loss: 0.7961\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.8867 - val_loss: 0.7967\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.8852 - val_loss: 0.7947\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.8839 - val_loss: 0.7980\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 289us/step - loss: 0.8829 - val_loss: 0.7922\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.8819 - val_loss: 0.7931\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.8806 - val_loss: 0.7909\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 289us/step - loss: 0.8797 - val_loss: 0.7922\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.8793 - val_loss: 0.7894\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 289us/step - loss: 0.8780 - val_loss: 0.7925\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.8773 - val_loss: 0.7926\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.8763 - val_loss: 0.7935\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.8759 - val_loss: 0.7851\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.8749 - val_loss: 0.7875\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.8741 - val_loss: 0.7857\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.8731 - val_loss: 0.7845\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 0.8723 - val_loss: 0.7814\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.8715 - val_loss: 0.7830\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.8710 - val_loss: 0.7858\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.8700 - val_loss: 0.7847\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.8696 - val_loss: 0.7815\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.8684 - val_loss: 0.7851\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.8680 - val_loss: 0.7815\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 0.8674 - val_loss: 0.7814\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.8667 - val_loss: 0.7803\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.8657 - val_loss: 0.7760\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.8655 - val_loss: 0.7771\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.8647 - val_loss: 0.7796\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.8640 - val_loss: 0.7769\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.8639 - val_loss: 0.7767\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.8624 - val_loss: 0.7789\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.8626 - val_loss: 0.7758\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.8617 - val_loss: 0.7736\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.8606 - val_loss: 0.7751\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.8604 - val_loss: 0.7698\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.8604 - val_loss: 0.7728\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.8596 - val_loss: 0.7748\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.8595 - val_loss: 0.7760\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.8589 - val_loss: 0.7718\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.8585 - val_loss: 0.7698\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.8579 - val_loss: 0.7722\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.8576 - val_loss: 0.7685\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.8569 - val_loss: 0.7686\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.8566 - val_loss: 0.7718\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.8559 - val_loss: 0.7712\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.8555 - val_loss: 0.7693\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.8556 - val_loss: 0.7691\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.8550 - val_loss: 0.7730\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.8544 - val_loss: 0.7654\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.8537 - val_loss: 0.7681\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.8533 - val_loss: 0.7686\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.8532 - val_loss: 0.7665\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.8530 - val_loss: 0.7708\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 308us/step - loss: 0.8521 - val_loss: 0.7668\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.8517 - val_loss: 0.7675\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.8515 - val_loss: 0.7692\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.8511 - val_loss: 0.7640\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.8505 - val_loss: 0.7623\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.8495 - val_loss: 0.7668\n",
      "Epoch 153/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.8487 - val_loss: 0.7606\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.8477 - val_loss: 0.7625\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.8478 - val_loss: 0.7648\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.8476 - val_loss: 0.7618\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.8471 - val_loss: 0.7636\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 0.8460 - val_loss: 0.7599\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.8466 - val_loss: 0.7624\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8458 - val_loss: 0.7578\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.8454 - val_loss: 0.7626\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8455 - val_loss: 0.7646\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.8449 - val_loss: 0.7602\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.8444 - val_loss: 0.7593\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8441 - val_loss: 0.7600\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.8442 - val_loss: 0.7609\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.8429 - val_loss: 0.7607\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8429 - val_loss: 0.7571\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8422 - val_loss: 0.7634\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.8425 - val_loss: 0.7609\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8420 - val_loss: 0.7556\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8415 - val_loss: 0.7580\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.8412 - val_loss: 0.7574\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8414 - val_loss: 0.7625\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.8409 - val_loss: 0.7583\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.8407 - val_loss: 0.7559\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8400 - val_loss: 0.7553\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.8396 - val_loss: 0.7534\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.8399 - val_loss: 0.7545\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.8391 - val_loss: 0.7557\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8377 - val_loss: 0.7556\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.8372 - val_loss: 0.7512\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8367 - val_loss: 0.7545\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.8370 - val_loss: 0.7526\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.8366 - val_loss: 0.7504\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8361 - val_loss: 0.7588\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8358 - val_loss: 0.7537\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 284us/step - loss: 0.8355 - val_loss: 0.7515\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.8356 - val_loss: 0.7517\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8354 - val_loss: 0.7493\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.8350 - val_loss: 0.7502\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 0.8346 - val_loss: 0.7504\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.8345 - val_loss: 0.7515\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8334 - val_loss: 0.7553\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.8333 - val_loss: 0.7480\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8336 - val_loss: 0.7508\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8325 - val_loss: 0.7476\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.8330 - val_loss: 0.7480\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.8319 - val_loss: 0.7492\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8315 - val_loss: 0.7476\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 0.8318 - val_loss: 0.7491\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8317 - val_loss: 0.7468\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.8313 - val_loss: 0.7467\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.8313 - val_loss: 0.7460\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8309 - val_loss: 0.7451\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8307 - val_loss: 0.7460\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.8305 - val_loss: 0.7471\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8299 - val_loss: 0.7503\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8297 - val_loss: 0.7455\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.8298 - val_loss: 0.7447\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8294 - val_loss: 0.7455\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8289 - val_loss: 0.7434\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.8285 - val_loss: 0.7475\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8281 - val_loss: 0.7459\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8278 - val_loss: 0.7517\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.8277 - val_loss: 0.7442\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8276 - val_loss: 0.7451\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8276 - val_loss: 0.7449\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8268 - val_loss: 0.7409\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.8268 - val_loss: 0.7411\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8267 - val_loss: 0.7465\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8265 - val_loss: 0.7434\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8254 - val_loss: 0.7442\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.8256 - val_loss: 0.7454\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8249 - val_loss: 0.7433\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.8244 - val_loss: 0.7412\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8246 - val_loss: 0.7419\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 0.8243 - val_loss: 0.7449\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.8228 - val_loss: 0.7419\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8236 - val_loss: 0.7381\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8234 - val_loss: 0.7393\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8228 - val_loss: 0.7394\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.8229 - val_loss: 0.7392\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.8228 - val_loss: 0.7430\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8221 - val_loss: 0.7393\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.8218 - val_loss: 0.7378\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.8219 - val_loss: 0.7403\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.8216 - val_loss: 0.7406\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8210 - val_loss: 0.7398\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8202 - val_loss: 0.7475\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.8205 - val_loss: 0.7392\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.8203 - val_loss: 0.7392\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8199 - val_loss: 0.7417\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.8203 - val_loss: 0.7394\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 0.8190 - val_loss: 0.7417\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.8193 - val_loss: 0.7357\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.8186 - val_loss: 0.7371\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 0.8186 - val_loss: 0.7378\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8186 - val_loss: 0.7355\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.8179 - val_loss: 0.7360\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "...#la regularization se debe incorporar a cada capa separadamente\n",
    "moptimizer = Adadelta(lr=0.01)\n",
    "idim=X_train_scaled.shape[1]\n",
    "model.add(Dense(256,input_dim=idim,kernel_initializer='uniform',W_regularizer=regularizers.l2(0.05)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=regularizers.l2(0.05)))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and RMSprop')\n",
    "im.legend(('Val_Loss', 'Loss' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesus\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, input_dim=1275, kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\Jesus\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "4100/9745 [===========>..................] - ETA: 2s - loss: 130.4622"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ef4b1e76d238>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmoptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "model = Sequential()\n",
    "...#la regularization se debe incorporar a cada capa separadamente\n",
    "moptimizer = Adadelta(lr=0.01)\n",
    "idim=X_train_scaled.shape[1]\n",
    "model.add(Dense(256,input_dim=idim,kernel_initializer='uniform',W_regularizer=regularizers.l2(0.05)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=regularizers.l2(0.05)))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(optimizer=moptimizer,loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x250c466bb00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and RMSprop')\n",
    "im.legend(('Val_Loss', 'Loss' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Entrene los modelos obtenidos en b) y c) utilizando *Dropout*. Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 1.5733 - val_loss: 0.9808\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.8161 - val_loss: 0.6666\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.7299 - val_loss: 0.5450\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.6865 - val_loss: 0.3872\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.6398 - val_loss: 0.3740\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.6065 - val_loss: 0.3554\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.5761 - val_loss: 0.3308\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.5594 - val_loss: 0.4153\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.5406 - val_loss: 0.3005\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.5448 - val_loss: 0.3206\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.5256 - val_loss: 0.3042\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.5130 - val_loss: 0.2894\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.5013 - val_loss: 0.3316\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.4992 - val_loss: 0.2757\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 285us/step - loss: 0.4902 - val_loss: 0.2803\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.4691 - val_loss: 0.2783\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.4728 - val_loss: 0.2535\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.4528 - val_loss: 0.3088\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.4544 - val_loss: 0.6281\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.4642 - val_loss: 0.2678\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.4430 - val_loss: 0.3232\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.4401 - val_loss: 0.3118\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.4394 - val_loss: 0.2996\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.4329 - val_loss: 0.2976\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.4215 - val_loss: 0.2303\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.4157 - val_loss: 0.2652\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.4211 - val_loss: 0.3453\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.4077 - val_loss: 0.2202\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.4140 - val_loss: 0.2597\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.4024 - val_loss: 0.4071\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.3947 - val_loss: 0.2311\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.4026 - val_loss: 0.2166\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.3894 - val_loss: 0.2087\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.3914 - val_loss: 0.2357\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.3849 - val_loss: 0.2644\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.3778 - val_loss: 0.2137\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.3803 - val_loss: 0.2535\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.3807 - val_loss: 0.2397\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.3827 - val_loss: 0.2020\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.3671 - val_loss: 0.2151\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.3688 - val_loss: 0.2111\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.3622 - val_loss: 0.1937\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.3700 - val_loss: 0.2937\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.3702 - val_loss: 0.2945\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.3487 - val_loss: 0.2186\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.3527 - val_loss: 0.1950\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.3641 - val_loss: 0.2220\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.3603 - val_loss: 0.1964\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 0.3484 - val_loss: 0.2382\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.3447 - val_loss: 0.2150\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.3511 - val_loss: 0.2429\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.3513 - val_loss: 0.3008\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.3489 - val_loss: 0.2890\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.3493 - val_loss: 0.2057\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.3363 - val_loss: 0.1833\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.3421 - val_loss: 0.3392\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.3376 - val_loss: 0.1929\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 0.3383 - val_loss: 0.1792\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.3359 - val_loss: 0.2135\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.3399 - val_loss: 0.2002\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.3412 - val_loss: 0.1835\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.3272 - val_loss: 0.2880\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.3319 - val_loss: 0.1762\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.3351 - val_loss: 0.1891\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.3297 - val_loss: 0.2622\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.3315 - val_loss: 0.2028\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.3298 - val_loss: 0.2197\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.3283 - val_loss: 0.2008\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.3270 - val_loss: 0.2990\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.3287 - val_loss: 0.2169\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.3157 - val_loss: 0.1837\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.3176 - val_loss: 0.1747\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.3159 - val_loss: 0.2003\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.3186 - val_loss: 0.1721\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.3246 - val_loss: 0.1882\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.3158 - val_loss: 0.1857\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.3126 - val_loss: 0.1811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.3223 - val_loss: 0.1710\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.3150 - val_loss: 0.2480\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.3125 - val_loss: 0.1692\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.3133 - val_loss: 0.2345\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.3202 - val_loss: 0.1775\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.3120 - val_loss: 0.1914\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.3114 - val_loss: 0.1734\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.3056 - val_loss: 0.1717\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.3064 - val_loss: 0.1627\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.3074 - val_loss: 0.2464\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.3045 - val_loss: 0.1719\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.3018 - val_loss: 0.1714\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.3045 - val_loss: 0.2253\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.3013 - val_loss: 0.2440\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.2986 - val_loss: 0.2629\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.2967 - val_loss: 0.1924\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.3015 - val_loss: 0.1581\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.3025 - val_loss: 0.1629\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.3026 - val_loss: 0.1592\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.3027 - val_loss: 0.2180\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.3015 - val_loss: 0.1785\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.2905 - val_loss: 0.1583\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.3017 - val_loss: 0.1794\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.2898 - val_loss: 0.2245\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.2939 - val_loss: 0.1752\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.3034 - val_loss: 0.2847\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.2900 - val_loss: 0.2001\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.2892 - val_loss: 0.2463\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.2979 - val_loss: 0.1726\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.2962 - val_loss: 0.1803\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.2824 - val_loss: 0.1701\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.2947 - val_loss: 0.1691\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.2926 - val_loss: 0.1720\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.2890 - val_loss: 0.1653\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.2920 - val_loss: 0.2299\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.2902 - val_loss: 0.2258\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.2851 - val_loss: 0.1977\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.2812 - val_loss: 0.3263\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.2900 - val_loss: 0.1870\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.2830 - val_loss: 0.1688\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.2899 - val_loss: 0.2208\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.2828 - val_loss: 0.1700\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2859 - val_loss: 0.1563\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.2836 - val_loss: 0.1590\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.2835 - val_loss: 0.1546\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.2880 - val_loss: 0.1499\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 275us/step - loss: 0.2735 - val_loss: 0.3231\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.2886 - val_loss: 0.1572\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.2781 - val_loss: 0.1550\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.2792 - val_loss: 0.1825\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.2802 - val_loss: 0.1794\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.2804 - val_loss: 0.2069\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.2731 - val_loss: 0.1520\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.2804 - val_loss: 0.1499\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.2780 - val_loss: 0.1837\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.2803 - val_loss: 0.1881s - loss: 0.\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.2688 - val_loss: 0.1528\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.2729 - val_loss: 0.1808\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.2803 - val_loss: 0.1469\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.2690 - val_loss: 0.1684\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.2739 - val_loss: 0.1572\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.2747 - val_loss: 0.1537\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.2735 - val_loss: 0.1502\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.2660 - val_loss: 0.1472\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.2810 - val_loss: 0.2471\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.2727 - val_loss: 0.1561\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.2708 - val_loss: 0.1506\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.2678 - val_loss: 0.1476\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.2697 - val_loss: 0.1897\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 0.2733 - val_loss: 0.1502\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 0.2678 - val_loss: 0.2193\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.2696 - val_loss: 0.1481\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.2686 - val_loss: 0.2510\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.2756 - val_loss: 0.1793\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.2661 - val_loss: 0.1506\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 0.2717 - val_loss: 0.2124\n",
      "Epoch 154/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.2681 - val_loss: 0.1845\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 292us/step - loss: 0.2654 - val_loss: 0.2700\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 0.2641 - val_loss: 0.2156\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.2636 - val_loss: 0.4114\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.2649 - val_loss: 0.2133\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 0.2703 - val_loss: 0.1459\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.2660 - val_loss: 0.1908\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 285us/step - loss: 0.2684 - val_loss: 0.1550\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.2616 - val_loss: 0.1925\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.2706 - val_loss: 0.2043\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 265us/step - loss: 0.2594 - val_loss: 0.1573\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.2587 - val_loss: 0.1483\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.2658 - val_loss: 0.1967\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.2657 - val_loss: 0.1548\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.2626 - val_loss: 0.1446\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.2624 - val_loss: 0.1514\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.2584 - val_loss: 0.1509\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 0.2586 - val_loss: 0.1723\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.2674 - val_loss: 0.1605\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 0.2622 - val_loss: 0.1602\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.2651 - val_loss: 0.1645\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.2560 - val_loss: 0.3729\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.2586 - val_loss: 0.1613\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.2629 - val_loss: 0.1601\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.2605 - val_loss: 0.1449\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.2602 - val_loss: 0.1631\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.2555 - val_loss: 0.1622\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.2541 - val_loss: 0.1414\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.2592 - val_loss: 0.1550\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.2550 - val_loss: 0.1757\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.2558 - val_loss: 0.1444\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.2555 - val_loss: 0.1443\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.2582 - val_loss: 0.1476\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.2563 - val_loss: 0.1437\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.2577 - val_loss: 0.1434\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.2595 - val_loss: 0.1839\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.2590 - val_loss: 0.1780\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.2539 - val_loss: 0.1711\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.2657 - val_loss: 0.2756\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.2536 - val_loss: 0.1453\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.2530 - val_loss: 0.1436\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.2570 - val_loss: 0.1539\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.2594 - val_loss: 0.2053\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.2454 - val_loss: 0.1935\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.2509 - val_loss: 0.1439\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.2554 - val_loss: 0.1440\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.2494 - val_loss: 0.1927\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.2465 - val_loss: 0.1438\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.2558 - val_loss: 0.1638\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.2493 - val_loss: 0.1406\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.2518 - val_loss: 0.1569\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2511 - val_loss: 0.1839\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.2516 - val_loss: 0.1377\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.2543 - val_loss: 0.1602\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.2507 - val_loss: 0.1425\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.2593 - val_loss: 0.1516\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.2452 - val_loss: 0.1431\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.2512 - val_loss: 0.2057\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.2494 - val_loss: 0.1385\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.2490 - val_loss: 0.1964\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.2477 - val_loss: 0.1506\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.2500 - val_loss: 0.1721\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.2509 - val_loss: 0.1510\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.2538 - val_loss: 0.1981\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.2469 - val_loss: 0.2049\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.2501 - val_loss: 0.2397\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.2520 - val_loss: 0.1619\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.2514 - val_loss: 0.1410\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.2427 - val_loss: 0.1419\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.2455 - val_loss: 0.1486\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.2506 - val_loss: 0.2353\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.2520 - val_loss: 0.2173\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.2478 - val_loss: 0.1357\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 287us/step - loss: 0.2448 - val_loss: 0.1544\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.2485 - val_loss: 0.1635\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.2494 - val_loss: 0.1386\n",
      "Epoch 230/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.2452 - val_loss: 0.1392\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.2497 - val_loss: 0.1748\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.2467 - val_loss: 0.1513\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.2367 - val_loss: 0.1505\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.2479 - val_loss: 0.1361\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.2410 - val_loss: 0.1552\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.2469 - val_loss: 0.1563\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.2521 - val_loss: 0.1526\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.2452 - val_loss: 0.1501\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.2443 - val_loss: 0.1560\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.2376 - val_loss: 0.2627\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.2454 - val_loss: 0.1545\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.2410 - val_loss: 0.1731\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.2434 - val_loss: 0.1469\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.2457 - val_loss: 0.1680\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.2308 - val_loss: 0.1528\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.2466 - val_loss: 0.1384\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.2418 - val_loss: 0.1389\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.2372 - val_loss: 0.1350\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.2495 - val_loss: 0.3529\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.2389 - val_loss: 0.1413\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1858e6b4f60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Tansig Dropout')\n",
    "im.legend(('Val_Loss', 'Loss' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 9.5699 - val_loss: 15.4160\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 3.1428 - val_loss: 3.3694\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 2.4393 - val_loss: 2.1796\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 2.0310 - val_loss: 1.3725\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.4316 - val_loss: 1.2231\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 1.1824 - val_loss: 1.0907\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 1.2491 - val_loss: 1.1067\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.1504 - val_loss: 1.5653\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 1.0713 - val_loss: 0.9244\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 1.0450 - val_loss: 0.7692\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.9660 - val_loss: 0.7786\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.8745 - val_loss: 0.6765\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.9412 - val_loss: 0.7163\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.8906 - val_loss: 0.7530\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.8093 - val_loss: 0.7031\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.7630 - val_loss: 0.7526\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.7488 - val_loss: 0.6095\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.7857 - val_loss: 0.5427\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.7294 - val_loss: 0.6484\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - ETA: 0s - loss: 0.701 - 2s 204us/step - loss: 0.7033 - val_loss: 0.6684\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.6991 - val_loss: 0.6346\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.7604 - val_loss: 0.6502\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.7613 - val_loss: 0.7590\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.6776 - val_loss: 0.6589\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.6748 - val_loss: 0.4825\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.6438 - val_loss: 0.4855\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.6244 - val_loss: 0.5674\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.6363 - val_loss: 0.5276\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.6787 - val_loss: 0.8142\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.6259 - val_loss: 0.7165\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.6464 - val_loss: 0.5518\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.5997 - val_loss: 0.5969\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.5859 - val_loss: 0.4638\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.5646 - val_loss: 0.5321\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.5523 - val_loss: 0.5140\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.5782 - val_loss: 0.5687\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.5502 - val_loss: 0.5292\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.5392 - val_loss: 0.5574\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.5610 - val_loss: 0.7080\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.5127 - val_loss: 0.5181\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.5151 - val_loss: 0.4883\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.5498 - val_loss: 0.5532\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.5464 - val_loss: 0.4384\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.5451 - val_loss: 0.4704\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.5353 - val_loss: 0.5132\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.5386 - val_loss: 0.6164\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.5764 - val_loss: 0.4760\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.5194 - val_loss: 0.5592\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.4897 - val_loss: 0.4969\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.5658 - val_loss: 0.8736\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.4925 - val_loss: 0.4977\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.5173 - val_loss: 0.6969\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.5096 - val_loss: 0.5396\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.4655 - val_loss: 0.5081\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.4611 - val_loss: 0.7191\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.4873 - val_loss: 0.5235\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.4761 - val_loss: 1.1108\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.4675 - val_loss: 0.6290\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.4750 - val_loss: 0.6078\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.4544 - val_loss: 0.6183\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.4618 - val_loss: 0.5049\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.4599 - val_loss: 0.5169\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.4650 - val_loss: 0.5783\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.4712 - val_loss: 0.5550\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.4679 - val_loss: 0.5159\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.4727 - val_loss: 0.8105\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4801 - val_loss: 0.5383\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.4633 - val_loss: 0.6693\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4441 - val_loss: 0.5456\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.4631 - val_loss: 0.6202\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.4601 - val_loss: 1.1620\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4520 - val_loss: 0.5431\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4699 - val_loss: 0.6683\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4772 - val_loss: 0.6758\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.4126 - val_loss: 0.5328\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4438 - val_loss: 0.5994\n",
      "Epoch 77/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3985 - val_loss: 0.6833\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.4295 - val_loss: 0.5622\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.4305 - val_loss: 0.5119\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.4194 - val_loss: 0.5487\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.4428 - val_loss: 0.5832\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.4146 - val_loss: 0.5655\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4082 - val_loss: 0.5954\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4021 - val_loss: 0.6570\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.4015 - val_loss: 0.5767\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.4057 - val_loss: 0.6009\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.4021 - val_loss: 0.5648\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4075 - val_loss: 0.5670\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.4076 - val_loss: 0.5687\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.3885 - val_loss: 0.7272\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.4085 - val_loss: 0.6617\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.4052 - val_loss: 0.5990\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.3838 - val_loss: 0.6252\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4131 - val_loss: 0.7121\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3853 - val_loss: 0.7599\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.3874 - val_loss: 0.6882\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.4066 - val_loss: 0.5805\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.3958 - val_loss: 0.7861\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3964 - val_loss: 0.6066\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3924 - val_loss: 0.5790\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3743 - val_loss: 0.7638\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3812 - val_loss: 0.5943\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3841 - val_loss: 0.5794\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3838 - val_loss: 0.6181\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3804 - val_loss: 0.6494\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.3659 - val_loss: 0.6639\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3665 - val_loss: 0.6404\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3943 - val_loss: 0.6938\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3509 - val_loss: 0.8121\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3696 - val_loss: 0.7452\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3751 - val_loss: 0.6998\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3525 - val_loss: 0.6476\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3624 - val_loss: 0.6484\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.3734 - val_loss: 0.6390\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3537 - val_loss: 0.6486\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.3522 - val_loss: 0.6361\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3561 - val_loss: 0.9252\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3534 - val_loss: 0.7580\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3326 - val_loss: 0.8541\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3374 - val_loss: 0.6904\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.3464 - val_loss: 0.6790\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3402 - val_loss: 0.6080\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.3187 - val_loss: 0.6637\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3407 - val_loss: 0.6668\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3401 - val_loss: 0.7117\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.3592 - val_loss: 0.7019\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.3789 - val_loss: 0.7902\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3636 - val_loss: 0.8312\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3490 - val_loss: 0.7225\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3356 - val_loss: 0.6896\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.3158 - val_loss: 0.6879\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3301 - val_loss: 0.6805\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.3217 - val_loss: 0.7984\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.3311 - val_loss: 0.7191\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3421 - val_loss: 0.7357\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3158 - val_loss: 0.6908\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3254 - val_loss: 0.7469\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.3216 - val_loss: 0.7537\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.3265 - val_loss: 0.7689\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3556 - val_loss: 0.6903\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3277 - val_loss: 0.6952\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3292 - val_loss: 0.7825\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3016 - val_loss: 0.7514\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.3087 - val_loss: 0.7356\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.3307 - val_loss: 0.9146\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.3176 - val_loss: 0.7097\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.3141 - val_loss: 0.7254\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.3088 - val_loss: 0.7279\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3197 - val_loss: 0.9046\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3389 - val_loss: 0.7684\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.3219 - val_loss: 0.7606\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.3075 - val_loss: 0.7481\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3204 - val_loss: 0.8547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3138 - val_loss: 0.8508\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3175 - val_loss: 0.7295\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3216 - val_loss: 0.9665\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.3053 - val_loss: 0.8171\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.3134 - val_loss: 0.7644\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3047 - val_loss: 0.7679\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.3134 - val_loss: 0.8169\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.2998 - val_loss: 0.8494\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.2946 - val_loss: 0.7552\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.2925 - val_loss: 0.7495\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.2923 - val_loss: 0.7653\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.2985 - val_loss: 0.8255\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.3016 - val_loss: 0.8917\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.2837 - val_loss: 0.7568\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.2914 - val_loss: 0.7857\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.2885 - val_loss: 0.9396\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.2918 - val_loss: 0.7247\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.2924 - val_loss: 0.8345\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.2978 - val_loss: 0.8124\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.2870 - val_loss: 0.8413\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.2996 - val_loss: 0.7686\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2937 - val_loss: 0.7925\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.2846 - val_loss: 0.7597\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2853 - val_loss: 0.8184\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.2909 - val_loss: 0.7764\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.2808 - val_loss: 0.7858\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.2921 - val_loss: 0.8128\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.3132 - val_loss: 0.8121\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.2764 - val_loss: 0.8062\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.2954 - val_loss: 0.8668\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.2840 - val_loss: 1.0900\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2965 - val_loss: 0.8051\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.2832 - val_loss: 0.8401\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.3008 - val_loss: 0.8642\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.2664 - val_loss: 0.9156\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.2725 - val_loss: 0.8713\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.2907 - val_loss: 0.7747\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.2699 - val_loss: 0.7696\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.2928 - val_loss: 0.8512\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.2811 - val_loss: 0.8737\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.2715 - val_loss: 0.7453\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.2815 - val_loss: 0.7683\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.2628 - val_loss: 0.7938\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.2719 - val_loss: 0.7842\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.2494 - val_loss: 0.7812\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 281us/step - loss: 0.2719 - val_loss: 0.8165\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.2756 - val_loss: 0.7492\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 4s 431us/step - loss: 0.2706 - val_loss: 1.0318\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 4s 422us/step - loss: 0.2755 - val_loss: 0.8192\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 4s 424us/step - loss: 0.2731 - val_loss: 0.8027\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 4s 452us/step - loss: 0.2668 - val_loss: 0.7679\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 4s 402us/step - loss: 0.2538 - val_loss: 0.8555\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.2652 - val_loss: 0.8416\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.2679 - val_loss: 1.2435\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.2677 - val_loss: 0.8448\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 4s 392us/step - loss: 0.2626 - val_loss: 0.9557\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 4s 386us/step - loss: 0.2428 - val_loss: 0.8217\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 0.2627 - val_loss: 0.8386\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 4s 395us/step - loss: 0.2525 - val_loss: 0.8216\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: 0.2502 - val_loss: 1.1205\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 4s 411us/step - loss: 0.2583 - val_loss: 0.8546\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 4s 429us/step - loss: 0.2430 - val_loss: 0.7972\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 4s 417us/step - loss: 0.2616 - val_loss: 0.8939\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 4s 431us/step - loss: 0.2748 - val_loss: 0.8048\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.2612 - val_loss: 1.0208\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 0.2659 - val_loss: 1.0933\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 4s 432us/step - loss: 0.2624 - val_loss: 0.8628\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 4s 442us/step - loss: 0.2478 - val_loss: 0.9844\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: 0.2630 - val_loss: 0.8659\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 4s 417us/step - loss: 0.2608 - val_loss: 0.8570\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: 0.2545 - val_loss: 0.8848\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 5s 476us/step - loss: 0.2632 - val_loss: 0.8851\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 5s 482us/step - loss: 0.2441 - val_loss: 0.8908\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.2424 - val_loss: 0.8990\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 4s 455us/step - loss: 0.2353 - val_loss: 0.8233\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 4s 451us/step - loss: 0.2383 - val_loss: 0.8696\n",
      "Epoch 230/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 4s 420us/step - loss: 0.2467 - val_loss: 0.8850\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 4s 445us/step - loss: 0.2422 - val_loss: 0.9565\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 4s 451us/step - loss: 0.2469 - val_loss: 0.8508\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 5s 496us/step - loss: 0.2419 - val_loss: 0.8686\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 0.2482 - val_loss: 0.8899\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 0.2309 - val_loss: 0.9106\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 4s 445us/step - loss: 0.2558 - val_loss: 0.8710\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.2473 - val_loss: 0.9350\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 4s 430us/step - loss: 0.2504 - val_loss: 0.8930\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.2347 - val_loss: 0.9075\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 4s 447us/step - loss: 0.2334 - val_loss: 0.8360\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 4s 416us/step - loss: 0.2380 - val_loss: 0.8349\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 4s 433us/step - loss: 0.2398 - val_loss: 0.8511\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 4s 418us/step - loss: 0.2404 - val_loss: 0.8697\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: 0.2438 - val_loss: 0.8519\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.2339 - val_loss: 0.8461\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 4s 418us/step - loss: 0.2275 - val_loss: 0.8793\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 4s 439us/step - loss: 0.2301 - val_loss: 0.8666\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 4s 417us/step - loss: 0.2415 - val_loss: 0.8426\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: 0.2353 - val_loss: 1.0742\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: 0.2297 - val_loss: 1.0177\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1858f761ef0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and RMSprop')\n",
    "im.legend(('Val_Loss', 'Loss' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Fijando todos los demás hiper-parámetros del modelo definido en b) y en c), utilice validación cruzada con un número de *folds* igual a *K* = 5 y *K*=10 para determinar el mejor valor correspondiente a un parámetro que usted elija (tasa de aprendizaje, número de neuronas, parámetro de regularización, etc) ¿El mejor parámetro para la red con sigmoidal es distinto que para ReLU? ¿Porqué sucede? Además mida el error real del modelo sobre el conjunto de pruebas, compare y concluya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "Xm = X_train_scaled.values\n",
    "ym = y_train\n",
    "kfold = cross_validation.KFold(len(Xm), 10)\n",
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    ...# create model\n",
    "    model = #model with hiperparam\n",
    "    ...# Compile model\n",
    "    model.compile(optimizer=,loss='mean_squared_error')\n",
    "    ...# Fit the model\n",
    "    model.fit(Xm[train], ym[train], epochs=250)\n",
    "    ...# evaluate the model\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
