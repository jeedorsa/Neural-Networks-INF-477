{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center>  Entendimiento de imágenes de personas <br>Andrea Reales && Jesus Ortiz </center>\n",
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"10%\" height=\"10%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Entendimiento de imágenes de personas\n",
    "\n",
    "El problema de inferir ciertas características de una persona a través de una foto de ella puede resultar bastante dificil incluso para nosotros, como por ejemplo de qué país es, la emoción que expresa, la edad que tiene, o el género. La automatización de este proceso para que máquinas logren identificar ciertas características de una persona puede ser algo crucial para el futuro desarrollo de Inteligencia Artificial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/6B072GE.jpg\" width=\"60%\" height=\"20%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta actividad trabajaremos con unos datos (imágenes) con la tarea de predecir la **edad** (*target value*) de la persona en la imagen. Los datos con corresponden a 3640 imágenes de Flickr de rostros de personas, pero, debido a que trabajamos con redes *feed forward*, se trabajará con representaciones de características extraídas. Para ésto necesitará descargar los datos del siguiente __[link](http://chenlab.ece.cornell.edu/people/Andy/ImagesOfGroups.html)__ en el extracto de *ageGenderClassification* o a través de la consola Unix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/ageGenderClassification.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trabajará con archivos *.mat* que pueden ser cargados de la siguiente manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para descripción sobre las columnas están en el archivo readme a través del siguiente __[link](http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt)__ o a través de la consola Unix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Cargue los datos dos dataset de entrenamiento y de pruebas ¿Cuántos datos hay en cada conjunto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_train = sio.loadmat(\"C:/Users/Jesus/Documents/GitHub/AgeGenderClassification/eventrain.mat\")\n",
    "mat_test = sio.loadmat(\"C:/Users/Jesus/Documents/GitHub/AgeGenderClassification/eventest.mat\")\n",
    "data_train= mat_train[\"trcoll\"][0][0]\n",
    "data_test= mat_test[\"tecoll\"][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=len(data_train)\n",
    "y=len(data_train[10])\n",
    "z=x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de caracteristicas de entrada= 11 Total de muestras= 3500 Total datos entrenamiento= 38500\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero de caracteristicas de entrada=\",x,\"Total de muestras=\",y,\"Total datos entrenamiento=\",z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51],\n",
       "       [28],\n",
       "       [28],\n",
       "       ...,\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 1]], dtype=uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=len(data_test)\n",
    "y1=len(data_test[0])\n",
    "z1=x1*y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de caracteristicas de entrada= 11 Total de muestras= 1050 Total datos test= 11550\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero de caracteristicas de entrada=\",x1,\"Total de muestras=\",y1,\"Total datos test=\",z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Eliga cuál representación utilizará para trabajar los datos y entregárselos como *input* al modelo neuronal denso. Además extraiga las etiquetas del problema. Describa los datos utilziados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageClass_train = data_train[1] #target\n",
    "ffcoefs_train = data_train[4]   #it can be used as representation: fisherface space\n",
    "X_train=ffcoefs_train[:int(0.8*y)]                       \n",
    "X_val=ffcoefs_train[int(0.8*y)::]\n",
    "y_train=ageClass_train[:int(0.8*y)]                       \n",
    "y_val=ageClass_train[int(0.8*y)::]    \n",
    "y_test = data_test[1] #target\n",
    "X_test = data_test[4]   #it can be used as representation: fisherface space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  16.453856 ,   34.83934  ,   48.7035   , ..., -198.66533  ,\n",
       "         109.027985 ,   78.18502  ],\n",
       "       [  53.82186  ,   93.61051  ,   65.62783  , ...,  657.3836   ,\n",
       "         820.10693  , -132.57458  ],\n",
       "       [   6.8143125,  -20.586594 ,  -50.35877  , ..., -409.62454  ,\n",
       "        -196.4159   ,  111.17357  ],\n",
       "       ...,\n",
       "       [ -34.0235   ,  -75.17138  ,  -87.20992  , ..., -212.65627  ,\n",
       "        -301.569    , -366.88312  ],\n",
       "       [  17.623787 ,    2.0862489,  -15.78859  , ...,   20.659277 ,\n",
       "         155.44498  ,  404.44925  ],\n",
       "       [ -29.624369 ,  -83.67899  ,  -97.403336 , ...,  416.36523  ,\n",
       "         415.95398  ,  -69.081314 ]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Defina y entrene una modelo de red neuronal *feed forward* para la inferencia de la edad de la persona a través de la representación escogida. Intente llegar a un *mse* menor a 100 en el conjunto de pruebas. Recuerde que **NO** puede seleccionar modelos a través del conjunto de pruebas. Visualice sus resultados si estima conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota: Puede notar que la cantidad de edades presentes en el problema son pocas (1,  5, 10, 16, 28, 51 o 75 años), por lo que puede tratar al problema así como de regresión o clasificación (considerando cada edad como una clase)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ayuda:\n",
    "> Para problemas de clasificación de múltiples clases es necesario transformar las etiquetas categóricas en *one hot vector*, donde cada columna del vector representará una categoría. Por ejemplo, si existen tres categorías (perro, gato, ratón), la categoría perro puede ser codificada como [1,0,0], y la categoría ratón puede ser codificada como [0,0,1]. Para ésto la librería *keras* nos ayuda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/2000\n",
      "2800/2800 [==============================] - 2s 751us/step - loss: 1001.7054 - val_loss: 1180.5723\n",
      "Epoch 2/2000\n",
      "2800/2800 [==============================] - 0s 170us/step - loss: 645.1332 - val_loss: 475.5410\n",
      "Epoch 3/2000\n",
      "2800/2800 [==============================] - 0s 177us/step - loss: 642.9073 - val_loss: 936.8676\n",
      "Epoch 4/2000\n",
      "2800/2800 [==============================] - 1s 188us/step - loss: 571.6715 - val_loss: 1075.7999\n",
      "Epoch 5/2000\n",
      "2800/2800 [==============================] - 1s 181us/step - loss: 557.4706 - val_loss: 689.8051\n",
      "Epoch 6/2000\n",
      "2800/2800 [==============================] - 1s 180us/step - loss: 560.7828 - val_loss: 513.2231\n",
      "Epoch 7/2000\n",
      "2800/2800 [==============================] - 1s 181us/step - loss: 553.9975 - val_loss: 469.6561\n",
      "Epoch 8/2000\n",
      "2800/2800 [==============================] - 0s 177us/step - loss: 555.8697 - val_loss: 527.3129\n",
      "Epoch 9/2000\n",
      "2800/2800 [==============================] - 1s 192us/step - loss: 547.7474 - val_loss: 606.7319\n",
      "Epoch 10/2000\n",
      "2800/2800 [==============================] - 1s 188us/step - loss: 527.1710 - val_loss: 627.4130\n",
      "Epoch 11/2000\n",
      "2800/2800 [==============================] - 1s 185us/step - loss: 526.0669 - val_loss: 678.5909\n",
      "Epoch 12/2000\n",
      "2800/2800 [==============================] - 1s 191us/step - loss: 525.7501 - val_loss: 1359.3673\n",
      "Epoch 13/2000\n",
      "2800/2800 [==============================] - 1s 208us/step - loss: 514.7316 - val_loss: 606.4198\n",
      "Epoch 14/2000\n",
      "2800/2800 [==============================] - 1s 260us/step - loss: 524.3053 - val_loss: 532.0498\n",
      "Epoch 15/2000\n",
      "2800/2800 [==============================] - 1s 237us/step - loss: 530.1251 - val_loss: 659.2043\n",
      "Epoch 16/2000\n",
      "2800/2800 [==============================] - 1s 255us/step - loss: 510.3953 - val_loss: 746.2582\n",
      "Epoch 17/2000\n",
      "2800/2800 [==============================] - 1s 262us/step - loss: 511.5425 - val_loss: 725.8241\n",
      "Epoch 18/2000\n",
      "2800/2800 [==============================] - 1s 254us/step - loss: 535.7713 - val_loss: 516.3731\n",
      "Epoch 19/2000\n",
      "2800/2800 [==============================] - 1s 227us/step - loss: 502.3695 - val_loss: 541.5253\n",
      "Epoch 20/2000\n",
      "2800/2800 [==============================] - 1s 246us/step - loss: 500.0447 - val_loss: 921.9546\n",
      "Epoch 21/2000\n",
      "2800/2800 [==============================] - 1s 247us/step - loss: 506.3455 - val_loss: 516.8161\n",
      "Epoch 22/2000\n",
      "2800/2800 [==============================] - 1s 238us/step - loss: 500.9263 - val_loss: 465.8601\n",
      "Epoch 23/2000\n",
      "2800/2800 [==============================] - 1s 247us/step - loss: 502.7379 - val_loss: 642.6722\n",
      "Epoch 24/2000\n",
      "2800/2800 [==============================] - 1s 232us/step - loss: 506.5642 - val_loss: 757.2074\n",
      "Epoch 25/2000\n",
      "2800/2800 [==============================] - 1s 245us/step - loss: 507.9096 - val_loss: 652.4700\n",
      "Epoch 26/2000\n",
      "2800/2800 [==============================] - 1s 278us/step - loss: 501.6122 - val_loss: 691.3119\n",
      "Epoch 27/2000\n",
      "2800/2800 [==============================] - 1s 248us/step - loss: 511.4285 - val_loss: 901.0406\n",
      "Epoch 28/2000\n",
      "2800/2800 [==============================] - 1s 252us/step - loss: 493.5687 - val_loss: 702.8520\n",
      "Epoch 29/2000\n",
      "2800/2800 [==============================] - 1s 259us/step - loss: 493.4055 - val_loss: 862.8401\n",
      "Epoch 30/2000\n",
      "2800/2800 [==============================] - 1s 268us/step - loss: 491.3136 - val_loss: 703.6789\n",
      "Epoch 31/2000\n",
      "2800/2800 [==============================] - 1s 283us/step - loss: 524.6981 - val_loss: 767.4841\n",
      "Epoch 32/2000\n",
      "2800/2800 [==============================] - 1s 253us/step - loss: 496.3201 - val_loss: 1029.8559\n",
      "Epoch 33/2000\n",
      "2800/2800 [==============================] - 1s 255us/step - loss: 501.8698 - val_loss: 559.0746\n",
      "Epoch 34/2000\n",
      "2800/2800 [==============================] - 1s 270us/step - loss: 508.3074 - val_loss: 495.5522\n",
      "Epoch 35/2000\n",
      "2800/2800 [==============================] - 1s 247us/step - loss: 491.8423 - val_loss: 608.3987\n",
      "Epoch 36/2000\n",
      "2800/2800 [==============================] - 1s 268us/step - loss: 495.3251 - val_loss: 573.2642\n",
      "Epoch 37/2000\n",
      "2800/2800 [==============================] - 1s 259us/step - loss: 481.8059 - val_loss: 699.2460\n",
      "Epoch 38/2000\n",
      "2800/2800 [==============================] - 1s 257us/step - loss: 479.0911 - val_loss: 479.4535\n",
      "Epoch 39/2000\n",
      "2800/2800 [==============================] - 1s 267us/step - loss: 493.4501 - val_loss: 602.4963\n",
      "Epoch 40/2000\n",
      "2800/2800 [==============================] - 1s 261us/step - loss: 482.3589 - val_loss: 455.2763\n",
      "Epoch 41/2000\n",
      "2800/2800 [==============================] - 1s 237us/step - loss: 484.1131 - val_loss: 539.7550\n",
      "Epoch 42/2000\n",
      "2800/2800 [==============================] - 1s 273us/step - loss: 473.6303 - val_loss: 504.3598\n",
      "Epoch 43/2000\n",
      "2800/2800 [==============================] - 1s 255us/step - loss: 473.6453 - val_loss: 465.0126\n",
      "Epoch 44/2000\n",
      "2800/2800 [==============================] - 1s 264us/step - loss: 503.0721 - val_loss: 833.0052\n",
      "Epoch 45/2000\n",
      "2800/2800 [==============================] - 1s 251us/step - loss: 487.5954 - val_loss: 613.1148\n",
      "Epoch 46/2000\n",
      "2800/2800 [==============================] - 1s 234us/step - loss: 500.5378 - val_loss: 614.7729\n",
      "Epoch 47/2000\n",
      "2800/2800 [==============================] - 1s 245us/step - loss: 480.5478 - val_loss: 809.2789\n",
      "Epoch 48/2000\n",
      "2800/2800 [==============================] - 1s 256us/step - loss: 481.6059 - val_loss: 665.3218\n",
      "Epoch 49/2000\n",
      "2800/2800 [==============================] - 1s 254us/step - loss: 468.2780 - val_loss: 615.4477\n",
      "Epoch 50/2000\n",
      "2800/2800 [==============================] - 1s 250us/step - loss: 465.7525 - val_loss: 817.0489\n",
      "Epoch 51/2000\n",
      "2800/2800 [==============================] - 1s 260us/step - loss: 477.9366 - val_loss: 520.4952\n",
      "Epoch 52/2000\n",
      "2800/2800 [==============================] - 1s 241us/step - loss: 474.0304 - val_loss: 968.4966\n",
      "Epoch 53/2000\n",
      "2800/2800 [==============================] - 1s 280us/step - loss: 467.6572 - val_loss: 630.7945\n",
      "Epoch 54/2000\n",
      "2800/2800 [==============================] - 1s 265us/step - loss: 477.8899 - val_loss: 691.4567\n",
      "Epoch 55/2000\n",
      "2800/2800 [==============================] - 1s 299us/step - loss: 467.7903 - val_loss: 623.5636\n",
      "Epoch 56/2000\n",
      "2800/2800 [==============================] - 1s 246us/step - loss: 483.4551 - val_loss: 476.0067\n",
      "Epoch 57/2000\n",
      "2800/2800 [==============================] - 1s 240us/step - loss: 471.7478 - val_loss: 516.4088\n",
      "Epoch 58/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 455.773 - 1s 238us/step - loss: 454.7105 - val_loss: 545.9561\n",
      "Epoch 59/2000\n",
      "2800/2800 [==============================] - 1s 283us/step - loss: 461.1895 - val_loss: 1031.1916\n",
      "Epoch 60/2000\n",
      "2800/2800 [==============================] - 1s 281us/step - loss: 448.8496 - val_loss: 516.6242\n",
      "Epoch 61/2000\n",
      "2800/2800 [==============================] - 1s 265us/step - loss: 468.5877 - val_loss: 587.6182\n",
      "Epoch 62/2000\n",
      "2800/2800 [==============================] - 1s 257us/step - loss: 462.8292 - val_loss: 462.4214\n",
      "Epoch 63/2000\n",
      "2800/2800 [==============================] - 1s 271us/step - loss: 449.2728 - val_loss: 723.1023\n",
      "Epoch 64/2000\n",
      "2800/2800 [==============================] - 1s 261us/step - loss: 446.3425 - val_loss: 564.0882\n",
      "Epoch 65/2000\n",
      "2800/2800 [==============================] - 1s 283us/step - loss: 453.8486 - val_loss: 480.4116\n",
      "Epoch 66/2000\n",
      "2800/2800 [==============================] - 1s 264us/step - loss: 440.1030 - val_loss: 683.5137\n",
      "Epoch 67/2000\n",
      "2800/2800 [==============================] - 1s 270us/step - loss: 458.9513 - val_loss: 756.9792\n",
      "Epoch 68/2000\n",
      "2800/2800 [==============================] - 1s 269us/step - loss: 463.5848 - val_loss: 746.1190\n",
      "Epoch 69/2000\n",
      "2800/2800 [==============================] - 1s 267us/step - loss: 447.0892 - val_loss: 680.2456\n",
      "Epoch 70/2000\n",
      "2800/2800 [==============================] - 1s 262us/step - loss: 446.3514 - val_loss: 539.4481\n",
      "Epoch 71/2000\n",
      "2800/2800 [==============================] - 1s 286us/step - loss: 454.0954 - val_loss: 540.8679\n",
      "Epoch 72/2000\n",
      "2800/2800 [==============================] - 1s 245us/step - loss: 438.7765 - val_loss: 1196.7879\n",
      "Epoch 73/2000\n",
      "2800/2800 [==============================] - 1s 278us/step - loss: 447.7235 - val_loss: 494.2632\n",
      "Epoch 74/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 1s 253us/step - loss: 442.7425 - val_loss: 1092.3120\n",
      "Epoch 75/2000\n",
      "2800/2800 [==============================] - 1s 244us/step - loss: 466.3797 - val_loss: 580.7908\n",
      "Epoch 76/2000\n",
      "2800/2800 [==============================] - 1s 238us/step - loss: 438.1353 - val_loss: 873.8054\n",
      "Epoch 77/2000\n",
      "2800/2800 [==============================] - 1s 249us/step - loss: 435.4059 - val_loss: 1516.5683\n",
      "Epoch 78/2000\n",
      "2800/2800 [==============================] - 1s 260us/step - loss: 427.2141 - val_loss: 784.1926\n",
      "Epoch 79/2000\n",
      "2800/2800 [==============================] - 1s 285us/step - loss: 443.2477 - val_loss: 570.4542\n",
      "Epoch 80/2000\n",
      "2800/2800 [==============================] - 1s 265us/step - loss: 426.9937 - val_loss: 854.2529\n",
      "Epoch 81/2000\n",
      "2800/2800 [==============================] - 1s 256us/step - loss: 430.3472 - val_loss: 509.8691\n",
      "Epoch 82/2000\n",
      "2800/2800 [==============================] - 1s 284us/step - loss: 427.8635 - val_loss: 1085.0468\n",
      "Epoch 83/2000\n",
      "2800/2800 [==============================] - 1s 246us/step - loss: 442.6921 - val_loss: 507.8018\n",
      "Epoch 84/2000\n",
      "2800/2800 [==============================] - 1s 246us/step - loss: 431.7964 - val_loss: 481.1487\n",
      "Epoch 85/2000\n",
      "2800/2800 [==============================] - 1s 300us/step - loss: 435.9120 - val_loss: 487.1811\n",
      "Epoch 86/2000\n",
      "2800/2800 [==============================] - 1s 301us/step - loss: 443.8090 - val_loss: 563.4328\n",
      "Epoch 87/2000\n",
      "2800/2800 [==============================] - 1s 315us/step - loss: 440.5579 - val_loss: 772.2213\n",
      "Epoch 88/2000\n",
      "2800/2800 [==============================] - 1s 265us/step - loss: 489.1098 - val_loss: 544.1178\n",
      "Epoch 89/2000\n",
      "2800/2800 [==============================] - 1s 254us/step - loss: 421.1650 - val_loss: 766.2476\n",
      "Epoch 90/2000\n",
      "2800/2800 [==============================] - 1s 235us/step - loss: 428.3245 - val_loss: 538.9553\n",
      "Epoch 91/2000\n",
      "2800/2800 [==============================] - 1s 238us/step - loss: 447.3779 - val_loss: 642.8862\n",
      "Epoch 92/2000\n",
      "2800/2800 [==============================] - 1s 258us/step - loss: 433.6845 - val_loss: 606.4713\n",
      "Epoch 93/2000\n",
      "2800/2800 [==============================] - 1s 258us/step - loss: 426.7838 - val_loss: 651.6651\n",
      "Epoch 94/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 415.048 - 1s 270us/step - loss: 416.5400 - val_loss: 527.5168\n",
      "Epoch 95/2000\n",
      "2800/2800 [==============================] - 1s 250us/step - loss: 433.0004 - val_loss: 484.3052\n",
      "Epoch 96/2000\n",
      "2800/2800 [==============================] - 1s 276us/step - loss: 435.0430 - val_loss: 590.8851\n",
      "Epoch 97/2000\n",
      "2800/2800 [==============================] - 1s 274us/step - loss: 425.6376 - val_loss: 612.1236\n",
      "Epoch 98/2000\n",
      "2800/2800 [==============================] - 1s 257us/step - loss: 424.5202 - val_loss: 886.8551\n",
      "Epoch 99/2000\n",
      "2800/2800 [==============================] - 1s 264us/step - loss: 424.7841 - val_loss: 517.6789\n",
      "Epoch 100/2000\n",
      "2800/2800 [==============================] - 1s 272us/step - loss: 413.3371 - val_loss: 630.6600\n",
      "Epoch 101/2000\n",
      "2800/2800 [==============================] - 1s 278us/step - loss: 416.8103 - val_loss: 583.6830\n",
      "Epoch 102/2000\n",
      "2800/2800 [==============================] - 1s 264us/step - loss: 470.8193 - val_loss: 483.3210\n",
      "Epoch 103/2000\n",
      "2800/2800 [==============================] - 1s 295us/step - loss: 413.2850 - val_loss: 630.2036\n",
      "Epoch 104/2000\n",
      "2800/2800 [==============================] - 1s 278us/step - loss: 421.9280 - val_loss: 987.7777\n",
      "Epoch 105/2000\n",
      "2800/2800 [==============================] - 1s 317us/step - loss: 430.3156 - val_loss: 513.5954\n",
      "Epoch 106/2000\n",
      "2800/2800 [==============================] - 1s 304us/step - loss: 422.7506 - val_loss: 616.7969\n",
      "Epoch 107/2000\n",
      "2800/2800 [==============================] - 1s 248us/step - loss: 398.5460 - val_loss: 522.3076\n",
      "Epoch 108/2000\n",
      "2800/2800 [==============================] - 1s 270us/step - loss: 393.7332 - val_loss: 630.5587\n",
      "Epoch 109/2000\n",
      "2800/2800 [==============================] - 1s 282us/step - loss: 429.5972 - val_loss: 595.0260\n",
      "Epoch 110/2000\n",
      "2800/2800 [==============================] - 1s 281us/step - loss: 418.9217 - val_loss: 665.9955\n",
      "Epoch 111/2000\n",
      "2800/2800 [==============================] - 1s 256us/step - loss: 419.2384 - val_loss: 519.0352\n",
      "Epoch 112/2000\n",
      "2800/2800 [==============================] - 1s 227us/step - loss: 411.3628 - val_loss: 988.7618\n",
      "Epoch 113/2000\n",
      "2800/2800 [==============================] - 1s 231us/step - loss: 383.7206 - val_loss: 589.3917\n",
      "Epoch 114/2000\n",
      "2800/2800 [==============================] - 1s 208us/step - loss: 397.4873 - val_loss: 622.0069\n",
      "Epoch 115/2000\n",
      "2800/2800 [==============================] - 0s 171us/step - loss: 412.1563 - val_loss: 503.2552\n",
      "Epoch 116/2000\n",
      "2800/2800 [==============================] - 0s 160us/step - loss: 392.5775 - val_loss: 517.9511\n",
      "Epoch 117/2000\n",
      "2800/2800 [==============================] - 1s 181us/step - loss: 402.7529 - val_loss: 494.7380\n",
      "Epoch 118/2000\n",
      "2800/2800 [==============================] - 1s 180us/step - loss: 408.4101 - val_loss: 664.5861\n",
      "Epoch 119/2000\n",
      "2800/2800 [==============================] - 0s 176us/step - loss: 395.0690 - val_loss: 499.6426\n",
      "Epoch 120/2000\n",
      "2800/2800 [==============================] - 0s 177us/step - loss: 395.3883 - val_loss: 772.8872\n",
      "Epoch 121/2000\n",
      "2800/2800 [==============================] - 1s 184us/step - loss: 464.5391 - val_loss: 691.5344\n",
      "Epoch 122/2000\n",
      "2800/2800 [==============================] - 1s 215us/step - loss: 409.8266 - val_loss: 961.4805\n",
      "Epoch 123/2000\n",
      "2800/2800 [==============================] - 0s 172us/step - loss: 380.8255 - val_loss: 590.4280\n",
      "Epoch 124/2000\n",
      "2800/2800 [==============================] - 0s 173us/step - loss: 393.1026 - val_loss: 829.9151\n",
      "Epoch 125/2000\n",
      "2800/2800 [==============================] - 0s 171us/step - loss: 386.6978 - val_loss: 520.8251\n",
      "Epoch 126/2000\n",
      "2800/2800 [==============================] - 0s 171us/step - loss: 430.9335 - val_loss: 495.0572\n",
      "Epoch 127/2000\n",
      "2800/2800 [==============================] - 0s 171us/step - loss: 418.0850 - val_loss: 946.5319\n",
      "Epoch 128/2000\n",
      "2800/2800 [==============================] - 1s 205us/step - loss: 380.2606 - val_loss: 661.0410\n",
      "Epoch 129/2000\n",
      "2800/2800 [==============================] - 0s 178us/step - loss: 383.6060 - val_loss: 679.4808\n",
      "Epoch 130/2000\n",
      "2800/2800 [==============================] - 0s 173us/step - loss: 377.7818 - val_loss: 955.5341\n",
      "Epoch 131/2000\n",
      "2800/2800 [==============================] - 1s 181us/step - loss: 389.3084 - val_loss: 667.6823\n",
      "Epoch 132/2000\n",
      "2800/2800 [==============================] - 0s 168us/step - loss: 408.1639 - val_loss: 514.8197\n",
      "Epoch 133/2000\n",
      "2800/2800 [==============================] - 0s 161us/step - loss: 384.1367 - val_loss: 632.6752\n",
      "Epoch 134/2000\n",
      "2800/2800 [==============================] - 1s 194us/step - loss: 381.8595 - val_loss: 532.6315\n",
      "Epoch 135/2000\n",
      "2800/2800 [==============================] - 0s 165us/step - loss: 395.9697 - val_loss: 610.7851\n",
      "Epoch 136/2000\n",
      "2800/2800 [==============================] - 1s 184us/step - loss: 386.1989 - val_loss: 877.2648\n",
      "Epoch 137/2000\n",
      "2800/2800 [==============================] - 1s 184us/step - loss: 371.4314 - val_loss: 530.6282\n",
      "Epoch 138/2000\n",
      "2800/2800 [==============================] - 1s 213us/step - loss: 421.0344 - val_loss: 753.8250\n",
      "Epoch 139/2000\n",
      "2800/2800 [==============================] - 1s 193us/step - loss: 402.8573 - val_loss: 526.4693\n",
      "Epoch 140/2000\n",
      "2800/2800 [==============================] - 1s 192us/step - loss: 383.0467 - val_loss: 836.5579\n",
      "Epoch 141/2000\n",
      "2800/2800 [==============================] - 1s 209us/step - loss: 385.8799 - val_loss: 839.5424\n",
      "Epoch 142/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 400.210 - 1s 234us/step - loss: 400.1204 - val_loss: 757.5685\n",
      "Epoch 143/2000\n",
      "2800/2800 [==============================] - 1s 216us/step - loss: 369.7775 - val_loss: 637.6075\n",
      "Epoch 144/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 397.5440 - val_loss: 1214.0484\n",
      "Epoch 145/2000\n",
      "2800/2800 [==============================] - 1s 217us/step - loss: 381.6082 - val_loss: 577.1547\n",
      "Epoch 146/2000\n",
      "2800/2800 [==============================] - 1s 188us/step - loss: 371.6816 - val_loss: 624.1971\n",
      "Epoch 147/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 1s 202us/step - loss: 388.0835 - val_loss: 710.2996\n",
      "Epoch 148/2000\n",
      "2800/2800 [==============================] - 1s 193us/step - loss: 374.3301 - val_loss: 547.8257\n",
      "Epoch 149/2000\n",
      "2800/2800 [==============================] - 1s 186us/step - loss: 381.6228 - val_loss: 891.2869\n",
      "Epoch 150/2000\n",
      "2800/2800 [==============================] - 0s 164us/step - loss: 358.6916 - val_loss: 565.7141\n",
      "Epoch 151/2000\n",
      "2800/2800 [==============================] - 1s 184us/step - loss: 376.5170 - val_loss: 602.4368\n",
      "Epoch 152/2000\n",
      "2800/2800 [==============================] - 1s 179us/step - loss: 417.2980 - val_loss: 643.4717\n",
      "Epoch 153/2000\n",
      "2800/2800 [==============================] - 1s 179us/step - loss: 374.0066 - val_loss: 862.8211\n",
      "Epoch 154/2000\n",
      "2800/2800 [==============================] - 1s 189us/step - loss: 405.2671 - val_loss: 1167.4955\n",
      "Epoch 155/2000\n",
      "2800/2800 [==============================] - 0s 170us/step - loss: 352.6903 - val_loss: 712.5489\n",
      "Epoch 156/2000\n",
      "2800/2800 [==============================] - 1s 188us/step - loss: 392.8796 - val_loss: 943.7609\n",
      "Epoch 157/2000\n",
      "2800/2800 [==============================] - 1s 179us/step - loss: 351.6370 - val_loss: 665.6950\n",
      "Epoch 158/2000\n",
      "2800/2800 [==============================] - 1s 215us/step - loss: 372.7021 - val_loss: 513.4925\n",
      "Epoch 159/2000\n",
      "2800/2800 [==============================] - 1s 194us/step - loss: 347.9927 - val_loss: 526.2989\n",
      "Epoch 160/2000\n",
      "2800/2800 [==============================] - 0s 174us/step - loss: 398.9214 - val_loss: 648.1467\n",
      "Epoch 161/2000\n",
      "2800/2800 [==============================] - 0s 177us/step - loss: 363.5634 - val_loss: 1017.3752\n",
      "Epoch 162/2000\n",
      "2800/2800 [==============================] - 1s 209us/step - loss: 371.4274 - val_loss: 556.0856\n",
      "Epoch 163/2000\n",
      "2800/2800 [==============================] - 1s 204us/step - loss: 364.7351 - val_loss: 551.7841\n",
      "Epoch 164/2000\n",
      "2800/2800 [==============================] - 1s 189us/step - loss: 400.5544 - val_loss: 869.5016\n",
      "Epoch 165/2000\n",
      "2800/2800 [==============================] - 1s 185us/step - loss: 381.5014 - val_loss: 685.5140\n",
      "Epoch 166/2000\n",
      "2800/2800 [==============================] - 0s 178us/step - loss: 444.3973 - val_loss: 904.0306\n",
      "Epoch 167/2000\n",
      "2800/2800 [==============================] - 1s 196us/step - loss: 406.6981 - val_loss: 594.2471\n",
      "Epoch 168/2000\n",
      "2800/2800 [==============================] - 1s 202us/step - loss: 371.8392 - val_loss: 867.5295\n",
      "Epoch 169/2000\n",
      "2800/2800 [==============================] - 1s 193us/step - loss: 388.2226 - val_loss: 1248.5963\n",
      "Epoch 170/2000\n",
      "2800/2800 [==============================] - 1s 191us/step - loss: 465.2058 - val_loss: 739.9240\n",
      "Epoch 171/2000\n",
      "2800/2800 [==============================] - 1s 181us/step - loss: 359.5427 - val_loss: 542.0300\n",
      "Epoch 172/2000\n",
      "2800/2800 [==============================] - 1s 179us/step - loss: 385.7641 - val_loss: 551.7376\n",
      "Epoch 173/2000\n",
      "2800/2800 [==============================] - 1s 190us/step - loss: 387.6262 - val_loss: 536.2421\n",
      "Epoch 174/2000\n",
      "2800/2800 [==============================] - 1s 196us/step - loss: 379.5981 - val_loss: 767.3825\n",
      "Epoch 175/2000\n",
      "2800/2800 [==============================] - 1s 200us/step - loss: 387.2610 - val_loss: 1007.8372\n",
      "Epoch 176/2000\n",
      "2800/2800 [==============================] - 1s 190us/step - loss: 340.9872 - val_loss: 1121.7864\n",
      "Epoch 177/2000\n",
      "2800/2800 [==============================] - 1s 193us/step - loss: 337.3162 - val_loss: 600.4411\n",
      "Epoch 178/2000\n",
      "2800/2800 [==============================] - 1s 206us/step - loss: 359.0045 - val_loss: 768.4688\n",
      "Epoch 179/2000\n",
      "2800/2800 [==============================] - 1s 194us/step - loss: 420.2762 - val_loss: 790.3735\n",
      "Epoch 180/2000\n",
      "2800/2800 [==============================] - 1s 194us/step - loss: 345.7044 - val_loss: 824.9461\n",
      "Epoch 181/2000\n",
      "2800/2800 [==============================] - 1s 233us/step - loss: 363.7666 - val_loss: 570.9853\n",
      "Epoch 182/2000\n",
      "2800/2800 [==============================] - 1s 228us/step - loss: 345.0285 - val_loss: 574.9422\n",
      "Epoch 183/2000\n",
      "2800/2800 [==============================] - 1s 228us/step - loss: 357.9732 - val_loss: 769.2299\n",
      "Epoch 184/2000\n",
      "2800/2800 [==============================] - 1s 244us/step - loss: 320.5497 - val_loss: 607.7240\n",
      "Epoch 185/2000\n",
      "2800/2800 [==============================] - 1s 214us/step - loss: 411.9936 - val_loss: 562.4006\n",
      "Epoch 186/2000\n",
      "2800/2800 [==============================] - 1s 222us/step - loss: 347.8383 - val_loss: 631.4399\n",
      "Epoch 187/2000\n",
      "2800/2800 [==============================] - 1s 241us/step - loss: 314.1746 - val_loss: 585.9504\n",
      "Epoch 188/2000\n",
      "2800/2800 [==============================] - 1s 249us/step - loss: 369.1223 - val_loss: 806.2131\n",
      "Epoch 189/2000\n",
      "2800/2800 [==============================] - 1s 269us/step - loss: 333.7536 - val_loss: 588.6220\n",
      "Epoch 190/2000\n",
      "2800/2800 [==============================] - 1s 305us/step - loss: 357.8823 - val_loss: 652.9844\n",
      "Epoch 191/2000\n",
      "2800/2800 [==============================] - 1s 309us/step - loss: 326.7561 - val_loss: 545.0327\n",
      "Epoch 192/2000\n",
      "2800/2800 [==============================] - 1s 209us/step - loss: 315.0236 - val_loss: 543.2862\n",
      "Epoch 193/2000\n",
      "2800/2800 [==============================] - 1s 191us/step - loss: 357.0982 - val_loss: 580.1623\n",
      "Epoch 194/2000\n",
      "2800/2800 [==============================] - 1s 198us/step - loss: 381.4871 - val_loss: 901.5117\n",
      "Epoch 195/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 365.0918 - val_loss: 949.0191\n",
      "Epoch 196/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 297.4356 - val_loss: 1048.3826\n",
      "Epoch 197/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 370.0983 - val_loss: 585.0911\n",
      "Epoch 198/2000\n",
      "2800/2800 [==============================] - 0s 105us/step - loss: 323.8039 - val_loss: 604.7382\n",
      "Epoch 199/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 333.5304 - val_loss: 724.6952\n",
      "Epoch 200/2000\n",
      "2800/2800 [==============================] - 0s 116us/step - loss: 321.7518 - val_loss: 698.2326\n",
      "Epoch 201/2000\n",
      "2800/2800 [==============================] - 0s 113us/step - loss: 300.5850 - val_loss: 548.5751\n",
      "Epoch 202/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 318.8750 - val_loss: 555.5582\n",
      "Epoch 203/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 295.8055 - val_loss: 896.6403\n",
      "Epoch 204/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 306.4519 - val_loss: 551.6279\n",
      "Epoch 205/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 289.9474 - val_loss: 665.3580\n",
      "Epoch 206/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 310.4228 - val_loss: 700.5321\n",
      "Epoch 207/2000\n",
      "2800/2800 [==============================] - 0s 108us/step - loss: 303.6221 - val_loss: 673.0763\n",
      "Epoch 208/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 280.6848 - val_loss: 724.3827\n",
      "Epoch 209/2000\n",
      "2800/2800 [==============================] - 0s 147us/step - loss: 307.9468 - val_loss: 663.4226\n",
      "Epoch 210/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 302.7836 - val_loss: 732.8267\n",
      "Epoch 211/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 302.6314 - val_loss: 605.1434\n",
      "Epoch 212/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 274.3285 - val_loss: 798.8178\n",
      "Epoch 213/2000\n",
      "2800/2800 [==============================] - 0s 110us/step - loss: 280.3276 - val_loss: 730.3475\n",
      "Epoch 214/2000\n",
      "2800/2800 [==============================] - 0s 111us/step - loss: 279.9641 - val_loss: 744.3820\n",
      "Epoch 215/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 286.0437 - val_loss: 614.5396\n",
      "Epoch 216/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 287.5526 - val_loss: 643.4974\n",
      "Epoch 217/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 274.9489 - val_loss: 741.9103\n",
      "Epoch 218/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 287.5165 - val_loss: 719.1134\n",
      "Epoch 219/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 278.7531 - val_loss: 612.4764\n",
      "Epoch 220/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 125us/step - loss: 260.5922 - val_loss: 632.1714\n",
      "Epoch 221/2000\n",
      "2800/2800 [==============================] - 0s 116us/step - loss: 281.0736 - val_loss: 591.0877\n",
      "Epoch 222/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 261.4673 - val_loss: 774.4489\n",
      "Epoch 223/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 255.5068 - val_loss: 596.6387\n",
      "Epoch 224/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 274.7689 - val_loss: 920.7983\n",
      "Epoch 225/2000\n",
      "2800/2800 [==============================] - 0s 120us/step - loss: 268.9324 - val_loss: 638.1079\n",
      "Epoch 226/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 292.7158 - val_loss: 825.9418\n",
      "Epoch 227/2000\n",
      "2800/2800 [==============================] - 0s 103us/step - loss: 253.7381 - val_loss: 738.6997\n",
      "Epoch 228/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 270.2047 - val_loss: 728.8403\n",
      "Epoch 229/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 282.3445 - val_loss: 703.3110\n",
      "Epoch 230/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 269.5419 - val_loss: 667.0008\n",
      "Epoch 231/2000\n",
      "2800/2800 [==============================] - 0s 120us/step - loss: 260.2090 - val_loss: 737.0079\n",
      "Epoch 232/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 267.0848 - val_loss: 624.5747\n",
      "Epoch 233/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 255.6901 - val_loss: 982.7978\n",
      "Epoch 234/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 255.7246 - val_loss: 704.6735\n",
      "Epoch 235/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 254.4664 - val_loss: 874.0477\n",
      "Epoch 236/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 278.0663 - val_loss: 593.0061\n",
      "Epoch 237/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 272.6640 - val_loss: 751.0120\n",
      "Epoch 238/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 247.4852 - val_loss: 1018.9038\n",
      "Epoch 239/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 273.0274 - val_loss: 607.4536\n",
      "Epoch 240/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 245.8260 - val_loss: 654.7906\n",
      "Epoch 241/2000\n",
      "2800/2800 [==============================] - 0s 158us/step - loss: 256.8858 - val_loss: 980.4340\n",
      "Epoch 242/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 253.0868 - val_loss: 746.6613\n",
      "Epoch 243/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 242.8648 - val_loss: 762.7362\n",
      "Epoch 244/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 263.013 - 0s 125us/step - loss: 260.2419 - val_loss: 866.6043\n",
      "Epoch 245/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 258.8721 - val_loss: 635.7217\n",
      "Epoch 246/2000\n",
      "2800/2800 [==============================] - 0s 109us/step - loss: 265.8071 - val_loss: 1231.2954\n",
      "Epoch 247/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 262.0899 - val_loss: 1067.1739\n",
      "Epoch 248/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 258.4968 - val_loss: 1274.8533\n",
      "Epoch 249/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 248.6121 - val_loss: 734.9931\n",
      "Epoch 250/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 244.8549 - val_loss: 683.7219\n",
      "Epoch 251/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 245.2928 - val_loss: 972.3571\n",
      "Epoch 252/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 254.8307 - val_loss: 839.4897\n",
      "Epoch 253/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 233.4581 - val_loss: 617.3273\n",
      "Epoch 254/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 242.8013 - val_loss: 830.7372\n",
      "Epoch 255/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 244.0235 - val_loss: 714.9156\n",
      "Epoch 256/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 237.7379 - val_loss: 606.2926\n",
      "Epoch 257/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 232.0280 - val_loss: 714.1976\n",
      "Epoch 258/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 261.0178 - val_loss: 1860.8991\n",
      "Epoch 259/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 268.0491 - val_loss: 887.8406\n",
      "Epoch 260/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 248.3251 - val_loss: 653.4325\n",
      "Epoch 261/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 258.4505 - val_loss: 1667.5612\n",
      "Epoch 262/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 244.3021 - val_loss: 746.8169\n",
      "Epoch 263/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 250.1912 - val_loss: 661.6674\n",
      "Epoch 264/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 262.5081 - val_loss: 1043.1880\n",
      "Epoch 265/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 233.8667 - val_loss: 647.2032\n",
      "Epoch 266/2000\n",
      "2800/2800 [==============================] - 0s 116us/step - loss: 259.3415 - val_loss: 625.2115\n",
      "Epoch 267/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 228.3475 - val_loss: 1070.3039\n",
      "Epoch 268/2000\n",
      "2800/2800 [==============================] - 0s 116us/step - loss: 256.8731 - val_loss: 671.3320\n",
      "Epoch 269/2000\n",
      "2800/2800 [==============================] - 0s 115us/step - loss: 242.0046 - val_loss: 680.4209\n",
      "Epoch 270/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 227.4886 - val_loss: 760.2621\n",
      "Epoch 271/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 236.1207 - val_loss: 816.8994\n",
      "Epoch 272/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 227.5974 - val_loss: 676.1651\n",
      "Epoch 273/2000\n",
      "2800/2800 [==============================] - 0s 110us/step - loss: 251.0844 - val_loss: 717.5290\n",
      "Epoch 274/2000\n",
      "2800/2800 [==============================] - 0s 109us/step - loss: 235.0362 - val_loss: 911.8370\n",
      "Epoch 275/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 227.9417 - val_loss: 664.2064\n",
      "Epoch 276/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 238.3777 - val_loss: 734.9673\n",
      "Epoch 277/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 225.6603 - val_loss: 670.7884\n",
      "Epoch 278/2000\n",
      "2800/2800 [==============================] - 0s 110us/step - loss: 232.3255 - val_loss: 684.7525\n",
      "Epoch 279/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 230.4892 - val_loss: 645.3761\n",
      "Epoch 280/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 206.3682 - val_loss: 1023.0788\n",
      "Epoch 281/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 238.8721 - val_loss: 1002.3465\n",
      "Epoch 282/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 215.1714 - val_loss: 730.7929\n",
      "Epoch 283/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 223.5798 - val_loss: 707.6681\n",
      "Epoch 284/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 214.5937 - val_loss: 803.9624\n",
      "Epoch 285/2000\n",
      "2800/2800 [==============================] - 0s 105us/step - loss: 226.6641 - val_loss: 866.8935\n",
      "Epoch 286/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 219.3899 - val_loss: 685.0535\n",
      "Epoch 287/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 216.8150 - val_loss: 659.3986\n",
      "Epoch 288/2000\n",
      "2800/2800 [==============================] - 0s 154us/step - loss: 216.6423 - val_loss: 633.8589\n",
      "Epoch 289/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 235.3989 - val_loss: 1262.3767\n",
      "Epoch 290/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 223.9253 - val_loss: 759.9922\n",
      "Epoch 291/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 215.1131 - val_loss: 918.3246\n",
      "Epoch 292/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 235.4130 - val_loss: 870.3241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 250.9509 - val_loss: 813.7534\n",
      "Epoch 294/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 224.2475 - val_loss: 1317.5131\n",
      "Epoch 295/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 227.0957 - val_loss: 806.2564\n",
      "Epoch 296/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 205.8547 - val_loss: 720.4897\n",
      "Epoch 297/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 225.7647 - val_loss: 1338.2454\n",
      "Epoch 298/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 239.9874 - val_loss: 806.3093\n",
      "Epoch 299/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 236.2305 - val_loss: 824.2640\n",
      "Epoch 300/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 210.7001 - val_loss: 891.0465\n",
      "Epoch 301/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 204.7330 - val_loss: 685.7776\n",
      "Epoch 302/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 224.2419 - val_loss: 981.0313\n",
      "Epoch 303/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 193.0210 - val_loss: 1486.3057\n",
      "Epoch 304/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 213.6338 - val_loss: 1161.9661\n",
      "Epoch 305/2000\n",
      "2800/2800 [==============================] - 0s 147us/step - loss: 257.2397 - val_loss: 834.1055\n",
      "Epoch 306/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 211.6213 - val_loss: 661.8115\n",
      "Epoch 307/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 209.9686 - val_loss: 774.9319\n",
      "Epoch 308/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 210.1046 - val_loss: 1083.7638\n",
      "Epoch 309/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 210.4437 - val_loss: 757.2657\n",
      "Epoch 310/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 220.2242 - val_loss: 720.0836\n",
      "Epoch 311/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 202.8972 - val_loss: 964.1659\n",
      "Epoch 312/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 216.0584 - val_loss: 836.0669\n",
      "Epoch 313/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 212.4110 - val_loss: 896.2115\n",
      "Epoch 314/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 191.7935 - val_loss: 769.1821\n",
      "Epoch 315/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 211.3693 - val_loss: 665.1073\n",
      "Epoch 316/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 203.0909 - val_loss: 939.3260\n",
      "Epoch 317/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 220.1409 - val_loss: 928.0525\n",
      "Epoch 318/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 197.7668 - val_loss: 739.9810\n",
      "Epoch 319/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 185.3961 - val_loss: 1121.0989\n",
      "Epoch 320/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 209.2861 - val_loss: 663.7955\n",
      "Epoch 321/2000\n",
      "2800/2800 [==============================] - 0s 147us/step - loss: 240.1400 - val_loss: 840.2677\n",
      "Epoch 322/2000\n",
      "2800/2800 [==============================] - 0s 154us/step - loss: 190.4248 - val_loss: 820.3967\n",
      "Epoch 323/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 183.3230 - val_loss: 823.9478\n",
      "Epoch 324/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 193.2192 - val_loss: 868.8997\n",
      "Epoch 325/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 192.9393 - val_loss: 746.7023\n",
      "Epoch 326/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 204.8753 - val_loss: 823.2307\n",
      "Epoch 327/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 195.4256 - val_loss: 776.9487\n",
      "Epoch 328/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 184.5581 - val_loss: 774.9275\n",
      "Epoch 329/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 215.1715 - val_loss: 699.3435\n",
      "Epoch 330/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 212.7833 - val_loss: 693.4924\n",
      "Epoch 331/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 180.3361 - val_loss: 677.9622\n",
      "Epoch 332/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 194.8981 - val_loss: 836.7131\n",
      "Epoch 333/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 210.1719 - val_loss: 810.7743\n",
      "Epoch 334/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 166.9017 - val_loss: 839.1062\n",
      "Epoch 335/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 178.6561 - val_loss: 947.0683\n",
      "Epoch 336/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 178.3908 - val_loss: 747.8368\n",
      "Epoch 337/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 198.8751 - val_loss: 918.9754\n",
      "Epoch 338/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 170.2143 - val_loss: 831.9365\n",
      "Epoch 339/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 178.2824 - val_loss: 860.8396\n",
      "Epoch 340/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 188.7015 - val_loss: 695.6747\n",
      "Epoch 341/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 199.7347 - val_loss: 743.4220\n",
      "Epoch 342/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 226.9849 - val_loss: 695.1180\n",
      "Epoch 343/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 186.0986 - val_loss: 749.4321\n",
      "Epoch 344/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 215.5077 - val_loss: 1397.1989\n",
      "Epoch 345/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 198.7417 - val_loss: 739.8464\n",
      "Epoch 346/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 175.0876 - val_loss: 782.1183\n",
      "Epoch 347/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 193.9824 - val_loss: 798.6811\n",
      "Epoch 348/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 183.3208 - val_loss: 1048.6372\n",
      "Epoch 349/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 213.6526 - val_loss: 1117.0673\n",
      "Epoch 350/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 191.1469 - val_loss: 1700.6025\n",
      "Epoch 351/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 188.3769 - val_loss: 840.7128\n",
      "Epoch 352/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 168.2248 - val_loss: 824.5176\n",
      "Epoch 353/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 187.6106 - val_loss: 1456.7319\n",
      "Epoch 354/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 198.7137 - val_loss: 1119.2332\n",
      "Epoch 355/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 186.7031 - val_loss: 743.8103\n",
      "Epoch 356/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 191.446 - 0s 126us/step - loss: 192.4731 - val_loss: 812.8563\n",
      "Epoch 357/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 174.1163 - val_loss: 807.3135\n",
      "Epoch 358/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 164.7386 - val_loss: 1007.8415\n",
      "Epoch 359/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 170.6330 - val_loss: 1028.5855\n",
      "Epoch 360/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 173.0742 - val_loss: 875.7079\n",
      "Epoch 361/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 167.0892 - val_loss: 712.2022\n",
      "Epoch 362/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 172.5610 - val_loss: 1074.3623\n",
      "Epoch 363/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 175.6716 - val_loss: 720.3149\n",
      "Epoch 364/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 169.3561 - val_loss: 928.5535\n",
      "Epoch 365/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 197.5896 - val_loss: 756.4529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 366/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 183.8034 - val_loss: 802.3206\n",
      "Epoch 367/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 159.0226 - val_loss: 777.1452\n",
      "Epoch 368/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 182.1764 - val_loss: 1015.0614\n",
      "Epoch 369/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 157.3004 - val_loss: 1115.3614\n",
      "Epoch 370/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 179.1933 - val_loss: 845.3693\n",
      "Epoch 371/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 185.8063 - val_loss: 723.4810\n",
      "Epoch 372/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 181.7706 - val_loss: 717.8966\n",
      "Epoch 373/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 168.4098 - val_loss: 800.3301\n",
      "Epoch 374/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 171.2458 - val_loss: 856.5517\n",
      "Epoch 375/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 183.3013 - val_loss: 2300.3264\n",
      "Epoch 376/2000\n",
      "2800/2800 [==============================] - 0s 154us/step - loss: 192.5495 - val_loss: 758.8992\n",
      "Epoch 377/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 153.1499 - val_loss: 951.8548\n",
      "Epoch 378/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 147.8361 - val_loss: 925.4283\n",
      "Epoch 379/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 167.6426 - val_loss: 745.6030\n",
      "Epoch 380/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 196.2087 - val_loss: 810.2015\n",
      "Epoch 381/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 163.0335 - val_loss: 880.2433\n",
      "Epoch 382/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 167.0191 - val_loss: 940.6364\n",
      "Epoch 383/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 175.4370 - val_loss: 817.3446\n",
      "Epoch 384/2000\n",
      "2800/2800 [==============================] - 0s 115us/step - loss: 174.5914 - val_loss: 1019.5075\n",
      "Epoch 385/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 196.4671 - val_loss: 866.9840\n",
      "Epoch 386/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 157.9830 - val_loss: 995.3708\n",
      "Epoch 387/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 162.0945 - val_loss: 844.9540\n",
      "Epoch 388/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 160.4511 - val_loss: 754.8037\n",
      "Epoch 389/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 163.4963 - val_loss: 978.0140\n",
      "Epoch 390/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 145.0304 - val_loss: 874.6077\n",
      "Epoch 391/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 146.2746 - val_loss: 1080.0642\n",
      "Epoch 392/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 151.0813 - val_loss: 923.4004\n",
      "Epoch 393/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 159.6680 - val_loss: 1014.4486\n",
      "Epoch 394/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 153.3606 - val_loss: 758.3913\n",
      "Epoch 395/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 165.4755 - val_loss: 958.5345\n",
      "Epoch 396/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 159.6653 - val_loss: 898.8729\n",
      "Epoch 397/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 156.1779 - val_loss: 1103.5748\n",
      "Epoch 398/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 182.4942 - val_loss: 818.3461\n",
      "Epoch 399/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 146.0498 - val_loss: 784.0281\n",
      "Epoch 400/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 174.0573 - val_loss: 866.0715\n",
      "Epoch 401/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 150.6345 - val_loss: 1186.3033\n",
      "Epoch 402/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 145.3657 - val_loss: 1026.5847\n",
      "Epoch 403/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 168.2262 - val_loss: 763.9405\n",
      "Epoch 404/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 148.5750 - val_loss: 782.4302\n",
      "Epoch 405/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 163.9066 - val_loss: 795.9009\n",
      "Epoch 406/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 152.0471 - val_loss: 720.2123\n",
      "Epoch 407/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 161.1569 - val_loss: 777.9291\n",
      "Epoch 408/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 143.7411 - val_loss: 1142.4032\n",
      "Epoch 409/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 147.8979 - val_loss: 822.7105\n",
      "Epoch 410/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 134.2688 - val_loss: 1096.3240\n",
      "Epoch 411/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 141.2741 - val_loss: 853.5688\n",
      "Epoch 412/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 158.9210 - val_loss: 1001.6346\n",
      "Epoch 413/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 151.6170 - val_loss: 1001.7387\n",
      "Epoch 414/2000\n",
      "2800/2800 [==============================] - 0s 156us/step - loss: 156.3517 - val_loss: 818.6503\n",
      "Epoch 415/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 134.3440 - val_loss: 860.8245\n",
      "Epoch 416/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 148.8040 - val_loss: 1034.8297\n",
      "Epoch 417/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 157.8160 - val_loss: 861.2523\n",
      "Epoch 418/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 141.0724 - val_loss: 901.8339\n",
      "Epoch 419/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 127.4972 - val_loss: 903.0014\n",
      "Epoch 420/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 166.1022 - val_loss: 1003.3613\n",
      "Epoch 421/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 165.3342 - val_loss: 845.5337\n",
      "Epoch 422/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 136.7918 - val_loss: 845.7159\n",
      "Epoch 423/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 165.3108 - val_loss: 1053.9689\n",
      "Epoch 424/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 142.1949 - val_loss: 1094.2396\n",
      "Epoch 425/2000\n",
      "2800/2800 [==============================] - 0s 165us/step - loss: 129.8229 - val_loss: 997.0111\n",
      "Epoch 426/2000\n",
      "2800/2800 [==============================] - 0s 161us/step - loss: 146.2279 - val_loss: 758.0306\n",
      "Epoch 427/2000\n",
      "2800/2800 [==============================] - 0s 156us/step - loss: 134.7916 - val_loss: 943.5816\n",
      "Epoch 428/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 183.1746 - val_loss: 794.6700\n",
      "Epoch 429/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 150.1255 - val_loss: 771.5360\n",
      "Epoch 430/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 135.8797 - val_loss: 792.9062\n",
      "Epoch 431/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 126.9505 - val_loss: 965.4944\n",
      "Epoch 432/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 137.3338 - val_loss: 1366.2724\n",
      "Epoch 433/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 133.6147 - val_loss: 1070.1881\n",
      "Epoch 434/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 138.6930 - val_loss: 993.5355\n",
      "Epoch 435/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 129.8772 - val_loss: 759.7778\n",
      "Epoch 436/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 134.1154 - val_loss: 867.3095\n",
      "Epoch 437/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 137.7677 - val_loss: 812.3484\n",
      "Epoch 438/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 136.0429 - val_loss: 820.5890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 439/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 141.2006 - val_loss: 787.0850\n",
      "Epoch 440/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 156.6351 - val_loss: 1084.9349\n",
      "Epoch 441/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 135.7775 - val_loss: 1190.6091\n",
      "Epoch 442/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 218.4245 - val_loss: 838.3960\n",
      "Epoch 443/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 150.6298 - val_loss: 798.1629\n",
      "Epoch 444/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 131.3427 - val_loss: 798.1567\n",
      "Epoch 445/2000\n",
      "2800/2800 [==============================] - 0s 172us/step - loss: 148.7644 - val_loss: 827.1206\n",
      "Epoch 446/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 130.9059 - val_loss: 873.3232\n",
      "Epoch 447/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 139.2977 - val_loss: 1060.8377\n",
      "Epoch 448/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 125.2479 - val_loss: 919.1566\n",
      "Epoch 449/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 132.1882 - val_loss: 930.0809\n",
      "Epoch 450/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 134.8613 - val_loss: 873.9994\n",
      "Epoch 451/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 154.7967 - val_loss: 986.9711\n",
      "Epoch 452/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 144.7688 - val_loss: 1058.9388\n",
      "Epoch 453/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 124.8075 - val_loss: 864.3289\n",
      "Epoch 454/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 120.6860 - val_loss: 1055.6759\n",
      "Epoch 455/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 157.7810 - val_loss: 922.2234\n",
      "Epoch 456/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 118.9606 - val_loss: 1318.5102\n",
      "Epoch 457/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 139.4215 - val_loss: 1513.8987\n",
      "Epoch 458/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 157.0688 - val_loss: 841.4605\n",
      "Epoch 459/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 118.5429 - val_loss: 1034.5448\n",
      "Epoch 460/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 125.2522 - val_loss: 827.7675\n",
      "Epoch 461/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 117.1452 - val_loss: 854.1318\n",
      "Epoch 462/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 132.2938 - val_loss: 948.2632\n",
      "Epoch 463/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 136.0451 - val_loss: 795.4460\n",
      "Epoch 464/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 141.4266 - val_loss: 1125.6953\n",
      "Epoch 465/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 130.3187 - val_loss: 876.6846\n",
      "Epoch 466/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 148.2156 - val_loss: 1083.4267\n",
      "Epoch 467/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 121.3959 - val_loss: 840.2686\n",
      "Epoch 468/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 136.7828 - val_loss: 1301.1854\n",
      "Epoch 469/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 122.8246 - val_loss: 876.7010\n",
      "Epoch 470/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 119.8428 - val_loss: 1145.7024\n",
      "Epoch 471/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 134.4760 - val_loss: 932.3579\n",
      "Epoch 472/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 111.8021 - val_loss: 872.9934\n",
      "Epoch 473/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 124.6240 - val_loss: 835.8606\n",
      "Epoch 474/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 125.2479 - val_loss: 1009.4274\n",
      "Epoch 475/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 117.1252 - val_loss: 1425.2633\n",
      "Epoch 476/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 139.9886 - val_loss: 1635.6311\n",
      "Epoch 477/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 168.8465 - val_loss: 947.1028\n",
      "Epoch 478/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 127.0267 - val_loss: 995.2719\n",
      "Epoch 479/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 197.3666 - val_loss: 1150.4097\n",
      "Epoch 480/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 143.0131 - val_loss: 834.0568\n",
      "Epoch 481/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 112.9433 - val_loss: 973.8284\n",
      "Epoch 482/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 128.6360 - val_loss: 888.1073\n",
      "Epoch 483/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 111.6928 - val_loss: 906.2351\n",
      "Epoch 484/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 103.8477 - val_loss: 868.2190\n",
      "Epoch 485/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 118.9653 - val_loss: 779.7143\n",
      "Epoch 486/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 138.0028 - val_loss: 1119.2139\n",
      "Epoch 487/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 143.8920 - val_loss: 958.7951\n",
      "Epoch 488/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 113.1725 - val_loss: 1085.3922\n",
      "Epoch 489/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 117.2340 - val_loss: 808.5161\n",
      "Epoch 490/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 129.7532 - val_loss: 1138.6602\n",
      "Epoch 491/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 125.1064 - val_loss: 976.6453\n",
      "Epoch 492/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 101.4066 - val_loss: 1039.0902\n",
      "Epoch 493/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 123.276 - 0s 142us/step - loss: 120.9060 - val_loss: 814.5264\n",
      "Epoch 494/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 133.9011 - val_loss: 1084.8330\n",
      "Epoch 495/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 106.7875 - val_loss: 1157.3526\n",
      "Epoch 496/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 110.2890 - val_loss: 1008.4907\n",
      "Epoch 497/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 110.5157 - val_loss: 1224.2233\n",
      "Epoch 498/2000\n",
      "2800/2800 [==============================] - 0s 156us/step - loss: 124.3540 - val_loss: 836.1392\n",
      "Epoch 499/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 116.5557 - val_loss: 1012.3381\n",
      "Epoch 500/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 109.6167 - val_loss: 1103.6442\n",
      "Epoch 501/2000\n",
      "2800/2800 [==============================] - 0s 154us/step - loss: 126.9233 - val_loss: 911.7863\n",
      "Epoch 502/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 144.0232 - val_loss: 1152.1811\n",
      "Epoch 503/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 127.0636 - val_loss: 960.4313\n",
      "Epoch 504/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 105.1278 - val_loss: 863.1757\n",
      "Epoch 505/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 100.7842 - val_loss: 929.6389\n",
      "Epoch 506/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 112.6219 - val_loss: 1072.6335\n",
      "Epoch 507/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 106.9617 - val_loss: 840.6963\n",
      "Epoch 508/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 113.7032 - val_loss: 850.4424\n",
      "Epoch 509/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 121.3388 - val_loss: 1005.7422\n",
      "Epoch 510/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 114.6677 - val_loss: 803.3834\n",
      "Epoch 511/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 143us/step - loss: 115.1490 - val_loss: 934.8089\n",
      "Epoch 512/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 104.8469 - val_loss: 1067.1534\n",
      "Epoch 513/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 123.7468 - val_loss: 1909.0395\n",
      "Epoch 514/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 131.2617 - val_loss: 891.0289\n",
      "Epoch 515/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 133.6806 - val_loss: 1096.5613\n",
      "Epoch 516/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 160.4607 - val_loss: 1165.4943\n",
      "Epoch 517/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 109.0275 - val_loss: 890.8356\n",
      "Epoch 518/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 103.3202 - val_loss: 1060.5063\n",
      "Epoch 519/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 121.6769 - val_loss: 976.4086\n",
      "Epoch 520/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 130.1983 - val_loss: 876.3927\n",
      "Epoch 521/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 114.1225 - val_loss: 860.4152\n",
      "Epoch 522/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 98.8415 - val_loss: 1444.1646\n",
      "Epoch 523/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 120.3681 - val_loss: 1180.2558\n",
      "Epoch 524/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 147.0559 - val_loss: 1107.9736\n",
      "Epoch 525/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 102.7625 - val_loss: 866.6301\n",
      "Epoch 526/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 107.6610 - val_loss: 919.1977\n",
      "Epoch 527/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 107.6721 - val_loss: 916.1626\n",
      "Epoch 528/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 98.8032 - val_loss: 1020.9345\n",
      "Epoch 529/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 148.9328 - val_loss: 892.4360\n",
      "Epoch 530/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 184.4610 - val_loss: 872.6932\n",
      "Epoch 531/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 156.5119 - val_loss: 1304.6171\n",
      "Epoch 532/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 121.8143 - val_loss: 1002.1435\n",
      "Epoch 533/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 139.8083 - val_loss: 954.6157\n",
      "Epoch 534/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 120.8243 - val_loss: 996.7807\n",
      "Epoch 535/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 120.9025 - val_loss: 792.7043\n",
      "Epoch 536/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 142.8150 - val_loss: 1064.5682\n",
      "Epoch 537/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 112.3340 - val_loss: 1181.8054\n",
      "Epoch 538/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 152.9247 - val_loss: 906.0187\n",
      "Epoch 539/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 118.0607 - val_loss: 821.3717\n",
      "Epoch 540/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 132.7283 - val_loss: 832.3614\n",
      "Epoch 541/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 126.9636 - val_loss: 1039.8784\n",
      "Epoch 542/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 128.0383 - val_loss: 860.9837\n",
      "Epoch 543/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 139.9910 - val_loss: 877.4023\n",
      "Epoch 544/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 103.7180 - val_loss: 1002.8528\n",
      "Epoch 545/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 197.3070 - val_loss: 1148.4496\n",
      "Epoch 546/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 219.6864 - val_loss: 1157.8886\n",
      "Epoch 547/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 220.3976 - val_loss: 897.5686\n",
      "Epoch 548/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 189.9204 - val_loss: 857.0980\n",
      "Epoch 549/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 175.6223 - val_loss: 904.2728\n",
      "Epoch 550/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 164.2245 - val_loss: 750.3911\n",
      "Epoch 551/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 177.1838 - val_loss: 818.9844\n",
      "Epoch 552/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 161.9117 - val_loss: 850.6856\n",
      "Epoch 553/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 178.3809 - val_loss: 760.1810\n",
      "Epoch 554/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 173.2705 - val_loss: 897.3890\n",
      "Epoch 555/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 166.7718 - val_loss: 1016.6003\n",
      "Epoch 556/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 187.5090 - val_loss: 867.0789\n",
      "Epoch 557/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 163.4147 - val_loss: 1164.2302\n",
      "Epoch 558/2000\n",
      "2800/2800 [==============================] - 0s 147us/step - loss: 159.6258 - val_loss: 1067.4763\n",
      "Epoch 559/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 156.6751 - val_loss: 854.7550\n",
      "Epoch 560/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 138.4355 - val_loss: 1010.0194\n",
      "Epoch 561/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 147.0471 - val_loss: 998.5353\n",
      "Epoch 562/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 135.3983 - val_loss: 854.7529\n",
      "Epoch 563/2000\n",
      "2800/2800 [==============================] - 0s 113us/step - loss: 154.5188 - val_loss: 833.7831\n",
      "Epoch 564/2000\n",
      "2800/2800 [==============================] - 0s 109us/step - loss: 159.3165 - val_loss: 914.7353\n",
      "Epoch 565/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 162.0181 - val_loss: 991.4289\n",
      "Epoch 566/2000\n",
      "2800/2800 [==============================] - 0s 108us/step - loss: 156.1400 - val_loss: 879.0228\n",
      "Epoch 567/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 158.5199 - val_loss: 1142.7825\n",
      "Epoch 568/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 142.7051 - val_loss: 818.1764\n",
      "Epoch 569/2000\n",
      "2800/2800 [==============================] - 0s 110us/step - loss: 140.2838 - val_loss: 909.4195\n",
      "Epoch 570/2000\n",
      "2800/2800 [==============================] - 0s 115us/step - loss: 160.7062 - val_loss: 829.0205\n",
      "Epoch 571/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 141.7243 - val_loss: 872.4368\n",
      "Epoch 572/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 130.3671 - val_loss: 1246.7647\n",
      "Epoch 573/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 161.2310 - val_loss: 955.2662\n",
      "Epoch 574/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 126.5110 - val_loss: 997.4482\n",
      "Epoch 575/2000\n",
      "2800/2800 [==============================] - 0s 109us/step - loss: 160.3980 - val_loss: 802.8369\n",
      "Epoch 576/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 148.4256 - val_loss: 809.1314\n",
      "Epoch 577/2000\n",
      "2800/2800 [==============================] - 0s 115us/step - loss: 129.0215 - val_loss: 1107.2983\n",
      "Epoch 578/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 141.7849 - val_loss: 1060.6401\n",
      "Epoch 579/2000\n",
      "2800/2800 [==============================] - 0s 111us/step - loss: 136.7024 - val_loss: 951.9633\n",
      "Epoch 580/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 129.0579 - val_loss: 838.9441\n",
      "Epoch 581/2000\n",
      "2800/2800 [==============================] - 0s 111us/step - loss: 136.9777 - val_loss: 822.3511\n",
      "Epoch 582/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 139.4703 - val_loss: 878.6852\n",
      "Epoch 583/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 125.0210 - val_loss: 998.8843\n",
      "Epoch 584/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 85us/step - loss: 127.0414 - val_loss: 796.1273\n",
      "Epoch 585/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 149.1007 - val_loss: 950.7857\n",
      "Epoch 586/2000\n",
      "2800/2800 [==============================] - 0s 111us/step - loss: 120.8862 - val_loss: 836.6694\n",
      "Epoch 587/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 145.9161 - val_loss: 904.8484\n",
      "Epoch 588/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 126.3217 - val_loss: 934.6315\n",
      "Epoch 589/2000\n",
      "2800/2800 [==============================] - 0s 109us/step - loss: 126.5472 - val_loss: 1121.0009\n",
      "Epoch 590/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 120.8123 - val_loss: 890.2631\n",
      "Epoch 591/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 121.3408 - val_loss: 915.0547\n",
      "Epoch 592/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 165.0246 - val_loss: 966.2567\n",
      "Epoch 593/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 123.9170 - val_loss: 987.2817\n",
      "Epoch 594/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 109.9840 - val_loss: 896.6487\n",
      "Epoch 595/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 127.1146 - val_loss: 940.4499\n",
      "Epoch 596/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 119.7999 - val_loss: 1002.6682\n",
      "Epoch 597/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 121.7137 - val_loss: 858.2315\n",
      "Epoch 598/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 144.4939 - val_loss: 1142.9620\n",
      "Epoch 599/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 122.4309 - val_loss: 1070.4796\n",
      "Epoch 600/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 118.1173 - val_loss: 951.6751\n",
      "Epoch 601/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 122.8880 - val_loss: 876.5619\n",
      "Epoch 602/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 130.0585 - val_loss: 835.3989\n",
      "Epoch 603/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 125.3301 - val_loss: 930.5258\n",
      "Epoch 604/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 124.6880 - val_loss: 1035.2900\n",
      "Epoch 605/2000\n",
      "2800/2800 [==============================] - 0s 105us/step - loss: 118.3246 - val_loss: 1458.3629\n",
      "Epoch 606/2000\n",
      "2800/2800 [==============================] - 0s 108us/step - loss: 115.9020 - val_loss: 1153.8158\n",
      "Epoch 607/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 125.3279 - val_loss: 920.2545\n",
      "Epoch 608/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 111.0403 - val_loss: 957.7590\n",
      "Epoch 609/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 136.4547 - val_loss: 995.4933\n",
      "Epoch 610/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 134.9688 - val_loss: 1050.8707\n",
      "Epoch 611/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 132.7742 - val_loss: 854.2431\n",
      "Epoch 612/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 122.7542 - val_loss: 1844.3692\n",
      "Epoch 613/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 152.7676 - val_loss: 876.7027\n",
      "Epoch 614/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 121.5784 - val_loss: 1136.5776\n",
      "Epoch 615/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 113.8185 - val_loss: 1071.7801\n",
      "Epoch 616/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 114.9802 - val_loss: 968.8659\n",
      "Epoch 617/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 114.8217 - val_loss: 990.2395\n",
      "Epoch 618/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 102.6642 - val_loss: 897.7529\n",
      "Epoch 619/2000\n",
      "2800/2800 [==============================] - 0s 111us/step - loss: 112.7050 - val_loss: 1003.0392\n",
      "Epoch 620/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 140.7336 - val_loss: 924.2017\n",
      "Epoch 621/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 134.9052 - val_loss: 957.0519\n",
      "Epoch 622/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 122.1290 - val_loss: 951.2757\n",
      "Epoch 623/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 120.9056 - val_loss: 1167.8251\n",
      "Epoch 624/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 109.2618 - val_loss: 941.6460\n",
      "Epoch 625/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 120.1866 - val_loss: 824.8291\n",
      "Epoch 626/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 121.8304 - val_loss: 972.4841\n",
      "Epoch 627/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 131.3431 - val_loss: 902.6483\n",
      "Epoch 628/2000\n",
      "2800/2800 [==============================] - 1s 189us/step - loss: 139.5330 - val_loss: 1080.1452\n",
      "Epoch 629/2000\n",
      "2800/2800 [==============================] - 1s 200us/step - loss: 132.7579 - val_loss: 920.2460\n",
      "Epoch 630/2000\n",
      "2800/2800 [==============================] - 1s 215us/step - loss: 131.3770 - val_loss: 982.1496\n",
      "Epoch 631/2000\n",
      "2800/2800 [==============================] - 0s 174us/step - loss: 132.3940 - val_loss: 880.5231\n",
      "Epoch 632/2000\n",
      "2800/2800 [==============================] - 1s 185us/step - loss: 116.8801 - val_loss: 963.1762\n",
      "Epoch 633/2000\n",
      "2800/2800 [==============================] - 1s 204us/step - loss: 101.0557 - val_loss: 892.0719\n",
      "Epoch 634/2000\n",
      "2800/2800 [==============================] - 0s 172us/step - loss: 113.3617 - val_loss: 995.0376\n",
      "Epoch 635/2000\n",
      "2800/2800 [==============================] - 0s 174us/step - loss: 107.1640 - val_loss: 919.6203\n",
      "Epoch 636/2000\n",
      "2800/2800 [==============================] - 1s 184us/step - loss: 104.9185 - val_loss: 884.2296\n",
      "Epoch 637/2000\n",
      "2800/2800 [==============================] - 0s 157us/step - loss: 102.7246 - val_loss: 996.3803\n",
      "Epoch 638/2000\n",
      "2800/2800 [==============================] - 0s 161us/step - loss: 110.3105 - val_loss: 942.1524\n",
      "Epoch 639/2000\n",
      "2800/2800 [==============================] - 0s 177us/step - loss: 105.8661 - val_loss: 950.3709\n",
      "Epoch 640/2000\n",
      "2800/2800 [==============================] - 0s 163us/step - loss: 102.9660 - val_loss: 914.9319\n",
      "Epoch 641/2000\n",
      "2800/2800 [==============================] - 0s 168us/step - loss: 107.1726 - val_loss: 835.7944\n",
      "Epoch 642/2000\n",
      "2800/2800 [==============================] - 1s 192us/step - loss: 115.7001 - val_loss: 1385.9442\n",
      "Epoch 643/2000\n",
      "2800/2800 [==============================] - 1s 187us/step - loss: 138.3362 - val_loss: 987.5267\n",
      "Epoch 644/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 104.8762 - val_loss: 942.9523\n",
      "Epoch 645/2000\n",
      "2800/2800 [==============================] - 0s 175us/step - loss: 123.6989 - val_loss: 869.4493\n",
      "Epoch 646/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 128.1091 - val_loss: 1273.9125\n",
      "Epoch 647/2000\n",
      "2800/2800 [==============================] - 1s 185us/step - loss: 107.9795 - val_loss: 1119.0556\n",
      "Epoch 648/2000\n",
      "2800/2800 [==============================] - 1s 195us/step - loss: 116.4843 - val_loss: 963.9325\n",
      "Epoch 649/2000\n",
      "2800/2800 [==============================] - 0s 164us/step - loss: 126.9369 - val_loss: 856.1780\n",
      "Epoch 650/2000\n",
      "2800/2800 [==============================] - 0s 168us/step - loss: 113.2146 - val_loss: 865.4366\n",
      "Epoch 651/2000\n",
      "2800/2800 [==============================] - 0s 169us/step - loss: 116.9012 - val_loss: 999.5237\n",
      "Epoch 652/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 112.3359 - val_loss: 1516.9307\n",
      "Epoch 653/2000\n",
      "2800/2800 [==============================] - 1s 182us/step - loss: 115.9219 - val_loss: 1042.7982\n",
      "Epoch 654/2000\n",
      "2800/2800 [==============================] - 0s 167us/step - loss: 112.2919 - val_loss: 1056.1594\n",
      "Epoch 655/2000\n",
      "2800/2800 [==============================] - 1s 181us/step - loss: 101.4097 - val_loss: 1030.0683\n",
      "Epoch 656/2000\n",
      "2800/2800 [==============================] - 1s 205us/step - loss: 107.9245 - val_loss: 1039.7895\n",
      "Epoch 657/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 160us/step - loss: 108.0508 - val_loss: 1047.0945\n",
      "Epoch 658/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 116.7131 - val_loss: 1317.1392\n",
      "Epoch 659/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 112.2631 - val_loss: 889.6796\n",
      "Epoch 660/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 103.6711 - val_loss: 863.9374\n",
      "Epoch 661/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 97.7751 - val_loss: 925.9502\n",
      "Epoch 662/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 120.9068 - val_loss: 904.2452\n",
      "Epoch 663/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 107.5829 - val_loss: 1029.4620\n",
      "Epoch 664/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 94.0648 - val_loss: 928.9082\n",
      "Epoch 665/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 104.405 - 0s 129us/step - loss: 106.3322 - val_loss: 867.5924\n",
      "Epoch 666/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 109.2781 - val_loss: 1150.5871\n",
      "Epoch 667/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 91.6059 - val_loss: 1034.9791\n",
      "Epoch 668/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 96.3561 - val_loss: 863.5192\n",
      "Epoch 669/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 102.7089 - val_loss: 1166.3429\n",
      "Epoch 670/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 102.4758 - val_loss: 1017.2249\n",
      "Epoch 671/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 100.7095 - val_loss: 830.4374\n",
      "Epoch 672/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 129.9977 - val_loss: 983.2617\n",
      "Epoch 673/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 93.9489 - val_loss: 955.3700\n",
      "Epoch 674/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 105.0283 - val_loss: 1084.2815\n",
      "Epoch 675/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 126.3990 - val_loss: 845.9949\n",
      "Epoch 676/2000\n",
      "2800/2800 [==============================] - 0s 158us/step - loss: 123.1004 - val_loss: 1008.7458\n",
      "Epoch 677/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 101.8240 - val_loss: 1116.9335\n",
      "Epoch 678/2000\n",
      "2800/2800 [==============================] - 0s 170us/step - loss: 94.8220 - val_loss: 995.5863\n",
      "Epoch 679/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 93.7214 - val_loss: 948.5923\n",
      "Epoch 680/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 95.6974 - val_loss: 1080.7082\n",
      "Epoch 681/2000\n",
      "2800/2800 [==============================] - 0s 162us/step - loss: 110.4277 - val_loss: 886.5911\n",
      "Epoch 682/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 103.5477 - val_loss: 1156.3973\n",
      "Epoch 683/2000\n",
      "2800/2800 [==============================] - 0s 161us/step - loss: 98.1134 - val_loss: 1052.5781\n",
      "Epoch 684/2000\n",
      "2800/2800 [==============================] - 0s 147us/step - loss: 97.2487 - val_loss: 899.1561\n",
      "Epoch 685/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 105.0038 - val_loss: 885.2339\n",
      "Epoch 686/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 99.5485 - val_loss: 1010.1553\n",
      "Epoch 687/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 97.7089 - val_loss: 1124.6479\n",
      "Epoch 688/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 93.9517 - val_loss: 969.6308\n",
      "Epoch 689/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 109.3058 - val_loss: 978.7097\n",
      "Epoch 690/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 94.0245 - val_loss: 982.0820\n",
      "Epoch 691/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 89.8332 - val_loss: 1093.0401\n",
      "Epoch 692/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 98.8211 - val_loss: 968.4381\n",
      "Epoch 693/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 92.0877 - val_loss: 1065.8749\n",
      "Epoch 694/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 101.5307 - val_loss: 904.8508\n",
      "Epoch 695/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 109.0670 - val_loss: 975.4292\n",
      "Epoch 696/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 101.5349 - val_loss: 1015.3990\n",
      "Epoch 697/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 112.0099 - val_loss: 1001.8506\n",
      "Epoch 698/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 83.6955 - val_loss: 1016.6707\n",
      "Epoch 699/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 97.7550 - val_loss: 945.6490\n",
      "Epoch 700/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 112.4619 - val_loss: 1005.0602\n",
      "Epoch 701/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 92.7552 - val_loss: 954.9363\n",
      "Epoch 702/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 106.2314 - val_loss: 1090.7440\n",
      "Epoch 703/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 91.4824 - val_loss: 928.2481\n",
      "Epoch 704/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 97.5527 - val_loss: 1015.3575\n",
      "Epoch 705/2000\n",
      "2800/2800 [==============================] - 0s 159us/step - loss: 95.1510 - val_loss: 1024.5863\n",
      "Epoch 706/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 106.0652 - val_loss: 1133.6724\n",
      "Epoch 707/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 104.1502 - val_loss: 920.0772\n",
      "Epoch 708/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 82.4420 - val_loss: 902.2091\n",
      "Epoch 709/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 92.2901 - val_loss: 1030.6775\n",
      "Epoch 710/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 100.4927 - val_loss: 1027.9843\n",
      "Epoch 711/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 89.4847 - val_loss: 969.4631\n",
      "Epoch 712/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 108.7391 - val_loss: 990.6044\n",
      "Epoch 713/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 91.5167 - val_loss: 998.2427\n",
      "Epoch 714/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 85.9946 - val_loss: 979.4545\n",
      "Epoch 715/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 88.3619 - val_loss: 1098.1901\n",
      "Epoch 716/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 108.7179 - val_loss: 949.9738\n",
      "Epoch 717/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 102.6317 - val_loss: 978.8850\n",
      "Epoch 718/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 102.0207 - val_loss: 1069.7295\n",
      "Epoch 719/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 97.7450 - val_loss: 1017.1923\n",
      "Epoch 720/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 84.8541 - val_loss: 1020.0820\n",
      "Epoch 721/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 89.8532 - val_loss: 975.9652\n",
      "Epoch 722/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 99.5399 - val_loss: 1001.7204\n",
      "Epoch 723/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 99.7713 - val_loss: 919.1525\n",
      "Epoch 724/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 122.4302 - val_loss: 989.7132\n",
      "Epoch 725/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 112.7499 - val_loss: 882.9512\n",
      "Epoch 726/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 97.4521 - val_loss: 936.8245\n",
      "Epoch 727/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 87.2905 - val_loss: 922.8765\n",
      "Epoch 728/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 86.5845 - val_loss: 1032.9727\n",
      "Epoch 729/2000\n",
      "2800/2800 [==============================] - 0s 171us/step - loss: 82.4131 - val_loss: 911.2363\n",
      "Epoch 730/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 153us/step - loss: 90.0853 - val_loss: 878.4760\n",
      "Epoch 731/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 93.9140 - val_loss: 1088.8861\n",
      "Epoch 732/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 90.1958 - val_loss: 1061.7371\n",
      "Epoch 733/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 95.3769 - val_loss: 877.1409\n",
      "Epoch 734/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 90.4323 - val_loss: 1004.6241\n",
      "Epoch 735/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 89.8094 - val_loss: 932.7707\n",
      "Epoch 736/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 84.3513 - val_loss: 1022.5530\n",
      "Epoch 737/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 84.2518 - val_loss: 1262.5955\n",
      "Epoch 738/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 97.2636 - val_loss: 878.6087\n",
      "Epoch 739/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 91.1676 - val_loss: 940.8778\n",
      "Epoch 740/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 88.5081 - val_loss: 951.7329\n",
      "Epoch 741/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 94.1775 - val_loss: 1080.7380\n",
      "Epoch 742/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 84.8964 - val_loss: 1041.8262\n",
      "Epoch 743/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 83.5361 - val_loss: 1028.7135\n",
      "Epoch 744/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 82.6567 - val_loss: 935.1443\n",
      "Epoch 745/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 88.9053 - val_loss: 1031.9159\n",
      "Epoch 746/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 120.8790 - val_loss: 1145.1063\n",
      "Epoch 747/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 80.8072 - val_loss: 1110.6846\n",
      "Epoch 748/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 86.0649 - val_loss: 1102.3504\n",
      "Epoch 749/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 100.4608 - val_loss: 900.1322\n",
      "Epoch 750/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 99.8549 - val_loss: 888.6887\n",
      "Epoch 751/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 95.7541 - val_loss: 1085.0471\n",
      "Epoch 752/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 91.7677 - val_loss: 966.1767\n",
      "Epoch 753/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 90.3913 - val_loss: 875.5848\n",
      "Epoch 754/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 89.9954 - val_loss: 1320.2555\n",
      "Epoch 755/2000\n",
      "2800/2800 [==============================] - 0s 160us/step - loss: 81.4359 - val_loss: 1021.0076\n",
      "Epoch 756/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 80.9052 - val_loss: 1061.0442\n",
      "Epoch 757/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 98.7131 - val_loss: 1011.5839\n",
      "Epoch 758/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 90.5634 - val_loss: 937.0277\n",
      "Epoch 759/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 74.1597 - val_loss: 1075.0755\n",
      "Epoch 760/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 83.9863 - val_loss: 1187.4882\n",
      "Epoch 761/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 82.6787 - val_loss: 849.8632\n",
      "Epoch 762/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 88.5707 - val_loss: 1055.8919\n",
      "Epoch 763/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 89.1529 - val_loss: 1031.9386\n",
      "Epoch 764/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 89.3101 - val_loss: 1017.1292\n",
      "Epoch 765/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 88.5611 - val_loss: 1091.0748\n",
      "Epoch 766/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 88.7964 - val_loss: 1006.0697\n",
      "Epoch 767/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 96.2222 - val_loss: 981.0489\n",
      "Epoch 768/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 81.4514 - val_loss: 967.5710\n",
      "Epoch 769/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 80.3333 - val_loss: 1036.7972\n",
      "Epoch 770/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 85.3935 - val_loss: 1025.8919\n",
      "Epoch 771/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 76.8527 - val_loss: 945.1676\n",
      "Epoch 772/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 95.4184 - val_loss: 868.7682\n",
      "Epoch 773/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 89.0175 - val_loss: 923.1004\n",
      "Epoch 774/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 79.3924 - val_loss: 943.4531\n",
      "Epoch 775/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 83.8751 - val_loss: 1093.9629\n",
      "Epoch 776/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 89.7370 - val_loss: 969.3459\n",
      "Epoch 777/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 78.3932 - val_loss: 1276.1803\n",
      "Epoch 778/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 89.8470 - val_loss: 1234.6167\n",
      "Epoch 779/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 79.6713 - val_loss: 998.3337\n",
      "Epoch 780/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 80.5683 - val_loss: 1174.6230\n",
      "Epoch 781/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 75.5612 - val_loss: 967.0940\n",
      "Epoch 782/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 80.5834 - val_loss: 979.8736\n",
      "Epoch 783/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 83.8839 - val_loss: 1247.5992\n",
      "Epoch 784/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 85.9558 - val_loss: 1311.7838\n",
      "Epoch 785/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 87.2483 - val_loss: 1564.7926\n",
      "Epoch 786/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 89.6850 - val_loss: 1090.1021\n",
      "Epoch 787/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 83.8422 - val_loss: 999.0934\n",
      "Epoch 788/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 107.0857 - val_loss: 1039.5889\n",
      "Epoch 789/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 116.3154 - val_loss: 914.1625\n",
      "Epoch 790/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 86.2864 - val_loss: 1198.4678\n",
      "Epoch 791/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 80.7833 - val_loss: 1022.2553\n",
      "Epoch 792/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 80.5325 - val_loss: 1163.5721\n",
      "Epoch 793/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 87.4098 - val_loss: 1272.8655\n",
      "Epoch 794/2000\n",
      "2800/2800 [==============================] - 0s 159us/step - loss: 86.6773 - val_loss: 1217.8461\n",
      "Epoch 795/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 86.4921 - val_loss: 1016.5377\n",
      "Epoch 796/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 85.8769 - val_loss: 1026.1461\n",
      "Epoch 797/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 81.8190 - val_loss: 1109.8476\n",
      "Epoch 798/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 86.3583 - val_loss: 999.8793\n",
      "Epoch 799/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 72.7721 - val_loss: 934.4186\n",
      "Epoch 800/2000\n",
      "2800/2800 [==============================] - 0s 160us/step - loss: 81.5289 - val_loss: 1162.6902\n",
      "Epoch 801/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 107.2930 - val_loss: 1176.6916\n",
      "Epoch 802/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 77.4062 - val_loss: 990.4999\n",
      "Epoch 803/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 141us/step - loss: 75.8277 - val_loss: 911.7749\n",
      "Epoch 804/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 85.5996 - val_loss: 1008.4778\n",
      "Epoch 805/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 77.9159 - val_loss: 899.8821\n",
      "Epoch 806/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 75.1800 - val_loss: 1008.4295\n",
      "Epoch 807/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 74.3223 - val_loss: 1221.7162\n",
      "Epoch 808/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 82.5743 - val_loss: 1115.4589\n",
      "Epoch 809/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 82.4545 - val_loss: 1004.0250\n",
      "Epoch 810/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 72.2577 - val_loss: 1017.9533\n",
      "Epoch 811/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 77.6329 - val_loss: 917.2760\n",
      "Epoch 812/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 77.1832 - val_loss: 977.3934\n",
      "Epoch 813/2000\n",
      "2800/2800 [==============================] - 0s 157us/step - loss: 74.2033 - val_loss: 1023.1886\n",
      "Epoch 814/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 92.8939 - val_loss: 1021.4316\n",
      "Epoch 815/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 77.8098 - val_loss: 979.8836\n",
      "Epoch 816/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 75.6625 - val_loss: 1088.2411\n",
      "Epoch 817/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 77.6827 - val_loss: 968.9406\n",
      "Epoch 818/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 80.3081 - val_loss: 1149.1948\n",
      "Epoch 819/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 77.0384 - val_loss: 941.4432\n",
      "Epoch 820/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 80.2573 - val_loss: 1026.0257\n",
      "Epoch 821/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 81.7134 - val_loss: 1017.7441\n",
      "Epoch 822/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 76.5798 - val_loss: 1052.8107\n",
      "Epoch 823/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 78.8115 - val_loss: 1113.5933\n",
      "Epoch 824/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 76.2429 - val_loss: 1049.7977\n",
      "Epoch 825/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 75.3759 - val_loss: 1185.8009\n",
      "Epoch 826/2000\n",
      "2800/2800 [==============================] - 0s 166us/step - loss: 75.6310 - val_loss: 921.5758\n",
      "Epoch 827/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 75.9918 - val_loss: 1245.1643\n",
      "Epoch 828/2000\n",
      "2800/2800 [==============================] - 0s 168us/step - loss: 93.8590 - val_loss: 956.0190\n",
      "Epoch 829/2000\n",
      "2800/2800 [==============================] - 0s 160us/step - loss: 75.7126 - val_loss: 963.7462\n",
      "Epoch 830/2000\n",
      "2800/2800 [==============================] - 0s 147us/step - loss: 74.4326 - val_loss: 984.0608\n",
      "Epoch 831/2000\n",
      "2800/2800 [==============================] - 0s 162us/step - loss: 80.5537 - val_loss: 1165.7613\n",
      "Epoch 832/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 77.4488 - val_loss: 1034.1384\n",
      "Epoch 833/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 105.2311 - val_loss: 1031.2259\n",
      "Epoch 834/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 77.4884 - val_loss: 948.0249\n",
      "Epoch 835/2000\n",
      "2800/2800 [==============================] - 0s 160us/step - loss: 76.8574 - val_loss: 1117.6908\n",
      "Epoch 836/2000\n",
      "2800/2800 [==============================] - 0s 159us/step - loss: 85.0398 - val_loss: 941.5413\n",
      "Epoch 837/2000\n",
      "2800/2800 [==============================] - 0s 161us/step - loss: 72.3403 - val_loss: 930.4472\n",
      "Epoch 838/2000\n",
      "2800/2800 [==============================] - 0s 157us/step - loss: 71.3959 - val_loss: 1085.1483\n",
      "Epoch 839/2000\n",
      "2800/2800 [==============================] - 0s 167us/step - loss: 73.3599 - val_loss: 1111.4223\n",
      "Epoch 840/2000\n",
      "2800/2800 [==============================] - 0s 165us/step - loss: 72.3998 - val_loss: 1043.1306\n",
      "Epoch 841/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 70.4401 - val_loss: 1095.3746\n",
      "Epoch 842/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 80.4429 - val_loss: 1195.5130\n",
      "Epoch 843/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 85.0090 - val_loss: 1034.2159\n",
      "Epoch 844/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 76.4795 - val_loss: 1087.9423\n",
      "Epoch 845/2000\n",
      "2800/2800 [==============================] - 0s 161us/step - loss: 67.8637 - val_loss: 971.4472\n",
      "Epoch 846/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 77.1187 - val_loss: 1081.7038\n",
      "Epoch 847/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 80.8373 - val_loss: 953.9146\n",
      "Epoch 848/2000\n",
      "2800/2800 [==============================] - 0s 168us/step - loss: 69.5164 - val_loss: 1246.9925\n",
      "Epoch 849/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 83.5593 - val_loss: 1029.1213\n",
      "Epoch 850/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 71.4363 - val_loss: 944.8644\n",
      "Epoch 851/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 75.7814 - val_loss: 1264.6122\n",
      "Epoch 852/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 80.8082 - val_loss: 890.8669\n",
      "Epoch 853/2000\n",
      "2800/2800 [==============================] - 0s 164us/step - loss: 76.4749 - val_loss: 975.4556\n",
      "Epoch 854/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 72.7695 - val_loss: 1167.9264\n",
      "Epoch 855/2000\n",
      "2800/2800 [==============================] - 0s 173us/step - loss: 87.8357 - val_loss: 1443.4698\n",
      "Epoch 856/2000\n",
      "2800/2800 [==============================] - 0s 173us/step - loss: 87.7210 - val_loss: 1041.1473\n",
      "Epoch 857/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 77.7971 - val_loss: 1065.6332\n",
      "Epoch 858/2000\n",
      "2800/2800 [==============================] - 1s 179us/step - loss: 68.4539 - val_loss: 942.6762\n",
      "Epoch 859/2000\n",
      "2800/2800 [==============================] - 0s 158us/step - loss: 74.6593 - val_loss: 968.4282\n",
      "Epoch 860/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 76.6723 - val_loss: 1232.7248\n",
      "Epoch 861/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 70.2811 - val_loss: 1033.5454\n",
      "Epoch 862/2000\n",
      "2800/2800 [==============================] - 0s 161us/step - loss: 71.9195 - val_loss: 1173.8115\n",
      "Epoch 863/2000\n",
      "2800/2800 [==============================] - 0s 162us/step - loss: 76.8252 - val_loss: 976.6909\n",
      "Epoch 864/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 76.1079 - val_loss: 1126.0566\n",
      "Epoch 865/2000\n",
      "2800/2800 [==============================] - 1s 194us/step - loss: 81.8677 - val_loss: 1001.7798\n",
      "Epoch 866/2000\n",
      "2800/2800 [==============================] - 0s 156us/step - loss: 93.5861 - val_loss: 1400.2737\n",
      "Epoch 867/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 72.9539 - val_loss: 1121.5438\n",
      "Epoch 868/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 75.7059 - val_loss: 1003.1076\n",
      "Epoch 869/2000\n",
      "2800/2800 [==============================] - 0s 161us/step - loss: 69.8253 - val_loss: 932.6924\n",
      "Epoch 870/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 76.6050 - val_loss: 1323.4670\n",
      "Epoch 871/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 79.6088 - val_loss: 953.6801\n",
      "Epoch 872/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 73.5448 - val_loss: 928.9662\n",
      "Epoch 873/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 79.0104 - val_loss: 916.7449\n",
      "Epoch 874/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 77.5231 - val_loss: 1112.9174\n",
      "Epoch 875/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 79.1948 - val_loss: 1038.2790\n",
      "Epoch 876/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 152us/step - loss: 77.6191 - val_loss: 959.3283\n",
      "Epoch 877/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 73.6863 - val_loss: 1111.4764\n",
      "Epoch 878/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 77.3090 - val_loss: 1028.4251\n",
      "Epoch 879/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 93.0632 - val_loss: 1043.5583\n",
      "Epoch 880/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 75.3792 - val_loss: 1033.5870\n",
      "Epoch 881/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 67.0620 - val_loss: 942.8489\n",
      "Epoch 882/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 69.4636 - val_loss: 1210.9932\n",
      "Epoch 883/2000\n",
      "2800/2800 [==============================] - 0s 103us/step - loss: 69.7218 - val_loss: 979.5112\n",
      "Epoch 884/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 72.2152 - val_loss: 964.1290\n",
      "Epoch 885/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 71.5401 - val_loss: 967.7445\n",
      "Epoch 886/2000\n",
      "2800/2800 [==============================] - 0s 109us/step - loss: 74.3607 - val_loss: 1079.7528\n",
      "Epoch 887/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 67.4142 - val_loss: 1175.6972\n",
      "Epoch 888/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 71.3833 - val_loss: 1060.3222\n",
      "Epoch 889/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 65.8975 - val_loss: 998.4525\n",
      "Epoch 890/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 70.3629 - val_loss: 1131.0231\n",
      "Epoch 891/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 71.5008 - val_loss: 1120.7419\n",
      "Epoch 892/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 87.0042 - val_loss: 1000.9682\n",
      "Epoch 893/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 73.2832 - val_loss: 971.4812\n",
      "Epoch 894/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 72.8607 - val_loss: 1128.4824\n",
      "Epoch 895/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 66.0875 - val_loss: 982.8394\n",
      "Epoch 896/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 68.6446 - val_loss: 1011.9826\n",
      "Epoch 897/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 65.4593 - val_loss: 1151.2765\n",
      "Epoch 898/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 66.2511 - val_loss: 1048.3911\n",
      "Epoch 899/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 74.5651 - val_loss: 913.5582\n",
      "Epoch 900/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 73.5757 - val_loss: 1105.8328\n",
      "Epoch 901/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 75.7729 - val_loss: 1241.1564\n",
      "Epoch 902/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 69.3710 - val_loss: 1030.6969\n",
      "Epoch 903/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 64.1561 - val_loss: 933.7936\n",
      "Epoch 904/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 71.4063 - val_loss: 1214.1004\n",
      "Epoch 905/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 68.0568 - val_loss: 989.6924\n",
      "Epoch 906/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 64.9376 - val_loss: 1320.9162\n",
      "Epoch 907/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 77.7617 - val_loss: 1071.3250\n",
      "Epoch 908/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 72.8643 - val_loss: 1101.4823\n",
      "Epoch 909/2000\n",
      "2800/2800 [==============================] - 0s 99us/step - loss: 75.0331 - val_loss: 1034.3661\n",
      "Epoch 910/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 76.2606 - val_loss: 1014.0401\n",
      "Epoch 911/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 66.8561 - val_loss: 1142.6409\n",
      "Epoch 912/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 66.0561 - val_loss: 1127.2016\n",
      "Epoch 913/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 68.4097 - val_loss: 1077.0516\n",
      "Epoch 914/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 73.2530 - val_loss: 1032.3020\n",
      "Epoch 915/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 66.0373 - val_loss: 996.4976\n",
      "Epoch 916/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 68.9598 - val_loss: 972.6414\n",
      "Epoch 917/2000\n",
      "2800/2800 [==============================] - 0s 157us/step - loss: 63.6991 - val_loss: 1053.3077\n",
      "Epoch 918/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 66.6423 - val_loss: 973.0793\n",
      "Epoch 919/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 60.6501 - val_loss: 1100.4216\n",
      "Epoch 920/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 62.1880 - val_loss: 1036.5888\n",
      "Epoch 921/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 67.1998 - val_loss: 1363.8335\n",
      "Epoch 922/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 72.4411 - val_loss: 1427.8721\n",
      "Epoch 923/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 64.8713 - val_loss: 1038.4158\n",
      "Epoch 924/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 60.3041 - val_loss: 1088.9582\n",
      "Epoch 925/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 69.2508 - val_loss: 1042.3341\n",
      "Epoch 926/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 61.8017 - val_loss: 916.0601\n",
      "Epoch 927/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 72.0134 - val_loss: 1066.1331\n",
      "Epoch 928/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 71.9047 - val_loss: 1282.2363\n",
      "Epoch 929/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 72.0096 - val_loss: 1185.8797\n",
      "Epoch 930/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 70.8176 - val_loss: 1271.1324\n",
      "Epoch 931/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 73.1748 - val_loss: 1100.1343\n",
      "Epoch 932/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 73.6154 - val_loss: 1171.1885\n",
      "Epoch 933/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 58.4976 - val_loss: 1301.4356\n",
      "Epoch 934/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 67.2875 - val_loss: 1035.5419\n",
      "Epoch 935/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 64.3135 - val_loss: 1341.7575\n",
      "Epoch 936/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 72.2219 - val_loss: 952.9202\n",
      "Epoch 937/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 65.1927 - val_loss: 1073.8459\n",
      "Epoch 938/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 67.2523 - val_loss: 1205.0934\n",
      "Epoch 939/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 65.6179 - val_loss: 1180.7967\n",
      "Epoch 940/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 66.5808 - val_loss: 913.7817\n",
      "Epoch 941/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 77.1255 - val_loss: 1152.1728\n",
      "Epoch 942/2000\n",
      "2800/2800 [==============================] - 0s 105us/step - loss: 76.3984 - val_loss: 995.8324\n",
      "Epoch 943/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 74.8753 - val_loss: 1067.8327\n",
      "Epoch 944/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 75.6900 - val_loss: 953.8197\n",
      "Epoch 945/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 72.7339 - val_loss: 1022.7094\n",
      "Epoch 946/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 60.5946 - val_loss: 953.5838\n",
      "Epoch 947/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 64.4035 - val_loss: 1092.0598\n",
      "Epoch 948/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 61.4961 - val_loss: 999.1508\n",
      "Epoch 949/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 65.1225 - val_loss: 950.3945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 950/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 66.1944 - val_loss: 1031.5394\n",
      "Epoch 951/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 62.2525 - val_loss: 996.1574\n",
      "Epoch 952/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 63.8126 - val_loss: 1047.5226\n",
      "Epoch 953/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 59.2686 - val_loss: 1044.7573\n",
      "Epoch 954/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 60.1166 - val_loss: 967.4298\n",
      "Epoch 955/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 67.0729 - val_loss: 1060.1834\n",
      "Epoch 956/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 63.0598 - val_loss: 1054.1304\n",
      "Epoch 957/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 86.00 - 0s 90us/step - loss: 83.2402 - val_loss: 1054.0404\n",
      "Epoch 958/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 65.9583 - val_loss: 1025.5927\n",
      "Epoch 959/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 57.3941 - val_loss: 1044.7588\n",
      "Epoch 960/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 64.1494 - val_loss: 1002.4640\n",
      "Epoch 961/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 69.2989 - val_loss: 926.8588\n",
      "Epoch 962/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 65.5131 - val_loss: 1105.2979\n",
      "Epoch 963/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 64.5973 - val_loss: 929.9779\n",
      "Epoch 964/2000\n",
      "2800/2800 [==============================] - 0s 78us/step - loss: 65.6535 - val_loss: 1040.2184\n",
      "Epoch 965/2000\n",
      "2800/2800 [==============================] - 0s 77us/step - loss: 65.7753 - val_loss: 915.6471\n",
      "Epoch 966/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 65.3660 - val_loss: 1084.1365\n",
      "Epoch 967/2000\n",
      "2800/2800 [==============================] - 0s 79us/step - loss: 69.3952 - val_loss: 988.6768\n",
      "Epoch 968/2000\n",
      "2800/2800 [==============================] - 0s 79us/step - loss: 66.6074 - val_loss: 1031.1612\n",
      "Epoch 969/2000\n",
      "2800/2800 [==============================] - 0s 79us/step - loss: 62.1306 - val_loss: 1124.1778\n",
      "Epoch 970/2000\n",
      "2800/2800 [==============================] - 0s 79us/step - loss: 67.8331 - val_loss: 891.6298\n",
      "Epoch 971/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 75.0558 - val_loss: 1007.3796\n",
      "Epoch 972/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 66.2849 - val_loss: 1164.2597\n",
      "Epoch 973/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 66.4445 - val_loss: 1439.9533\n",
      "Epoch 974/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 67.0273 - val_loss: 1127.4595\n",
      "Epoch 975/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 62.5546 - val_loss: 1418.7614\n",
      "Epoch 976/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 67.6044 - val_loss: 1298.5950\n",
      "Epoch 977/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 63.8626 - val_loss: 1001.0073\n",
      "Epoch 978/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 62.3455 - val_loss: 1017.1788\n",
      "Epoch 979/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 69.4629 - val_loss: 948.1758\n",
      "Epoch 980/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 61.4627 - val_loss: 1013.2748\n",
      "Epoch 981/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 61.2612 - val_loss: 1192.1384\n",
      "Epoch 982/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 60.7540 - val_loss: 1007.3208\n",
      "Epoch 983/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 63.7275 - val_loss: 928.0449\n",
      "Epoch 984/2000\n",
      "2800/2800 [==============================] - 0s 78us/step - loss: 68.9237 - val_loss: 1263.4490\n",
      "Epoch 985/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 69.8001 - val_loss: 1009.7028\n",
      "Epoch 986/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 70.0821 - val_loss: 1110.0807\n",
      "Epoch 987/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 65.4041 - val_loss: 1004.6524\n",
      "Epoch 988/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 57.2071 - val_loss: 993.6720\n",
      "Epoch 989/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 62.6953 - val_loss: 1135.2478\n",
      "Epoch 990/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 61.9413 - val_loss: 1062.7950\n",
      "Epoch 991/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 75.5191 - val_loss: 1077.1441\n",
      "Epoch 992/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 64.3910 - val_loss: 1071.9381\n",
      "Epoch 993/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 75.1872 - val_loss: 987.3912\n",
      "Epoch 994/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 64.3347 - val_loss: 1103.8560\n",
      "Epoch 995/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 64.2210 - val_loss: 959.3754\n",
      "Epoch 996/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 57.7213 - val_loss: 986.6600\n",
      "Epoch 997/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 68.9291 - val_loss: 1025.1311\n",
      "Epoch 998/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 57.6588 - val_loss: 974.1343\n",
      "Epoch 999/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 61.7312 - val_loss: 976.6103\n",
      "Epoch 1000/2000\n",
      "2800/2800 [==============================] - 0s 103us/step - loss: 64.1064 - val_loss: 1181.3149\n",
      "Epoch 1001/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 60.2397 - val_loss: 1018.5551\n",
      "Epoch 1002/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 78.5196 - val_loss: 1021.1854\n",
      "Epoch 1003/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 57.6794 - val_loss: 1041.2214\n",
      "Epoch 1004/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 60.1648 - val_loss: 1024.6794\n",
      "Epoch 1005/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 60.6037 - val_loss: 968.6679\n",
      "Epoch 1006/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 63.8150 - val_loss: 1027.7516\n",
      "Epoch 1007/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 64.4659 - val_loss: 1148.3574\n",
      "Epoch 1008/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 64.1368 - val_loss: 1168.9057\n",
      "Epoch 1009/2000\n",
      "2800/2800 [==============================] - 0s 115us/step - loss: 64.3844 - val_loss: 1103.6317\n",
      "Epoch 1010/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 68.5621 - val_loss: 1169.9273\n",
      "Epoch 1011/2000\n",
      "2800/2800 [==============================] - 0s 157us/step - loss: 61.2664 - val_loss: 1105.9821\n",
      "Epoch 1012/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 65.7875 - val_loss: 973.8906\n",
      "Epoch 1013/2000\n",
      "2800/2800 [==============================] - 0s 120us/step - loss: 59.9174 - val_loss: 1118.8077\n",
      "Epoch 1014/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 59.5982 - val_loss: 959.7857\n",
      "Epoch 1015/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 57.5988 - val_loss: 1025.6146\n",
      "Epoch 1016/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 59.1809 - val_loss: 1085.9488\n",
      "Epoch 1017/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 60.3831 - val_loss: 1100.2641\n",
      "Epoch 1018/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 59.8648 - val_loss: 1029.2431\n",
      "Epoch 1019/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 58.2878 - val_loss: 1051.8672\n",
      "Epoch 1020/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 54.5999 - val_loss: 1426.3959\n",
      "Epoch 1021/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 89.9646 - val_loss: 1008.5991\n",
      "Epoch 1022/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 65.1576 - val_loss: 979.5750\n",
      "Epoch 1023/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 135us/step - loss: 58.4919 - val_loss: 1022.5729\n",
      "Epoch 1024/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 59.0790 - val_loss: 1034.1227\n",
      "Epoch 1025/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 66.5538 - val_loss: 1062.2427\n",
      "Epoch 1026/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 65.6373 - val_loss: 1150.1426\n",
      "Epoch 1027/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 57.0519 - val_loss: 1067.6111\n",
      "Epoch 1028/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 64.0692 - val_loss: 973.3363\n",
      "Epoch 1029/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 61.0159 - val_loss: 1067.5837\n",
      "Epoch 1030/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 57.3559 - val_loss: 1031.3292\n",
      "Epoch 1031/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 53.9525 - val_loss: 1265.3542\n",
      "Epoch 1032/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 61.1333 - val_loss: 1124.4212\n",
      "Epoch 1033/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 59.0554 - val_loss: 973.6624\n",
      "Epoch 1034/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 61.5654 - val_loss: 1110.5135\n",
      "Epoch 1035/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 56.3038 - val_loss: 1243.0373\n",
      "Epoch 1036/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 60.5266 - val_loss: 1191.7504\n",
      "Epoch 1037/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 64.6686 - val_loss: 1079.5057\n",
      "Epoch 1038/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 59.6155 - val_loss: 1068.4287\n",
      "Epoch 1039/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 61.1827 - val_loss: 1110.3324\n",
      "Epoch 1040/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 65.5677 - val_loss: 923.5869\n",
      "Epoch 1041/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 67.2817 - val_loss: 982.6741\n",
      "Epoch 1042/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 59.5468 - val_loss: 1011.5341\n",
      "Epoch 1043/2000\n",
      "2800/2800 [==============================] - 0s 158us/step - loss: 57.4410 - val_loss: 1035.4067\n",
      "Epoch 1044/2000\n",
      "2800/2800 [==============================] - 0s 158us/step - loss: 63.4679 - val_loss: 973.3175\n",
      "Epoch 1045/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 59.7344 - val_loss: 1036.8298\n",
      "Epoch 1046/2000\n",
      "2800/2800 [==============================] - 0s 165us/step - loss: 53.3853 - val_loss: 972.7184\n",
      "Epoch 1047/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 57.6029 - val_loss: 982.9115\n",
      "Epoch 1048/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 56.6538 - val_loss: 1102.0085\n",
      "Epoch 1049/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 59.3926 - val_loss: 1012.4890\n",
      "Epoch 1050/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 59.9194 - val_loss: 1110.0329\n",
      "Epoch 1051/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 55.6270 - val_loss: 1129.5531\n",
      "Epoch 1052/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 52.3825 - val_loss: 978.7390\n",
      "Epoch 1053/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 65.5647 - val_loss: 1123.7345\n",
      "Epoch 1054/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 56.9140 - val_loss: 1059.1682\n",
      "Epoch 1055/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 54.2199 - val_loss: 992.1367\n",
      "Epoch 1056/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 57.5891 - val_loss: 1025.0888\n",
      "Epoch 1057/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 53.7975 - val_loss: 1146.2243\n",
      "Epoch 1058/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 56.0723 - val_loss: 935.3352\n",
      "Epoch 1059/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 61.3433 - val_loss: 966.7053\n",
      "Epoch 1060/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 55.3211 - val_loss: 979.2722\n",
      "Epoch 1061/2000\n",
      "2800/2800 [==============================] - 0s 154us/step - loss: 54.2633 - val_loss: 1017.0712\n",
      "Epoch 1062/2000\n",
      "2800/2800 [==============================] - 0s 147us/step - loss: 70.1678 - val_loss: 990.7145\n",
      "Epoch 1063/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 61.7802 - val_loss: 1203.3683\n",
      "Epoch 1064/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 58.1666 - val_loss: 1234.9353\n",
      "Epoch 1065/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 61.6310 - val_loss: 990.3161\n",
      "Epoch 1066/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 56.1206 - val_loss: 1113.3477\n",
      "Epoch 1067/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 55.0107 - val_loss: 957.7875\n",
      "Epoch 1068/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 55.6084 - val_loss: 1273.6411\n",
      "Epoch 1069/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 59.6888 - val_loss: 1068.1594\n",
      "Epoch 1070/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 55.5919 - val_loss: 995.7812\n",
      "Epoch 1071/2000\n",
      "2800/2800 [==============================] - 0s 113us/step - loss: 64.6227 - val_loss: 1216.6237\n",
      "Epoch 1072/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 59.6982 - val_loss: 1074.6540\n",
      "Epoch 1073/2000\n",
      "2800/2800 [==============================] - 0s 109us/step - loss: 57.0345 - val_loss: 1149.9725\n",
      "Epoch 1074/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 63.3802 - val_loss: 1067.0525\n",
      "Epoch 1075/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 57.3874 - val_loss: 993.3357\n",
      "Epoch 1076/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 67.2223 - val_loss: 989.7417\n",
      "Epoch 1077/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 58.9058 - val_loss: 1269.8393\n",
      "Epoch 1078/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 57.1259 - val_loss: 1154.9157\n",
      "Epoch 1079/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 53.6323 - val_loss: 1050.4444\n",
      "Epoch 1080/2000\n",
      "2800/2800 [==============================] - 0s 120us/step - loss: 56.3306 - val_loss: 1081.6524\n",
      "Epoch 1081/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 58.6510 - val_loss: 1108.5773\n",
      "Epoch 1082/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 57.0935 - val_loss: 1012.4631\n",
      "Epoch 1083/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 55.0096 - val_loss: 1134.9963\n",
      "Epoch 1084/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 59.7563 - val_loss: 1172.5051\n",
      "Epoch 1085/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 60.2253 - val_loss: 1017.2258\n",
      "Epoch 1086/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 57.2250 - val_loss: 940.8723\n",
      "Epoch 1087/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 58.2531 - val_loss: 987.2006\n",
      "Epoch 1088/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 57.9282 - val_loss: 1147.0665\n",
      "Epoch 1089/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 61.3088 - val_loss: 1114.4863\n",
      "Epoch 1090/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 51.9391 - val_loss: 1082.3794\n",
      "Epoch 1091/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 53.2365 - val_loss: 1077.0543\n",
      "Epoch 1092/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 59.4348 - val_loss: 986.6105\n",
      "Epoch 1093/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 57.9977 - val_loss: 1014.2642\n",
      "Epoch 1094/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 53.5749 - val_loss: 1044.6431\n",
      "Epoch 1095/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 55.4650 - val_loss: 939.5910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1096/2000\n",
      "2800/2800 [==============================] - 0s 99us/step - loss: 56.7740 - val_loss: 1104.0659\n",
      "Epoch 1097/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 54.7295 - val_loss: 1078.5042\n",
      "Epoch 1098/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 52.0393 - val_loss: 1122.8316\n",
      "Epoch 1099/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 56.4431 - val_loss: 1048.0927\n",
      "Epoch 1100/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 57.1430 - val_loss: 1078.0410\n",
      "Epoch 1101/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 60.5888 - val_loss: 1094.3474\n",
      "Epoch 1102/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 54.5701 - val_loss: 1135.7936\n",
      "Epoch 1103/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 60.4601 - val_loss: 1048.8871\n",
      "Epoch 1104/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 63.5297 - val_loss: 1017.9779\n",
      "Epoch 1105/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 55.0599 - val_loss: 1228.9474\n",
      "Epoch 1106/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 63.2557 - val_loss: 979.0919\n",
      "Epoch 1107/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 59.3256 - val_loss: 989.0988\n",
      "Epoch 1108/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 57.5475 - val_loss: 972.7403\n",
      "Epoch 1109/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 55.2751 - val_loss: 1073.1687\n",
      "Epoch 1110/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 58.6391 - val_loss: 1095.7380\n",
      "Epoch 1111/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 57.9016 - val_loss: 1089.8709\n",
      "Epoch 1112/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 52.4267 - val_loss: 1015.0549\n",
      "Epoch 1113/2000\n",
      "2800/2800 [==============================] - 0s 99us/step - loss: 52.5983 - val_loss: 1004.0769\n",
      "Epoch 1114/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 52.2631 - val_loss: 1144.2736\n",
      "Epoch 1115/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 50.7460 - val_loss: 1110.4143\n",
      "Epoch 1116/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 61.7925 - val_loss: 1015.2955\n",
      "Epoch 1117/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 58.1870 - val_loss: 1007.8913\n",
      "Epoch 1118/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 57.0031 - val_loss: 1100.8075\n",
      "Epoch 1119/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 51.1793 - val_loss: 1069.1943\n",
      "Epoch 1120/2000\n",
      "2800/2800 [==============================] - 0s 160us/step - loss: 56.3833 - val_loss: 1009.1351\n",
      "Epoch 1121/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 51.7368 - val_loss: 1003.3131\n",
      "Epoch 1122/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 54.4761 - val_loss: 1106.7798\n",
      "Epoch 1123/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 52.6581 - val_loss: 1101.1728\n",
      "Epoch 1124/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 53.3968 - val_loss: 1150.3496\n",
      "Epoch 1125/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 58.2306 - val_loss: 1036.3112\n",
      "Epoch 1126/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 52.0884 - val_loss: 955.8156\n",
      "Epoch 1127/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 54.0502 - val_loss: 997.6648\n",
      "Epoch 1128/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 56.0881 - val_loss: 1055.0957\n",
      "Epoch 1129/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 48.8862 - val_loss: 1098.5430\n",
      "Epoch 1130/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 51.1099 - val_loss: 1002.6708\n",
      "Epoch 1131/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 57.4584 - val_loss: 997.3532\n",
      "Epoch 1132/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 54.1839 - val_loss: 1019.9665\n",
      "Epoch 1133/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 48.7106 - val_loss: 984.5765\n",
      "Epoch 1134/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 54.6640 - val_loss: 1121.2774\n",
      "Epoch 1135/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 49.6342 - val_loss: 1142.6166\n",
      "Epoch 1136/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 65.2316 - val_loss: 1170.0629\n",
      "Epoch 1137/2000\n",
      "2800/2800 [==============================] - 0s 171us/step - loss: 63.3170 - val_loss: 1112.5651\n",
      "Epoch 1138/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 56.0593 - val_loss: 1007.4187\n",
      "Epoch 1139/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 52.9436 - val_loss: 1093.0134\n",
      "Epoch 1140/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 52.2904 - val_loss: 1066.9365\n",
      "Epoch 1141/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 53.1779 - val_loss: 1126.9463\n",
      "Epoch 1142/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 56.0782 - val_loss: 1123.2223\n",
      "Epoch 1143/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 59.8362 - val_loss: 1167.0420\n",
      "Epoch 1144/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 61.6808 - val_loss: 1067.4297\n",
      "Epoch 1145/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 56.8278 - val_loss: 951.5000\n",
      "Epoch 1146/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 58.0615 - val_loss: 1117.2115\n",
      "Epoch 1147/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 62.6378 - val_loss: 1064.9424\n",
      "Epoch 1148/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 49.5980 - val_loss: 971.0103\n",
      "Epoch 1149/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 65.9023 - val_loss: 1052.7599\n",
      "Epoch 1150/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 53.8740 - val_loss: 984.4159\n",
      "Epoch 1151/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 56.3736 - val_loss: 1038.2176\n",
      "Epoch 1152/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 54.33 - 0s 127us/step - loss: 54.1420 - val_loss: 1110.4083\n",
      "Epoch 1153/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 56.3843 - val_loss: 1071.4427\n",
      "Epoch 1154/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 52.7535 - val_loss: 997.3531\n",
      "Epoch 1155/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 51.0464 - val_loss: 1046.0234\n",
      "Epoch 1156/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 53.3302 - val_loss: 972.7754\n",
      "Epoch 1157/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 53.4648 - val_loss: 1020.2139\n",
      "Epoch 1158/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 60.9037 - val_loss: 1109.1380\n",
      "Epoch 1159/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 49.0879 - val_loss: 1185.5391\n",
      "Epoch 1160/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 60.8438 - val_loss: 944.3332\n",
      "Epoch 1161/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 51.2837 - val_loss: 987.7058\n",
      "Epoch 1162/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 56.5284 - val_loss: 1090.0659\n",
      "Epoch 1163/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 51.3944 - val_loss: 1304.3124\n",
      "Epoch 1164/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 59.3409 - val_loss: 1151.8878\n",
      "Epoch 1165/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 53.8745 - val_loss: 1142.6215\n",
      "Epoch 1166/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 51.5887 - val_loss: 1081.5074\n",
      "Epoch 1167/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 55.5877 - val_loss: 1009.0925\n",
      "Epoch 1168/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 135us/step - loss: 57.1086 - val_loss: 1078.5381\n",
      "Epoch 1169/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 53.6885 - val_loss: 1191.0448\n",
      "Epoch 1170/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 50.8363 - val_loss: 1114.7799\n",
      "Epoch 1171/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 53.0503 - val_loss: 1187.5056\n",
      "Epoch 1172/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 55.6690 - val_loss: 1107.8559\n",
      "Epoch 1173/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 52.3467 - val_loss: 1115.4830\n",
      "Epoch 1174/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 53.2681 - val_loss: 1345.7854\n",
      "Epoch 1175/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 58.7773 - val_loss: 1062.6579\n",
      "Epoch 1176/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 109.4904 - val_loss: 1160.7186\n",
      "Epoch 1177/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 78.5556 - val_loss: 1036.0766\n",
      "Epoch 1178/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 82.0372 - val_loss: 1187.9233\n",
      "Epoch 1179/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 77.0053 - val_loss: 1032.3824\n",
      "Epoch 1180/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 68.6080 - val_loss: 932.4383\n",
      "Epoch 1181/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 71.6202 - val_loss: 1140.5486\n",
      "Epoch 1182/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 69.2771 - val_loss: 1019.0385\n",
      "Epoch 1183/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 68.3692 - val_loss: 1100.5089\n",
      "Epoch 1184/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 60.1095 - val_loss: 1031.0849\n",
      "Epoch 1185/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 62.4057 - val_loss: 1001.5765\n",
      "Epoch 1186/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 63.0206 - val_loss: 1103.2070\n",
      "Epoch 1187/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 64.4411 - val_loss: 1034.1245\n",
      "Epoch 1188/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 70.6295 - val_loss: 994.7217\n",
      "Epoch 1189/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 66.3242 - val_loss: 1018.1565\n",
      "Epoch 1190/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 61.9061 - val_loss: 955.0358\n",
      "Epoch 1191/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 68.7685 - val_loss: 1029.8376\n",
      "Epoch 1192/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 65.7286 - val_loss: 1017.5154\n",
      "Epoch 1193/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 58.2833 - val_loss: 1129.9308\n",
      "Epoch 1194/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 58.9672 - val_loss: 1203.6924\n",
      "Epoch 1195/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 58.5040 - val_loss: 1075.6357\n",
      "Epoch 1196/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 57.3244 - val_loss: 1062.3826\n",
      "Epoch 1197/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 58.9772 - val_loss: 1063.0075\n",
      "Epoch 1198/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 65.2446 - val_loss: 976.1178\n",
      "Epoch 1199/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 64.4997 - val_loss: 1077.0460\n",
      "Epoch 1200/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 55.5505 - val_loss: 1001.3863\n",
      "Epoch 1201/2000\n",
      "2800/2800 [==============================] - 0s 146us/step - loss: 61.0110 - val_loss: 1095.3821\n",
      "Epoch 1202/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 59.2897 - val_loss: 1034.1233\n",
      "Epoch 1203/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 59.2188 - val_loss: 1146.8205\n",
      "Epoch 1204/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 57.7697 - val_loss: 1204.5733\n",
      "Epoch 1205/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 63.2335 - val_loss: 1185.9649\n",
      "Epoch 1206/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 55.6818 - val_loss: 1011.6808\n",
      "Epoch 1207/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 60.1397 - val_loss: 1269.3585\n",
      "Epoch 1208/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 60.8137 - val_loss: 1184.7992\n",
      "Epoch 1209/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 54.4047 - val_loss: 1104.7183\n",
      "Epoch 1210/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 57.2251 - val_loss: 1097.1511\n",
      "Epoch 1211/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 52.2247 - val_loss: 1032.4683\n",
      "Epoch 1212/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 51.8789 - val_loss: 1086.8374\n",
      "Epoch 1213/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 52.9815 - val_loss: 1028.3717\n",
      "Epoch 1214/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 54.1397 - val_loss: 1064.7399\n",
      "Epoch 1215/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 52.5803 - val_loss: 996.4339\n",
      "Epoch 1216/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 56.6845 - val_loss: 1047.2428\n",
      "Epoch 1217/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 58.4165 - val_loss: 1118.3902\n",
      "Epoch 1218/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 52.8454 - val_loss: 1123.6841\n",
      "Epoch 1219/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 52.9573 - val_loss: 1011.7314\n",
      "Epoch 1220/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 54.4657 - val_loss: 1028.9451\n",
      "Epoch 1221/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 55.1001 - val_loss: 1058.9792\n",
      "Epoch 1222/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 51.5073 - val_loss: 948.8031\n",
      "Epoch 1223/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 58.8114 - val_loss: 978.1335\n",
      "Epoch 1224/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 59.2697 - val_loss: 1057.4952\n",
      "Epoch 1225/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 53.3240 - val_loss: 977.0898\n",
      "Epoch 1226/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 52.6809 - val_loss: 1032.7633\n",
      "Epoch 1227/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 54.0850 - val_loss: 1120.2181\n",
      "Epoch 1228/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 51.8676 - val_loss: 1102.6527\n",
      "Epoch 1229/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 53.8190 - val_loss: 986.2829\n",
      "Epoch 1230/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 57.9229 - val_loss: 1182.0390\n",
      "Epoch 1231/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 58.8342 - val_loss: 1068.1066\n",
      "Epoch 1232/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 56.4800 - val_loss: 1154.7114\n",
      "Epoch 1233/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 60.5293 - val_loss: 1069.2415\n",
      "Epoch 1234/2000\n",
      "2800/2800 [==============================] - 0s 144us/step - loss: 55.7550 - val_loss: 1151.1171\n",
      "Epoch 1235/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 53.4275 - val_loss: 1109.7610\n",
      "Epoch 1236/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 54.8663 - val_loss: 1086.8559\n",
      "Epoch 1237/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 58.2244 - val_loss: 1047.4743\n",
      "Epoch 1238/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 53.0380 - val_loss: 1059.3546\n",
      "Epoch 1239/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 53.8227 - val_loss: 1118.4960\n",
      "Epoch 1240/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 134us/step - loss: 49.8503 - val_loss: 1318.1697\n",
      "Epoch 1241/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 51.6003 - val_loss: 944.0982\n",
      "Epoch 1242/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 61.5486 - val_loss: 1098.8483\n",
      "Epoch 1243/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 51.3490 - val_loss: 1046.0740\n",
      "Epoch 1244/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 55.4925 - val_loss: 979.2213\n",
      "Epoch 1245/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 57.7157 - val_loss: 1047.7890\n",
      "Epoch 1246/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 57.9419 - val_loss: 1148.0511\n",
      "Epoch 1247/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 52.6126 - val_loss: 1078.3375\n",
      "Epoch 1248/2000\n",
      "2800/2800 [==============================] - 1s 179us/step - loss: 54.2079 - val_loss: 1110.1238\n",
      "Epoch 1249/2000\n",
      "2800/2800 [==============================] - 1s 204us/step - loss: 58.2457 - val_loss: 1067.9748\n",
      "Epoch 1250/2000\n",
      "2800/2800 [==============================] - 0s 162us/step - loss: 52.5506 - val_loss: 1238.4879\n",
      "Epoch 1251/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 58.4229 - val_loss: 954.9726\n",
      "Epoch 1252/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 55.0447 - val_loss: 1021.6470\n",
      "Epoch 1253/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 53.0394 - val_loss: 1122.9707\n",
      "Epoch 1254/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 53.7040 - val_loss: 1006.0708\n",
      "Epoch 1255/2000\n",
      "2800/2800 [==============================] - 0s 154us/step - loss: 49.4218 - val_loss: 1026.2539\n",
      "Epoch 1256/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 60.0406 - val_loss: 1087.1939\n",
      "Epoch 1257/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 51.4350 - val_loss: 1001.0193\n",
      "Epoch 1258/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 49.9441 - val_loss: 1061.0783\n",
      "Epoch 1259/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 55.88 - 0s 149us/step - loss: 55.5920 - val_loss: 1082.0441\n",
      "Epoch 1260/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 54.4591 - val_loss: 974.5416\n",
      "Epoch 1261/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 52.2365 - val_loss: 1068.4671\n",
      "Epoch 1262/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 54.3938 - val_loss: 1092.2042\n",
      "Epoch 1263/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 54.2958 - val_loss: 1102.3531\n",
      "Epoch 1264/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 55.3398 - val_loss: 1016.2530\n",
      "Epoch 1265/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 50.3071 - val_loss: 1164.6218\n",
      "Epoch 1266/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 51.9021 - val_loss: 952.3752\n",
      "Epoch 1267/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 54.9355 - val_loss: 1001.2638\n",
      "Epoch 1268/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 53.8545 - val_loss: 1198.4319\n",
      "Epoch 1269/2000\n",
      "2800/2800 [==============================] - 0s 158us/step - loss: 50.5739 - val_loss: 971.4060\n",
      "Epoch 1270/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 52.5729 - val_loss: 1019.1773\n",
      "Epoch 1271/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 54.1673 - val_loss: 1194.0756\n",
      "Epoch 1272/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 56.7889 - val_loss: 1094.1824\n",
      "Epoch 1273/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 50.3096 - val_loss: 1098.2019\n",
      "Epoch 1274/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 58.5559 - val_loss: 1118.2682\n",
      "Epoch 1275/2000\n",
      "2800/2800 [==============================] - 0s 145us/step - loss: 53.8794 - val_loss: 995.9449\n",
      "Epoch 1276/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 53.4188 - val_loss: 1120.3750\n",
      "Epoch 1277/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 50.8909 - val_loss: 1094.7473\n",
      "Epoch 1278/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 57.8057 - val_loss: 1138.0366\n",
      "Epoch 1279/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 53.6083 - val_loss: 1149.1999\n",
      "Epoch 1280/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 57.2095 - val_loss: 1082.2141\n",
      "Epoch 1281/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 56.1974 - val_loss: 1035.1676\n",
      "Epoch 1282/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 54.9759 - val_loss: 1133.0796\n",
      "Epoch 1283/2000\n",
      "2800/2800 [==============================] - 0s 158us/step - loss: 50.1365 - val_loss: 1170.7249\n",
      "Epoch 1284/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 53.5239 - val_loss: 1102.2764\n",
      "Epoch 1285/2000\n",
      "2800/2800 [==============================] - 1s 180us/step - loss: 49.0393 - val_loss: 997.3532\n",
      "Epoch 1286/2000\n",
      "2800/2800 [==============================] - 0s 175us/step - loss: 47.7948 - val_loss: 1213.9877\n",
      "Epoch 1287/2000\n",
      "2800/2800 [==============================] - 0s 162us/step - loss: 52.9328 - val_loss: 1122.8438\n",
      "Epoch 1288/2000\n",
      "2800/2800 [==============================] - 1s 188us/step - loss: 50.6256 - val_loss: 1188.1057\n",
      "Epoch 1289/2000\n",
      "2800/2800 [==============================] - 0s 159us/step - loss: 49.8776 - val_loss: 1069.4392\n",
      "Epoch 1290/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 51.1499 - val_loss: 1097.1698\n",
      "Epoch 1291/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 54.1346 - val_loss: 1132.6571\n",
      "Epoch 1292/2000\n",
      "2800/2800 [==============================] - 0s 159us/step - loss: 53.5943 - val_loss: 1080.3109\n",
      "Epoch 1293/2000\n",
      "2800/2800 [==============================] - 0s 154us/step - loss: 55.5042 - val_loss: 1082.9810\n",
      "Epoch 1294/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 51.0477 - val_loss: 1130.5980\n",
      "Epoch 1295/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 56.9990 - val_loss: 993.8786\n",
      "Epoch 1296/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 54.7142 - val_loss: 1110.0439\n",
      "Epoch 1297/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 48.8986 - val_loss: 970.6181\n",
      "Epoch 1298/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 52.95 - 0s 92us/step - loss: 51.6136 - val_loss: 1074.0731\n",
      "Epoch 1299/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 51.1643 - val_loss: 1078.1823\n",
      "Epoch 1300/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 54.9574 - val_loss: 1015.7819\n",
      "Epoch 1301/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 50.4485 - val_loss: 1027.3547\n",
      "Epoch 1302/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 57.0578 - val_loss: 990.8749\n",
      "Epoch 1303/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 53.7245 - val_loss: 1078.0757\n",
      "Epoch 1304/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 51.9205 - val_loss: 1119.8406\n",
      "Epoch 1305/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 54.0555 - val_loss: 1122.9766\n",
      "Epoch 1306/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 52.1791 - val_loss: 1150.2834\n",
      "Epoch 1307/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 51.8270 - val_loss: 1067.0919\n",
      "Epoch 1308/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 56.5684 - val_loss: 1255.2978\n",
      "Epoch 1309/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 51.8738 - val_loss: 1168.8594\n",
      "Epoch 1310/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 53.1115 - val_loss: 1013.9623\n",
      "Epoch 1311/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 50.1459 - val_loss: 1057.8718\n",
      "Epoch 1312/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 83us/step - loss: 51.3734 - val_loss: 1047.8853\n",
      "Epoch 1313/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 48.5014 - val_loss: 1078.3507\n",
      "Epoch 1314/2000\n",
      "2800/2800 [==============================] - 0s 99us/step - loss: 50.5028 - val_loss: 1256.5675\n",
      "Epoch 1315/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 50.1885 - val_loss: 977.2165\n",
      "Epoch 1316/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 50.5456 - val_loss: 1092.5591\n",
      "Epoch 1317/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 50.6079 - val_loss: 1034.7860\n",
      "Epoch 1318/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 49.1037 - val_loss: 1058.5790\n",
      "Epoch 1319/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 48.9450 - val_loss: 1078.5049\n",
      "Epoch 1320/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 57.3564 - val_loss: 1199.2783\n",
      "Epoch 1321/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 48.7632 - val_loss: 1242.7448\n",
      "Epoch 1322/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 82.8571 - val_loss: 1112.6310\n",
      "Epoch 1323/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 75.3218 - val_loss: 1032.9366\n",
      "Epoch 1324/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 59.8352 - val_loss: 1117.1259\n",
      "Epoch 1325/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 61.1748 - val_loss: 1078.4064\n",
      "Epoch 1326/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 60.8155 - val_loss: 1066.9157\n",
      "Epoch 1327/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 71.9899 - val_loss: 1212.5599\n",
      "Epoch 1328/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 74.0682 - val_loss: 1034.0622\n",
      "Epoch 1329/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 67.0344 - val_loss: 1071.1065\n",
      "Epoch 1330/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 70.8127 - val_loss: 1062.9204\n",
      "Epoch 1331/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 66.9187 - val_loss: 1233.1052\n",
      "Epoch 1332/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 63.0795 - val_loss: 1137.7995\n",
      "Epoch 1333/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 61.4069 - val_loss: 1017.4138\n",
      "Epoch 1334/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 57.6760 - val_loss: 1100.5966\n",
      "Epoch 1335/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 62.7413 - val_loss: 1011.5400\n",
      "Epoch 1336/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 58.7126 - val_loss: 1090.8416\n",
      "Epoch 1337/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 54.3405 - val_loss: 1084.2080\n",
      "Epoch 1338/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 57.1190 - val_loss: 1040.9865\n",
      "Epoch 1339/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 54.7354 - val_loss: 1188.9926\n",
      "Epoch 1340/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 54.4538 - val_loss: 961.8621\n",
      "Epoch 1341/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 50.4964 - val_loss: 1416.8133\n",
      "Epoch 1342/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 58.5018 - val_loss: 1002.7868\n",
      "Epoch 1343/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 57.9945 - val_loss: 1072.5076\n",
      "Epoch 1344/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 52.5315 - val_loss: 1139.0769\n",
      "Epoch 1345/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 56.0383 - val_loss: 1044.5993\n",
      "Epoch 1346/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 54.3786 - val_loss: 1060.5770\n",
      "Epoch 1347/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 50.7619 - val_loss: 1107.8449\n",
      "Epoch 1348/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 49.7702 - val_loss: 1070.4056\n",
      "Epoch 1349/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 54.0441 - val_loss: 992.1842\n",
      "Epoch 1350/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 56.0478 - val_loss: 1005.6946\n",
      "Epoch 1351/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 52.6809 - val_loss: 942.4660\n",
      "Epoch 1352/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 50.8267 - val_loss: 951.3023\n",
      "Epoch 1353/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 53.5762 - val_loss: 1048.1909\n",
      "Epoch 1354/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 49.2719 - val_loss: 1033.2420\n",
      "Epoch 1355/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 50.7553 - val_loss: 1081.2771\n",
      "Epoch 1356/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 54.8390 - val_loss: 1088.4461\n",
      "Epoch 1357/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 54.0993 - val_loss: 975.9242\n",
      "Epoch 1358/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 52.2117 - val_loss: 1068.9379\n",
      "Epoch 1359/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 52.4753 - val_loss: 981.0995\n",
      "Epoch 1360/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 49.1039 - val_loss: 1101.9352\n",
      "Epoch 1361/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 52.1069 - val_loss: 1020.8522\n",
      "Epoch 1362/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 50.8790 - val_loss: 1015.6934\n",
      "Epoch 1363/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 54.8160 - val_loss: 1027.9239\n",
      "Epoch 1364/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 49.9365 - val_loss: 1130.1417\n",
      "Epoch 1365/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 50.7772 - val_loss: 1041.3819\n",
      "Epoch 1366/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 47.2301 - val_loss: 1042.6150\n",
      "Epoch 1367/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 53.1617 - val_loss: 1092.4448\n",
      "Epoch 1368/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 49.5215 - val_loss: 1032.7115\n",
      "Epoch 1369/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 48.9768 - val_loss: 1136.4492\n",
      "Epoch 1370/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 51.3169 - val_loss: 1056.1351\n",
      "Epoch 1371/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 48.2076 - val_loss: 967.3109\n",
      "Epoch 1372/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 49.9121 - val_loss: 1037.9805\n",
      "Epoch 1373/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 50.6614 - val_loss: 1029.4306\n",
      "Epoch 1374/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 50.7422 - val_loss: 1051.2224\n",
      "Epoch 1375/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 49.1957 - val_loss: 1015.0668\n",
      "Epoch 1376/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 50.8223 - val_loss: 959.5929\n",
      "Epoch 1377/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 51.3444 - val_loss: 1081.6721\n",
      "Epoch 1378/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 50.4535 - val_loss: 1040.6765\n",
      "Epoch 1379/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 46.5691 - val_loss: 1081.0562\n",
      "Epoch 1380/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 46.2160 - val_loss: 1263.9451\n",
      "Epoch 1381/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 51.0050 - val_loss: 1002.7175\n",
      "Epoch 1382/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 50.1325 - val_loss: 1062.9177\n",
      "Epoch 1383/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 57.3394 - val_loss: 1003.9173\n",
      "Epoch 1384/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 54.0693 - val_loss: 965.8352\n",
      "Epoch 1385/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 83us/step - loss: 54.0748 - val_loss: 1198.2241\n",
      "Epoch 1386/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 50.1871 - val_loss: 1223.5220\n",
      "Epoch 1387/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 51.7440 - val_loss: 1146.9153\n",
      "Epoch 1388/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 50.8841 - val_loss: 1008.3354\n",
      "Epoch 1389/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 47.7395 - val_loss: 1062.4144\n",
      "Epoch 1390/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 48.9584 - val_loss: 992.3041\n",
      "Epoch 1391/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 48.5237 - val_loss: 1077.8490\n",
      "Epoch 1392/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 45.8157 - val_loss: 1118.5426\n",
      "Epoch 1393/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 51.4047 - val_loss: 1207.5901\n",
      "Epoch 1394/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 46.7025 - val_loss: 1192.5276\n",
      "Epoch 1395/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 56.2503 - val_loss: 1188.9243\n",
      "Epoch 1396/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 47.4330 - val_loss: 999.1800\n",
      "Epoch 1397/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 46.2760 - val_loss: 1007.9531\n",
      "Epoch 1398/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 48.4542 - val_loss: 1165.5422\n",
      "Epoch 1399/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 55.3904 - val_loss: 1120.8716\n",
      "Epoch 1400/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 48.4210 - val_loss: 1021.1615\n",
      "Epoch 1401/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 49.8897 - val_loss: 1048.3803\n",
      "Epoch 1402/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 50.3475 - val_loss: 966.3956\n",
      "Epoch 1403/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 52.3336 - val_loss: 1076.3636\n",
      "Epoch 1404/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 52.0830 - val_loss: 1099.7339\n",
      "Epoch 1405/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 48.1911 - val_loss: 1069.2413\n",
      "Epoch 1406/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 49.6511 - val_loss: 997.3243\n",
      "Epoch 1407/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 52.3417 - val_loss: 1023.7848\n",
      "Epoch 1408/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 50.7151 - val_loss: 1012.8985\n",
      "Epoch 1409/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 52.1672 - val_loss: 1021.3270\n",
      "Epoch 1410/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 47.2922 - val_loss: 1176.7579\n",
      "Epoch 1411/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 45.6657 - val_loss: 1065.7911\n",
      "Epoch 1412/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 50.3723 - val_loss: 1147.7932\n",
      "Epoch 1413/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 51.5807 - val_loss: 1034.2027\n",
      "Epoch 1414/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 46.4508 - val_loss: 987.1398\n",
      "Epoch 1415/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 49.7445 - val_loss: 1081.2346 loss: 51.37\n",
      "Epoch 1416/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 51.5087 - val_loss: 1058.7768\n",
      "Epoch 1417/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 46.8035 - val_loss: 1226.1283\n",
      "Epoch 1418/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 48.4730 - val_loss: 1135.0663\n",
      "Epoch 1419/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 48.1225 - val_loss: 949.0627\n",
      "Epoch 1420/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 49.7405 - val_loss: 1039.6181\n",
      "Epoch 1421/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 48.8296 - val_loss: 1043.8035\n",
      "Epoch 1422/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 47.9609 - val_loss: 1130.1501\n",
      "Epoch 1423/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 52.8944 - val_loss: 1010.6848\n",
      "Epoch 1424/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 48.7710 - val_loss: 1075.2555\n",
      "Epoch 1425/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 47.3261 - val_loss: 1273.6386\n",
      "Epoch 1426/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 46.5892 - val_loss: 1028.9806\n",
      "Epoch 1427/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 46.8042 - val_loss: 1033.1808\n",
      "Epoch 1428/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 47.5910 - val_loss: 1066.3699\n",
      "Epoch 1429/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 47.4746 - val_loss: 1072.4379\n",
      "Epoch 1430/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 50.1888 - val_loss: 1060.7087\n",
      "Epoch 1431/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 51.2775 - val_loss: 1018.0474\n",
      "Epoch 1432/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 49.1410 - val_loss: 1022.5543\n",
      "Epoch 1433/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 48.0795 - val_loss: 1049.7907\n",
      "Epoch 1434/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 49.2773 - val_loss: 1086.0269\n",
      "Epoch 1435/2000\n",
      "2800/2800 [==============================] - 0s 150us/step - loss: 47.7162 - val_loss: 1034.2517\n",
      "Epoch 1436/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 46.8228 - val_loss: 1068.0938\n",
      "Epoch 1437/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 50.1296 - val_loss: 1135.5165\n",
      "Epoch 1438/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 47.6705 - val_loss: 1043.4921\n",
      "Epoch 1439/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 46.8354 - val_loss: 1091.0265\n",
      "Epoch 1440/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 49.8723 - val_loss: 1081.1159\n",
      "Epoch 1441/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 46.4069 - val_loss: 1040.7322\n",
      "Epoch 1442/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 46.1924 - val_loss: 1077.9692\n",
      "Epoch 1443/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 45.9405 - val_loss: 1203.2387\n",
      "Epoch 1444/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 47.6574 - val_loss: 1086.5933\n",
      "Epoch 1445/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 49.7967 - val_loss: 1025.5253\n",
      "Epoch 1446/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 46.6899 - val_loss: 1007.2037\n",
      "Epoch 1447/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 51.0553 - val_loss: 1002.1630\n",
      "Epoch 1448/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 76.8869 - val_loss: 992.0672\n",
      "Epoch 1449/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 68.5856 - val_loss: 1004.3484\n",
      "Epoch 1450/2000\n",
      "2800/2800 [==============================] - 0s 143us/step - loss: 67.9426 - val_loss: 940.5993\n",
      "Epoch 1451/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 64.7007 - val_loss: 1065.1738\n",
      "Epoch 1452/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 56.8090 - val_loss: 1080.4411\n",
      "Epoch 1453/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 58.0592 - val_loss: 1037.9904\n",
      "Epoch 1454/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 60.3117 - val_loss: 1063.7802\n",
      "Epoch 1455/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 51.0831 - val_loss: 1295.5547\n",
      "Epoch 1456/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 57.4464 - val_loss: 1138.0166\n",
      "Epoch 1457/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 129us/step - loss: 59.1536 - val_loss: 1084.9487\n",
      "Epoch 1458/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 51.3233 - val_loss: 1085.7401\n",
      "Epoch 1459/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 48.7475 - val_loss: 1116.6764\n",
      "Epoch 1460/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 54.2997 - val_loss: 1283.2003\n",
      "Epoch 1461/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 56.0736 - val_loss: 1137.8637\n",
      "Epoch 1462/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 48.7287 - val_loss: 938.7569\n",
      "Epoch 1463/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 54.2828 - val_loss: 983.4709\n",
      "Epoch 1464/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 53.0786 - val_loss: 1234.8222\n",
      "Epoch 1465/2000\n",
      "2800/2800 [==============================] - 0s 125us/step - loss: 50.6020 - val_loss: 1102.7132\n",
      "Epoch 1466/2000\n",
      "2800/2800 [==============================] - 0s 129us/step - loss: 49.2450 - val_loss: 1118.7002\n",
      "Epoch 1467/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 49.2832 - val_loss: 1134.8588\n",
      "Epoch 1468/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 50.3058 - val_loss: 1018.9574\n",
      "Epoch 1469/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 55.0186 - val_loss: 1162.8949\n",
      "Epoch 1470/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 51.7955 - val_loss: 1021.3746\n",
      "Epoch 1471/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 50.9639 - val_loss: 1109.6748\n",
      "Epoch 1472/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 48.3713 - val_loss: 1051.1143\n",
      "Epoch 1473/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 52.5130 - val_loss: 1109.9655\n",
      "Epoch 1474/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 47.3427 - val_loss: 1130.1134\n",
      "Epoch 1475/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 48.4114 - val_loss: 1173.3251\n",
      "Epoch 1476/2000\n",
      "2800/2800 [==============================] - 0s 127us/step - loss: 52.2891 - val_loss: 1138.6532\n",
      "Epoch 1477/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 49.7005 - val_loss: 1082.5418\n",
      "Epoch 1478/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 51.6321 - val_loss: 1043.7850\n",
      "Epoch 1479/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 56.8309 - val_loss: 1058.2264\n",
      "Epoch 1480/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 51.0659 - val_loss: 1267.4811\n",
      "Epoch 1481/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 52.5060 - val_loss: 1108.9365\n",
      "Epoch 1482/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 46.5751 - val_loss: 1017.7663\n",
      "Epoch 1483/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 46.8642 - val_loss: 1220.1652\n",
      "Epoch 1484/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 51.0448 - val_loss: 1099.5581\n",
      "Epoch 1485/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 46.4758 - val_loss: 1007.4172\n",
      "Epoch 1486/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 48.2006 - val_loss: 1258.2432\n",
      "Epoch 1487/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 52.2821 - val_loss: 1164.4216\n",
      "Epoch 1488/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 49.8785 - val_loss: 1018.1419\n",
      "Epoch 1489/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 48.9676 - val_loss: 1110.9344\n",
      "Epoch 1490/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 47.6033 - val_loss: 974.5246\n",
      "Epoch 1491/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 47.2507 - val_loss: 1089.7628\n",
      "Epoch 1492/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 48.8571 - val_loss: 1083.9980\n",
      "Epoch 1493/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 46.5686 - val_loss: 1056.2610\n",
      "Epoch 1494/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 48.8141 - val_loss: 1152.9664\n",
      "Epoch 1495/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 44.6238 - val_loss: 979.6971\n",
      "Epoch 1496/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 48.1406 - val_loss: 1016.9563\n",
      "Epoch 1497/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 45.0728 - val_loss: 1058.1684\n",
      "Epoch 1498/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 49.5774 - val_loss: 1087.8555\n",
      "Epoch 1499/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 47.5412 - val_loss: 1062.8496\n",
      "Epoch 1500/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 46.5660 - val_loss: 1074.8574\n",
      "Epoch 1501/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 45.0599 - val_loss: 1113.4142\n",
      "Epoch 1502/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 47.77 - 0s 86us/step - loss: 48.2366 - val_loss: 1013.6096\n",
      "Epoch 1503/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 45.7054 - val_loss: 1095.7757\n",
      "Epoch 1504/2000\n",
      "2800/2800 [==============================] - 1s 190us/step - loss: 46.2545 - val_loss: 1048.8403\n",
      "Epoch 1505/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 50.3221 - val_loss: 1100.5186\n",
      "Epoch 1506/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 45.2280 - val_loss: 1157.8795\n",
      "Epoch 1507/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 43.8814 - val_loss: 1071.9024\n",
      "Epoch 1508/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 47.3914 - val_loss: 1034.4008\n",
      "Epoch 1509/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 45.8063 - val_loss: 1001.5889\n",
      "Epoch 1510/2000\n",
      "2800/2800 [==============================] - 0s 99us/step - loss: 47.0811 - val_loss: 1309.0287\n",
      "Epoch 1511/2000\n",
      "2800/2800 [==============================] - 0s 111us/step - loss: 47.9687 - val_loss: 1065.5947\n",
      "Epoch 1512/2000\n",
      "2800/2800 [==============================] - 0s 103us/step - loss: 48.8465 - val_loss: 1143.9226\n",
      "Epoch 1513/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 47.7757 - val_loss: 1062.2539\n",
      "Epoch 1514/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 45.3371 - val_loss: 1147.5291\n",
      "Epoch 1515/2000\n",
      "2800/2800 [==============================] - 0s 99us/step - loss: 47.2189 - val_loss: 1045.9212\n",
      "Epoch 1516/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 45.4402 - val_loss: 1157.8864\n",
      "Epoch 1517/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 47.8394 - val_loss: 1056.0778\n",
      "Epoch 1518/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 49.8926 - val_loss: 1053.9297\n",
      "Epoch 1519/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 47.6571 - val_loss: 995.3815\n",
      "Epoch 1520/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 46.6650 - val_loss: 1122.5812\n",
      "Epoch 1521/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 44.8951 - val_loss: 979.6892\n",
      "Epoch 1522/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 48.7002 - val_loss: 1123.1306\n",
      "Epoch 1523/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 45.2899 - val_loss: 1147.1038\n",
      "Epoch 1524/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 44.9201 - val_loss: 1047.0070\n",
      "Epoch 1525/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 44.5209 - val_loss: 1141.1537\n",
      "Epoch 1526/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 42.8730 - val_loss: 1017.2484\n",
      "Epoch 1527/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 47.6101 - val_loss: 1106.1058\n",
      "Epoch 1528/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 43.7097 - val_loss: 1020.5797\n",
      "Epoch 1529/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 48.2459 - val_loss: 1254.2492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1530/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 47.4799 - val_loss: 992.1418\n",
      "Epoch 1531/2000\n",
      "2800/2800 [==============================] - 0s 112us/step - loss: 45.5215 - val_loss: 1109.1880\n",
      "Epoch 1532/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 51.6753 - val_loss: 1020.4170\n",
      "Epoch 1533/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 46.1406 - val_loss: 1059.0560\n",
      "Epoch 1534/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 48.3091 - val_loss: 983.2612\n",
      "Epoch 1535/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 45.6447 - val_loss: 1063.4936\n",
      "Epoch 1536/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 45.5688 - val_loss: 976.6334\n",
      "Epoch 1537/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 44.0540 - val_loss: 1062.1239\n",
      "Epoch 1538/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 47.2835 - val_loss: 1085.2479\n",
      "Epoch 1539/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 42.8177 - val_loss: 1053.0446\n",
      "Epoch 1540/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 43.7723 - val_loss: 1110.4934\n",
      "Epoch 1541/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 44.5688 - val_loss: 1028.6151\n",
      "Epoch 1542/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 43.2007 - val_loss: 1109.0249\n",
      "Epoch 1543/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 45.5996 - val_loss: 1244.4532\n",
      "Epoch 1544/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 48.7689 - val_loss: 1043.1865\n",
      "Epoch 1545/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 46.9283 - val_loss: 1058.5479\n",
      "Epoch 1546/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 44.2328 - val_loss: 1068.4358\n",
      "Epoch 1547/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 42.6540 - val_loss: 1019.8657\n",
      "Epoch 1548/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 46.4173 - val_loss: 1049.2061\n",
      "Epoch 1549/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 46.4461 - val_loss: 1057.4721\n",
      "Epoch 1550/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 44.5383 - val_loss: 1033.6771\n",
      "Epoch 1551/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 46.8335 - val_loss: 1163.4351\n",
      "Epoch 1552/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 44.6184 - val_loss: 1137.8418\n",
      "Epoch 1553/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 42.7429 - val_loss: 1154.3899\n",
      "Epoch 1554/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 47.4174 - val_loss: 1126.4424\n",
      "Epoch 1555/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 43.7095 - val_loss: 1215.0513\n",
      "Epoch 1556/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 44.5252 - val_loss: 1045.4707\n",
      "Epoch 1557/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 48.0779 - val_loss: 1036.7167\n",
      "Epoch 1558/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 42.6155 - val_loss: 1072.1538\n",
      "Epoch 1559/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 42.6034 - val_loss: 1090.4500\n",
      "Epoch 1560/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 41.7739 - val_loss: 1221.6341\n",
      "Epoch 1561/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 45.5818 - val_loss: 1137.7298\n",
      "Epoch 1562/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 45.6123 - val_loss: 1045.9352\n",
      "Epoch 1563/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 44.1993 - val_loss: 1084.7501\n",
      "Epoch 1564/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 43.4458 - val_loss: 1104.7095\n",
      "Epoch 1565/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 48.6063 - val_loss: 1121.7838\n",
      "Epoch 1566/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 43.4261 - val_loss: 1110.2739\n",
      "Epoch 1567/2000\n",
      "2800/2800 [==============================] - 0s 106us/step - loss: 42.2075 - val_loss: 1157.3597\n",
      "Epoch 1568/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 43.0368 - val_loss: 1081.1118\n",
      "Epoch 1569/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 45.8134 - val_loss: 1067.3866\n",
      "Epoch 1570/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 46.6919 - val_loss: 1019.7943\n",
      "Epoch 1571/2000\n",
      "2800/2800 [==============================] - 0s 106us/step - loss: 43.7540 - val_loss: 1073.8687\n",
      "Epoch 1572/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 44.4104 - val_loss: 976.3935\n",
      "Epoch 1573/2000\n",
      "2800/2800 [==============================] - 0s 103us/step - loss: 44.6613 - val_loss: 1105.6223\n",
      "Epoch 1574/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 44.2703 - val_loss: 1010.9654\n",
      "Epoch 1575/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 43.6764 - val_loss: 990.6376\n",
      "Epoch 1576/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 44.1457 - val_loss: 1024.7576\n",
      "Epoch 1577/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 42.2141 - val_loss: 1068.4058\n",
      "Epoch 1578/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 45.8922 - val_loss: 1185.3991\n",
      "Epoch 1579/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 44.8961 - val_loss: 1081.8269\n",
      "Epoch 1580/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 42.5669 - val_loss: 1064.4761\n",
      "Epoch 1581/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 42.4302 - val_loss: 1013.2804\n",
      "Epoch 1582/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 44.3033 - val_loss: 996.6604\n",
      "Epoch 1583/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 45.5424 - val_loss: 1146.5193\n",
      "Epoch 1584/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 41.5576 - val_loss: 1106.8679\n",
      "Epoch 1585/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 42.8646 - val_loss: 1125.3017\n",
      "Epoch 1586/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 45.3169 - val_loss: 1027.5149\n",
      "Epoch 1587/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 45.2227 - val_loss: 1109.4644\n",
      "Epoch 1588/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 43.0261 - val_loss: 1129.5132\n",
      "Epoch 1589/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 43.1116 - val_loss: 1210.6922\n",
      "Epoch 1590/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 46.0080 - val_loss: 954.0243\n",
      "Epoch 1591/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 44.7537 - val_loss: 1058.1635\n",
      "Epoch 1592/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 45.1454 - val_loss: 1126.6173\n",
      "Epoch 1593/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 43.2765 - val_loss: 1011.6299\n",
      "Epoch 1594/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 44.0857 - val_loss: 1022.7582\n",
      "Epoch 1595/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 40.5522 - val_loss: 1237.4418\n",
      "Epoch 1596/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 41.6551 - val_loss: 1107.0630\n",
      "Epoch 1597/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 40.3765 - val_loss: 1064.9977\n",
      "Epoch 1598/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 42.4208 - val_loss: 1073.8141\n",
      "Epoch 1599/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 44.6012 - val_loss: 1202.0604\n",
      "Epoch 1600/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 51.1542 - val_loss: 1209.9041\n",
      "Epoch 1601/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 43.6146 - val_loss: 1089.5900\n",
      "Epoch 1602/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 43.8137 - val_loss: 1101.7994\n",
      "Epoch 1603/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 84us/step - loss: 43.0852 - val_loss: 1116.9554\n",
      "Epoch 1604/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 46.7054 - val_loss: 1078.6917\n",
      "Epoch 1605/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 45.8661 - val_loss: 1068.3741\n",
      "Epoch 1606/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 41.7588 - val_loss: 1048.1652\n",
      "Epoch 1607/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 41.6271 - val_loss: 1051.5818\n",
      "Epoch 1608/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 42.6786 - val_loss: 1104.6007\n",
      "Epoch 1609/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 48.2463 - val_loss: 1018.5995\n",
      "Epoch 1610/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 45.6682 - val_loss: 1142.5847\n",
      "Epoch 1611/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 45.6979 - val_loss: 1055.5580\n",
      "Epoch 1612/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 41.6339 - val_loss: 1113.0179\n",
      "Epoch 1613/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 42.9547 - val_loss: 1213.6150\n",
      "Epoch 1614/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 45.2623 - val_loss: 1068.0800\n",
      "Epoch 1615/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 43.6859 - val_loss: 1025.3015\n",
      "Epoch 1616/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 39.6662 - val_loss: 1205.3243\n",
      "Epoch 1617/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 42.8755 - val_loss: 1050.0313\n",
      "Epoch 1618/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 44.7320 - val_loss: 1233.6356\n",
      "Epoch 1619/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 41.8317 - val_loss: 1150.0763\n",
      "Epoch 1620/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 42.4070 - val_loss: 1021.9238\n",
      "Epoch 1621/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 42.4188 - val_loss: 1067.3114\n",
      "Epoch 1622/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 45.6119 - val_loss: 1137.9286\n",
      "Epoch 1623/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 42.1607 - val_loss: 1157.8555\n",
      "Epoch 1624/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 43.3197 - val_loss: 1058.3694\n",
      "Epoch 1625/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 45.1291 - val_loss: 1073.4251\n",
      "Epoch 1626/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 49.9619 - val_loss: 1132.6370\n",
      "Epoch 1627/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 44.4049 - val_loss: 1185.2781\n",
      "Epoch 1628/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 45.5743 - val_loss: 1048.5887\n",
      "Epoch 1629/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 48.1332 - val_loss: 1087.1442\n",
      "Epoch 1630/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 42.6169 - val_loss: 1105.0550\n",
      "Epoch 1631/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 44.1846 - val_loss: 1085.7634\n",
      "Epoch 1632/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 45.4516 - val_loss: 1169.1142\n",
      "Epoch 1633/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 44.2536 - val_loss: 1084.5648\n",
      "Epoch 1634/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 40.6368 - val_loss: 1025.9562\n",
      "Epoch 1635/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 41.0979 - val_loss: 1044.7545\n",
      "Epoch 1636/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 40.8029 - val_loss: 1144.5905\n",
      "Epoch 1637/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 41.6863 - val_loss: 1147.5887\n",
      "Epoch 1638/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 45.6515 - val_loss: 1115.5839\n",
      "Epoch 1639/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 44.3124 - val_loss: 1253.3700\n",
      "Epoch 1640/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 47.3828 - val_loss: 1176.3859\n",
      "Epoch 1641/2000\n",
      "2800/2800 [==============================] - 0s 115us/step - loss: 40.9766 - val_loss: 1051.9182\n",
      "Epoch 1642/2000\n",
      "2800/2800 [==============================] - 0s 103us/step - loss: 45.0156 - val_loss: 1025.8247\n",
      "Epoch 1643/2000\n",
      "2800/2800 [==============================] - 0s 116us/step - loss: 45.1652 - val_loss: 1082.6288\n",
      "Epoch 1644/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 46.1447 - val_loss: 1117.3992\n",
      "Epoch 1645/2000\n",
      "2800/2800 [==============================] - 0s 103us/step - loss: 41.8185 - val_loss: 1145.6177\n",
      "Epoch 1646/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 40.7060 - val_loss: 1006.6453\n",
      "Epoch 1647/2000\n",
      "2800/2800 [==============================] - 0s 103us/step - loss: 45.2684 - val_loss: 1042.1595\n",
      "Epoch 1648/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 41.0220 - val_loss: 1058.1355\n",
      "Epoch 1649/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 43.6511 - val_loss: 1198.3777\n",
      "Epoch 1650/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 44.0235 - val_loss: 1047.3712\n",
      "Epoch 1651/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 45.5939 - val_loss: 1084.0210\n",
      "Epoch 1652/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 42.1892 - val_loss: 1136.5044\n",
      "Epoch 1653/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 41.1906 - val_loss: 1096.5922\n",
      "Epoch 1654/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 44.9148 - val_loss: 1038.5788\n",
      "Epoch 1655/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 41.1351 - val_loss: 1092.9727\n",
      "Epoch 1656/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 43.1282 - val_loss: 1044.5892\n",
      "Epoch 1657/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 42.9587 - val_loss: 1054.0955\n",
      "Epoch 1658/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 42.8401 - val_loss: 1275.7893\n",
      "Epoch 1659/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 42.5745 - val_loss: 1074.8363\n",
      "Epoch 1660/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 41.1700 - val_loss: 1161.9219\n",
      "Epoch 1661/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 41.5367 - val_loss: 1213.9773\n",
      "Epoch 1662/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 42.3555 - val_loss: 1149.4252\n",
      "Epoch 1663/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 45.9312 - val_loss: 1056.6823\n",
      "Epoch 1664/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 39.3653 - val_loss: 1069.6971\n",
      "Epoch 1665/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 39.1890 - val_loss: 1060.7189\n",
      "Epoch 1666/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 42.7907 - val_loss: 1128.4392\n",
      "Epoch 1667/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 39.9278 - val_loss: 1099.9970\n",
      "Epoch 1668/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 40.7403 - val_loss: 1092.2376\n",
      "Epoch 1669/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 40.7034 - val_loss: 1030.9904\n",
      "Epoch 1670/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 43.5774 - val_loss: 976.1102\n",
      "Epoch 1671/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 43.3933 - val_loss: 1138.5441\n",
      "Epoch 1672/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 40.8255 - val_loss: 1041.1276\n",
      "Epoch 1673/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 41.6899 - val_loss: 1167.1195\n",
      "Epoch 1674/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 45.0291 - val_loss: 1081.9976\n",
      "Epoch 1675/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 40.8944 - val_loss: 1102.1947\n",
      "Epoch 1676/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 93us/step - loss: 41.5771 - val_loss: 1160.9284\n",
      "Epoch 1677/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 41.8992 - val_loss: 1059.7485\n",
      "Epoch 1678/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 45.2853 - val_loss: 1075.8052\n",
      "Epoch 1679/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 46.5127 - val_loss: 1083.2190\n",
      "Epoch 1680/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 42.6684 - val_loss: 1005.9292\n",
      "Epoch 1681/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 41.0281 - val_loss: 1059.7798\n",
      "Epoch 1682/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 42.4601 - val_loss: 1039.4066\n",
      "Epoch 1683/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 41.0389 - val_loss: 1074.2353\n",
      "Epoch 1684/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 40.0300 - val_loss: 1140.9777\n",
      "Epoch 1685/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 42.3593 - val_loss: 1129.3711\n",
      "Epoch 1686/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 41.6592 - val_loss: 1064.0159\n",
      "Epoch 1687/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 41.2098 - val_loss: 1099.4484\n",
      "Epoch 1688/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 44.4113 - val_loss: 1032.9653\n",
      "Epoch 1689/2000\n",
      "2800/2800 [==============================] - 0s 110us/step - loss: 40.7845 - val_loss: 1135.7968\n",
      "Epoch 1690/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 41.1623 - val_loss: 1091.2350\n",
      "Epoch 1691/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 40.9710 - val_loss: 1051.9415\n",
      "Epoch 1692/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 38.6542 - val_loss: 1044.2068\n",
      "Epoch 1693/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 39.8653 - val_loss: 1144.0372\n",
      "Epoch 1694/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 41.2101 - val_loss: 1186.8073\n",
      "Epoch 1695/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 42.5110 - val_loss: 1098.9268\n",
      "Epoch 1696/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 42.9550 - val_loss: 1047.9672\n",
      "Epoch 1697/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 42.3552 - val_loss: 1258.2625\n",
      "Epoch 1698/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 44.0874 - val_loss: 1013.7381\n",
      "Epoch 1699/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 39.7996 - val_loss: 1233.8028\n",
      "Epoch 1700/2000\n",
      "2800/2800 [==============================] - 0s 110us/step - loss: 39.3815 - val_loss: 990.0405\n",
      "Epoch 1701/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 44.7399 - val_loss: 1123.5360\n",
      "Epoch 1702/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 42.1106 - val_loss: 1053.8142\n",
      "Epoch 1703/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 40.7994 - val_loss: 1135.9871\n",
      "Epoch 1704/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 43.0496 - val_loss: 1016.0823\n",
      "Epoch 1705/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 39.5034 - val_loss: 1078.0797\n",
      "Epoch 1706/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 41.0891 - val_loss: 1126.2683\n",
      "Epoch 1707/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 40.7844 - val_loss: 1077.7892\n",
      "Epoch 1708/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 41.5836 - val_loss: 1030.0953\n",
      "Epoch 1709/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 43.1250 - val_loss: 1102.1258\n",
      "Epoch 1710/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 42.9310 - val_loss: 1031.2336\n",
      "Epoch 1711/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 41.6409 - val_loss: 1029.9311\n",
      "Epoch 1712/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 42.1761 - val_loss: 1036.8990\n",
      "Epoch 1713/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 41.5384 - val_loss: 1052.1226\n",
      "Epoch 1714/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 39.1939 - val_loss: 1052.6819\n",
      "Epoch 1715/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 41.1132 - val_loss: 1111.3648\n",
      "Epoch 1716/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 41.3136 - val_loss: 1068.6993\n",
      "Epoch 1717/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 39.2865 - val_loss: 1203.1556\n",
      "Epoch 1718/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 42.3083 - val_loss: 1108.2184\n",
      "Epoch 1719/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 39.0688 - val_loss: 1037.9974\n",
      "Epoch 1720/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 42.6083 - val_loss: 1068.3327\n",
      "Epoch 1721/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 41.9858 - val_loss: 1102.2729\n",
      "Epoch 1722/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 39.2410 - val_loss: 1018.4239\n",
      "Epoch 1723/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 40.4497 - val_loss: 1112.6203\n",
      "Epoch 1724/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 44.0656 - val_loss: 1046.3837\n",
      "Epoch 1725/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 42.6725 - val_loss: 999.6709\n",
      "Epoch 1726/2000\n",
      "2800/2800 [==============================] - 0s 98us/step - loss: 41.7342 - val_loss: 1062.1777\n",
      "Epoch 1727/2000\n",
      "2800/2800 [==============================] - 0s 120us/step - loss: 43.0796 - val_loss: 1214.4871\n",
      "Epoch 1728/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 43.5628 - val_loss: 1139.9104\n",
      "Epoch 1729/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 40.4504 - val_loss: 1090.5630\n",
      "Epoch 1730/2000\n",
      "2800/2800 [==============================] - 0s 157us/step - loss: 40.3869 - val_loss: 1149.6728\n",
      "Epoch 1731/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 40.2096 - val_loss: 1236.0485\n",
      "Epoch 1732/2000\n",
      "2800/2800 [==============================] - 0s 153us/step - loss: 39.2559 - val_loss: 1064.5569\n",
      "Epoch 1733/2000\n",
      "2800/2800 [==============================] - 0s 156us/step - loss: 37.9454 - val_loss: 1154.5714\n",
      "Epoch 1734/2000\n",
      "2800/2800 [==============================] - 0s 154us/step - loss: 40.5284 - val_loss: 1073.2443\n",
      "Epoch 1735/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 42.0870 - val_loss: 1257.2443\n",
      "Epoch 1736/2000\n",
      "2800/2800 [==============================] - 0s 149us/step - loss: 42.6415 - val_loss: 1233.0244\n",
      "Epoch 1737/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 39.3711 - val_loss: 1064.5106\n",
      "Epoch 1738/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 39.8303 - val_loss: 1115.1793\n",
      "Epoch 1739/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 41.4535 - val_loss: 1058.0343\n",
      "Epoch 1740/2000\n",
      "2800/2800 [==============================] - 0s 154us/step - loss: 42.9924 - val_loss: 1077.2662\n",
      "Epoch 1741/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 40.1084 - val_loss: 1184.0420\n",
      "Epoch 1742/2000\n",
      "2800/2800 [==============================] - 0s 155us/step - loss: 41.2351 - val_loss: 1139.5270\n",
      "Epoch 1743/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 41.9972 - val_loss: 1024.8343\n",
      "Epoch 1744/2000\n",
      "2800/2800 [==============================] - 0s 151us/step - loss: 42.3693 - val_loss: 1103.8683\n",
      "Epoch 1745/2000\n",
      "2800/2800 [==============================] - 0s 157us/step - loss: 39.9495 - val_loss: 1137.0174\n",
      "Epoch 1746/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 38.3937 - val_loss: 1241.9817\n",
      "Epoch 1747/2000\n",
      "2800/2800 [==============================] - 0s 139us/step - loss: 42.4367 - val_loss: 1103.8231\n",
      "Epoch 1748/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 38.3398 - val_loss: 1207.7851\n",
      "Epoch 1749/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 134us/step - loss: 43.0809 - val_loss: 1079.8865\n",
      "Epoch 1750/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 40.6455 - val_loss: 1069.7923\n",
      "Epoch 1751/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 42.7509 - val_loss: 1096.1551\n",
      "Epoch 1752/2000\n",
      "2800/2800 [==============================] - 0s 142us/step - loss: 40.6150 - val_loss: 1124.0541\n",
      "Epoch 1753/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 42.3637 - val_loss: 1102.1865\n",
      "Epoch 1754/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 39.1141 - val_loss: 1011.4781\n",
      "Epoch 1755/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 42.3397 - val_loss: 1034.6262\n",
      "Epoch 1756/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 39.8692 - val_loss: 1128.1022\n",
      "Epoch 1757/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 41.0005 - val_loss: 1048.3678\n",
      "Epoch 1758/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 42.4502 - val_loss: 1124.5582\n",
      "Epoch 1759/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 41.7178 - val_loss: 1166.1346\n",
      "Epoch 1760/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 39.0318 - val_loss: 1078.4249\n",
      "Epoch 1761/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 40.8047 - val_loss: 1099.6525\n",
      "Epoch 1762/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 40.1320 - val_loss: 1077.8289\n",
      "Epoch 1763/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 40.0961 - val_loss: 1164.5301\n",
      "Epoch 1764/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 40.2252 - val_loss: 1063.8634\n",
      "Epoch 1765/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 38.4798 - val_loss: 1210.1240\n",
      "Epoch 1766/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 39.1612 - val_loss: 1217.8383\n",
      "Epoch 1767/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 42.4489 - val_loss: 1227.4281\n",
      "Epoch 1768/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 38.8971 - val_loss: 1112.6517\n",
      "Epoch 1769/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 37.2035 - val_loss: 1122.6279\n",
      "Epoch 1770/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 42.3453 - val_loss: 1242.3709\n",
      "Epoch 1771/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 39.8258 - val_loss: 1053.0263\n",
      "Epoch 1772/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 42.3479 - val_loss: 1137.3796\n",
      "Epoch 1773/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 37.9593 - val_loss: 1011.0545\n",
      "Epoch 1774/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 40.5331 - val_loss: 1058.9147\n",
      "Epoch 1775/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 41.4833 - val_loss: 1161.8347\n",
      "Epoch 1776/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 38.3030 - val_loss: 1079.3361\n",
      "Epoch 1777/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 42.4859 - val_loss: 1227.9994\n",
      "Epoch 1778/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 42.7626 - val_loss: 1066.5906\n",
      "Epoch 1779/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 41.8171 - val_loss: 1347.7048\n",
      "Epoch 1780/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 41.6609 - val_loss: 1129.1559\n",
      "Epoch 1781/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 40.6580 - val_loss: 1057.7609\n",
      "Epoch 1782/2000\n",
      "2800/2800 [==============================] - 0s 135us/step - loss: 39.9870 - val_loss: 1092.0616\n",
      "Epoch 1783/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 42.0383 - val_loss: 1160.8710\n",
      "Epoch 1784/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 39.6119 - val_loss: 1123.9016\n",
      "Epoch 1785/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 40.3282 - val_loss: 1184.0546\n",
      "Epoch 1786/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 38.2254 - val_loss: 1018.8697\n",
      "Epoch 1787/2000\n",
      "2800/2800 [==============================] - 0s 141us/step - loss: 41.1266 - val_loss: 1072.0448\n",
      "Epoch 1788/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 41.3402 - val_loss: 1038.9569\n",
      "Epoch 1789/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 39.8822 - val_loss: 1127.0652\n",
      "Epoch 1790/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 47.6778 - val_loss: 1135.2678\n",
      "Epoch 1791/2000\n",
      "2800/2800 [==============================] - 0s 131us/step - loss: 39.6686 - val_loss: 1149.0872\n",
      "Epoch 1792/2000\n",
      "2800/2800 [==============================] - 0s 134us/step - loss: 40.1167 - val_loss: 1150.0029\n",
      "Epoch 1793/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 40.2440 - val_loss: 1221.8135\n",
      "Epoch 1794/2000\n",
      "2800/2800 [==============================] - 0s 132us/step - loss: 38.4666 - val_loss: 1109.0775\n",
      "Epoch 1795/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 36.8368 - val_loss: 1079.2268\n",
      "Epoch 1796/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 39.4886 - val_loss: 1096.7631\n",
      "Epoch 1797/2000\n",
      "2800/2800 [==============================] - 0s 138us/step - loss: 39.6695 - val_loss: 1075.9256\n",
      "Epoch 1798/2000\n",
      "2800/2800 [==============================] - 0s 140us/step - loss: 39.4296 - val_loss: 1225.1176\n",
      "Epoch 1799/2000\n",
      "2800/2800 [==============================] - 0s 148us/step - loss: 40.2316 - val_loss: 1090.6026\n",
      "Epoch 1800/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 38.5046 - val_loss: 1173.0342\n",
      "Epoch 1801/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 43.6876 - val_loss: 1182.3247\n",
      "Epoch 1802/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 40.3007 - val_loss: 1072.4570\n",
      "Epoch 1803/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 37.2911 - val_loss: 1115.5797\n",
      "Epoch 1804/2000\n",
      "2800/2800 [==============================] - 0s 128us/step - loss: 37.7321 - val_loss: 1086.9996\n",
      "Epoch 1805/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 42.2413 - val_loss: 1157.5809\n",
      "Epoch 1806/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 40.9656 - val_loss: 1082.9668\n",
      "Epoch 1807/2000\n",
      "2800/2800 [==============================] - 0s 113us/step - loss: 43.1289 - val_loss: 1124.1908\n",
      "Epoch 1808/2000\n",
      "2800/2800 [==============================] - 0s 120us/step - loss: 40.4261 - val_loss: 1154.5148\n",
      "Epoch 1809/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 39.0345 - val_loss: 1055.6197\n",
      "Epoch 1810/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 38.2136 - val_loss: 1143.7914\n",
      "Epoch 1811/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 38.5196 - val_loss: 1052.7191\n",
      "Epoch 1812/2000\n",
      "2800/2800 [==============================] - 0s 116us/step - loss: 42.6309 - val_loss: 1064.4420\n",
      "Epoch 1813/2000\n",
      "2800/2800 [==============================] - 0s 120us/step - loss: 40.2304 - val_loss: 1021.8498\n",
      "Epoch 1814/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 40.0444 - val_loss: 1067.6612\n",
      "Epoch 1815/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 41.1143 - val_loss: 1082.8909\n",
      "Epoch 1816/2000\n",
      "2800/2800 [==============================] - 0s 120us/step - loss: 39.3985 - val_loss: 1076.4467\n",
      "Epoch 1817/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 38.8222 - val_loss: 1061.9963\n",
      "Epoch 1818/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 40.6505 - val_loss: 1146.3925\n",
      "Epoch 1819/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 39.0558 - val_loss: 1068.1546\n",
      "Epoch 1820/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 38.1837 - val_loss: 1080.9783\n",
      "Epoch 1821/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 117us/step - loss: 38.5975 - val_loss: 1082.0687\n",
      "Epoch 1822/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 39.2783 - val_loss: 1053.0356\n",
      "Epoch 1823/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 38.8515 - val_loss: 1068.7936\n",
      "Epoch 1824/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 37.9139 - val_loss: 1058.2669\n",
      "Epoch 1825/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 39.8044 - val_loss: 1076.1348\n",
      "Epoch 1826/2000\n",
      "2800/2800 [==============================] - 0s 123us/step - loss: 39.9232 - val_loss: 1094.0440\n",
      "Epoch 1827/2000\n",
      "2800/2800 [==============================] - 0s 122us/step - loss: 40.6731 - val_loss: 1178.7305\n",
      "Epoch 1828/2000\n",
      "2800/2800 [==============================] - 0s 111us/step - loss: 39.2076 - val_loss: 1246.3946\n",
      "Epoch 1829/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 39.3144 - val_loss: 1115.4116\n",
      "Epoch 1830/2000\n",
      "2800/2800 [==============================] - 0s 116us/step - loss: 37.2564 - val_loss: 1033.2574\n",
      "Epoch 1831/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 42.1455 - val_loss: 1317.9612\n",
      "Epoch 1832/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 42.4711 - val_loss: 1184.3449\n",
      "Epoch 1833/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 40.0059 - val_loss: 1099.5571\n",
      "Epoch 1834/2000\n",
      "2800/2800 [==============================] - 0s 115us/step - loss: 40.6201 - val_loss: 1116.3559\n",
      "Epoch 1835/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 37.9951 - val_loss: 1200.4360\n",
      "Epoch 1836/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 37.8094 - val_loss: 1097.2087\n",
      "Epoch 1837/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 40.2967 - val_loss: 1152.9044\n",
      "Epoch 1838/2000\n",
      "2800/2800 [==============================] - 0s 130us/step - loss: 38.2020 - val_loss: 1054.6081\n",
      "Epoch 1839/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 39.7627 - val_loss: 1126.5489\n",
      "Epoch 1840/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 39.5699 - val_loss: 1085.0560\n",
      "Epoch 1841/2000\n",
      "2800/2800 [==============================] - 0s 114us/step - loss: 40.7794 - val_loss: 1119.7130\n",
      "Epoch 1842/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 41.7762 - val_loss: 1091.5309\n",
      "Epoch 1843/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 39.4177 - val_loss: 1153.0488\n",
      "Epoch 1844/2000\n",
      "2800/2800 [==============================] - 0s 113us/step - loss: 37.3119 - val_loss: 986.2535\n",
      "Epoch 1845/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 37.7435 - val_loss: 1140.5955\n",
      "Epoch 1846/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 36.9850 - val_loss: 1034.7064\n",
      "Epoch 1847/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 40.6017 - val_loss: 1152.7671\n",
      "Epoch 1848/2000\n",
      "2800/2800 [==============================] - 0s 108us/step - loss: 38.2939 - val_loss: 1106.6765\n",
      "Epoch 1849/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 39.2260 - val_loss: 1107.6163\n",
      "Epoch 1850/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 37.0269 - val_loss: 1148.7692\n",
      "Epoch 1851/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 38.9691 - val_loss: 1095.9659\n",
      "Epoch 1852/2000\n",
      "2800/2800 [==============================] - 0s 111us/step - loss: 39.4904 - val_loss: 1126.9840\n",
      "Epoch 1853/2000\n",
      "2800/2800 [==============================] - 0s 116us/step - loss: 38.9557 - val_loss: 1115.3318\n",
      "Epoch 1854/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 39.7770 - val_loss: 1100.1287\n",
      "Epoch 1855/2000\n",
      "2800/2800 [==============================] - 0s 137us/step - loss: 39.1129 - val_loss: 1119.7983\n",
      "Epoch 1856/2000\n",
      "2800/2800 [==============================] - 0s 133us/step - loss: 38.4869 - val_loss: 1123.2364\n",
      "Epoch 1857/2000\n",
      "2800/2800 [==============================] - 0s 126us/step - loss: 36.2733 - val_loss: 1184.0543\n",
      "Epoch 1858/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 39.1136 - val_loss: 1081.9791\n",
      "Epoch 1859/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 38.4983 - val_loss: 1156.3582\n",
      "Epoch 1860/2000\n",
      "2800/2800 [==============================] - 0s 117us/step - loss: 38.4599 - val_loss: 1045.9249\n",
      "Epoch 1861/2000\n",
      "2800/2800 [==============================] - 0s 101us/step - loss: 37.7882 - val_loss: 1130.3192\n",
      "Epoch 1862/2000\n",
      "2800/2800 [==============================] - 0s 113us/step - loss: 40.1028 - val_loss: 1173.8438\n",
      "Epoch 1863/2000\n",
      "2800/2800 [==============================] - 0s 124us/step - loss: 39.7846 - val_loss: 1110.3912\n",
      "Epoch 1864/2000\n",
      "2800/2800 [==============================] - 0s 121us/step - loss: 39.6421 - val_loss: 1178.0120\n",
      "Epoch 1865/2000\n",
      "2800/2800 [==============================] - 0s 173us/step - loss: 44.6670 - val_loss: 1175.5985\n",
      "Epoch 1866/2000\n",
      "2800/2800 [==============================] - 0s 158us/step - loss: 36.8889 - val_loss: 1299.7863\n",
      "Epoch 1867/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 40.3958 - val_loss: 1029.2663\n",
      "Epoch 1868/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 38.0460 - val_loss: 1214.3940\n",
      "Epoch 1869/2000\n",
      "2800/2800 [==============================] - 0s 110us/step - loss: 37.2601 - val_loss: 1024.6087\n",
      "Epoch 1870/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 41.1147 - val_loss: 1114.1110\n",
      "Epoch 1871/2000\n",
      "2800/2800 [==============================] - 0s 103us/step - loss: 37.7867 - val_loss: 1085.7096\n",
      "Epoch 1872/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 41.5342 - val_loss: 1093.4140\n",
      "Epoch 1873/2000\n",
      "2800/2800 [==============================] - 0s 113us/step - loss: 39.1205 - val_loss: 1123.1262\n",
      "Epoch 1874/2000\n",
      "2800/2800 [==============================] - 0s 118us/step - loss: 38.4795 - val_loss: 1236.0734\n",
      "Epoch 1875/2000\n",
      "2800/2800 [==============================] - 0s 136us/step - loss: 42.3000 - val_loss: 1135.4137\n",
      "Epoch 1876/2000\n",
      "2800/2800 [==============================] - 1s 246us/step - loss: 38.3983 - val_loss: 1087.3306\n",
      "Epoch 1877/2000\n",
      "2800/2800 [==============================] - 1s 191us/step - loss: 40.0430 - val_loss: 1097.8843\n",
      "Epoch 1878/2000\n",
      "2800/2800 [==============================] - 0s 152us/step - loss: 40.1484 - val_loss: 1083.1556\n",
      "Epoch 1879/2000\n",
      "2800/2800 [==============================] - 0s 119us/step - loss: 38.7825 - val_loss: 1106.9677\n",
      "Epoch 1880/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 39.2247 - val_loss: 1163.8275\n",
      "Epoch 1881/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 41.0566 - val_loss: 1313.6629\n",
      "Epoch 1882/2000\n",
      "2800/2800 [==============================] - 0s 106us/step - loss: 41.7369 - val_loss: 1096.0747\n",
      "Epoch 1883/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 38.7592 - val_loss: 1167.0903\n",
      "Epoch 1884/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 40.1576 - val_loss: 1169.9793\n",
      "Epoch 1885/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 37.6747 - val_loss: 1126.9666\n",
      "Epoch 1886/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 38.6429 - val_loss: 1131.2072\n",
      "Epoch 1887/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 40.8008 - val_loss: 1076.5838\n",
      "Epoch 1888/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 37.7529 - val_loss: 1084.5005\n",
      "Epoch 1889/2000\n",
      "2800/2800 [==============================] - 0s 99us/step - loss: 40.8722 - val_loss: 1102.4619\n",
      "Epoch 1890/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 39.0163 - val_loss: 1158.0232\n",
      "Epoch 1891/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 39.0599 - val_loss: 1019.7219\n",
      "Epoch 1892/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 39.8754 - val_loss: 1065.7056\n",
      "Epoch 1893/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 89us/step - loss: 35.9586 - val_loss: 1128.0170\n",
      "Epoch 1894/2000\n",
      "2800/2800 [==============================] - 0s 102us/step - loss: 43.6830 - val_loss: 1082.8533\n",
      "Epoch 1895/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 38.3112 - val_loss: 1092.0396\n",
      "Epoch 1896/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 38.6173 - val_loss: 1023.4169\n",
      "Epoch 1897/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 38.6923 - val_loss: 1185.0786\n",
      "Epoch 1898/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 41.1837 - val_loss: 1058.8367\n",
      "Epoch 1899/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 39.8877 - val_loss: 1074.0429\n",
      "Epoch 1900/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 39.4066 - val_loss: 1072.0995\n",
      "Epoch 1901/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 39.3605 - val_loss: 1129.2292\n",
      "Epoch 1902/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 41.5274 - val_loss: 1120.1754\n",
      "Epoch 1903/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 37.8887 - val_loss: 1141.2640\n",
      "Epoch 1904/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 39.0528 - val_loss: 1213.9819\n",
      "Epoch 1905/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 40.0434 - val_loss: 1129.6944\n",
      "Epoch 1906/2000\n",
      "2800/2800 [==============================] - 0s 107us/step - loss: 39.6606 - val_loss: 1130.1386\n",
      "Epoch 1907/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 40.3656 - val_loss: 1148.3467\n",
      "Epoch 1908/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 41.1461 - val_loss: 1163.5445\n",
      "Epoch 1909/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 38.5691 - val_loss: 1129.1084\n",
      "Epoch 1910/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 39.5582 - val_loss: 1175.8984\n",
      "Epoch 1911/2000\n",
      "2800/2800 [==============================] - 0s 104us/step - loss: 41.1287 - val_loss: 1170.4183\n",
      "Epoch 1912/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 36.3295 - val_loss: 1114.1585\n",
      "Epoch 1913/2000\n",
      "2800/2800 [==============================] - 0s 100us/step - loss: 37.7450 - val_loss: 1120.5487\n",
      "Epoch 1914/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 39.9339 - val_loss: 1076.4597\n",
      "Epoch 1915/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 37.2664 - val_loss: 1081.3865\n",
      "Epoch 1916/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 38.0608 - val_loss: 1174.1134\n",
      "Epoch 1917/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 39.7720 - val_loss: 1123.8247\n",
      "Epoch 1918/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 40.8108 - val_loss: 1027.2676\n",
      "Epoch 1919/2000\n",
      "2800/2800 [==============================] - 0s 92us/step - loss: 38.2225 - val_loss: 1160.6732\n",
      "Epoch 1920/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 38.3899 - val_loss: 1160.9987\n",
      "Epoch 1921/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 40.2147 - val_loss: 1058.4395\n",
      "Epoch 1922/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 40.5723 - val_loss: 1042.1379\n",
      "Epoch 1923/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 36.6614 - val_loss: 1092.7717\n",
      "Epoch 1924/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 37.1167 - val_loss: 1155.9877\n",
      "Epoch 1925/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 37.5481 - val_loss: 1077.7179\n",
      "Epoch 1926/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 38.7904 - val_loss: 1147.5782\n",
      "Epoch 1927/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 38.5538 - val_loss: 1134.3993\n",
      "Epoch 1928/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 37.6810 - val_loss: 1105.4833\n",
      "Epoch 1929/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 37.6323 - val_loss: 1102.8008\n",
      "Epoch 1930/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 37.9408 - val_loss: 1124.6625\n",
      "Epoch 1931/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 37.6096 - val_loss: 1052.9790\n",
      "Epoch 1932/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 36.8088 - val_loss: 1036.8507\n",
      "Epoch 1933/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 40.3972 - val_loss: 1046.8168\n",
      "Epoch 1934/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 41.4235 - val_loss: 1163.1480\n",
      "Epoch 1935/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 41.3171 - val_loss: 1098.4368\n",
      "Epoch 1936/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 39.4240 - val_loss: 1180.4370\n",
      "Epoch 1937/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 37.3891 - val_loss: 1140.8133\n",
      "Epoch 1938/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 39.8745 - val_loss: 1234.4368\n",
      "Epoch 1939/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 37.6650 - val_loss: 1182.9310\n",
      "Epoch 1940/2000\n",
      "2800/2800 [==============================] - 0s 79us/step - loss: 36.5659 - val_loss: 1077.0227\n",
      "Epoch 1941/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 38.8118 - val_loss: 1123.0709\n",
      "Epoch 1942/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 39.7382 - val_loss: 1197.4996\n",
      "Epoch 1943/2000\n",
      "2800/2800 [==============================] - 0s 83us/step - loss: 37.1303 - val_loss: 1102.2000\n",
      "Epoch 1944/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 39.32 - 0s 86us/step - loss: 38.9561 - val_loss: 1095.1163\n",
      "Epoch 1945/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 39.7070 - val_loss: 1084.2435\n",
      "Epoch 1946/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 36.7191 - val_loss: 1067.1835\n",
      "Epoch 1947/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 38.9350 - val_loss: 1116.3372\n",
      "Epoch 1948/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 39.9781 - val_loss: 1208.3992\n",
      "Epoch 1949/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 41.5024 - val_loss: 1108.6990\n",
      "Epoch 1950/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 37.1299 - val_loss: 1087.9425\n",
      "Epoch 1951/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 39.4389 - val_loss: 1085.7758\n",
      "Epoch 1952/2000\n",
      "2800/2800 [==============================] - ETA: 0s - loss: 37.12 - 0s 84us/step - loss: 37.3154 - val_loss: 1188.9638\n",
      "Epoch 1953/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 39.7632 - val_loss: 1190.0564\n",
      "Epoch 1954/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 41.1167 - val_loss: 1199.7675\n",
      "Epoch 1955/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 37.4766 - val_loss: 1144.6682\n",
      "Epoch 1956/2000\n",
      "2800/2800 [==============================] - 0s 78us/step - loss: 39.0330 - val_loss: 1020.8660\n",
      "Epoch 1957/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 38.9610 - val_loss: 1180.9656\n",
      "Epoch 1958/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 37.5717 - val_loss: 1116.4188\n",
      "Epoch 1959/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 37.0868 - val_loss: 1147.0310\n",
      "Epoch 1960/2000\n",
      "2800/2800 [==============================] - 0s 79us/step - loss: 38.1462 - val_loss: 1100.6380\n",
      "Epoch 1961/2000\n",
      "2800/2800 [==============================] - 0s 86us/step - loss: 38.8766 - val_loss: 1088.5854\n",
      "Epoch 1962/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 38.0219 - val_loss: 1056.7334\n",
      "Epoch 1963/2000\n",
      "2800/2800 [==============================] - 0s 97us/step - loss: 37.8121 - val_loss: 1117.1389\n",
      "Epoch 1964/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 39.8035 - val_loss: 1107.0097\n",
      "Epoch 1965/2000\n",
      "2800/2800 [==============================] - 0s 106us/step - loss: 37.6453 - val_loss: 1115.8253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1966/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 38.3637 - val_loss: 1207.7918\n",
      "Epoch 1967/2000\n",
      "2800/2800 [==============================] - 0s 96us/step - loss: 37.6221 - val_loss: 1092.9546\n",
      "Epoch 1968/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 36.2836 - val_loss: 1195.2119\n",
      "Epoch 1969/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 36.6937 - val_loss: 1077.9313\n",
      "Epoch 1970/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 41.4381 - val_loss: 1005.1806\n",
      "Epoch 1971/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 39.1349 - val_loss: 1145.5068\n",
      "Epoch 1972/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 38.1808 - val_loss: 1194.2640\n",
      "Epoch 1973/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 38.1208 - val_loss: 1086.7788\n",
      "Epoch 1974/2000\n",
      "2800/2800 [==============================] - 0s 87us/step - loss: 36.3426 - val_loss: 1104.4070\n",
      "Epoch 1975/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 36.1071 - val_loss: 1064.0038\n",
      "Epoch 1976/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 37.8862 - val_loss: 1035.7691\n",
      "Epoch 1977/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 42.6729 - val_loss: 1055.3901\n",
      "Epoch 1978/2000\n",
      "2800/2800 [==============================] - 0s 88us/step - loss: 38.6653 - val_loss: 1105.1050\n",
      "Epoch 1979/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 38.5145 - val_loss: 1096.3867\n",
      "Epoch 1980/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 37.7048 - val_loss: 1148.7962\n",
      "Epoch 1981/2000\n",
      "2800/2800 [==============================] - 0s 95us/step - loss: 37.2106 - val_loss: 1161.3569\n",
      "Epoch 1982/2000\n",
      "2800/2800 [==============================] - 0s 94us/step - loss: 39.0482 - val_loss: 1087.7124\n",
      "Epoch 1983/2000\n",
      "2800/2800 [==============================] - 0s 89us/step - loss: 39.5463 - val_loss: 1087.3868\n",
      "Epoch 1984/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 39.7489 - val_loss: 1124.8647\n",
      "Epoch 1985/2000\n",
      "2800/2800 [==============================] - 0s 84us/step - loss: 37.9094 - val_loss: 1074.2434\n",
      "Epoch 1986/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 36.8406 - val_loss: 1065.6316\n",
      "Epoch 1987/2000\n",
      "2800/2800 [==============================] - 0s 93us/step - loss: 39.1574 - val_loss: 1089.4529\n",
      "Epoch 1988/2000\n",
      "2800/2800 [==============================] - 0s 85us/step - loss: 37.0689 - val_loss: 1119.3125\n",
      "Epoch 1989/2000\n",
      "2800/2800 [==============================] - 0s 91us/step - loss: 38.2866 - val_loss: 1145.5639\n",
      "Epoch 1990/2000\n",
      "2800/2800 [==============================] - 0s 90us/step - loss: 37.6544 - val_loss: 1094.7946\n",
      "Epoch 1991/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 36.7371 - val_loss: 1096.3754\n",
      "Epoch 1992/2000\n",
      "2800/2800 [==============================] - 0s 79us/step - loss: 40.2181 - val_loss: 1178.5324\n",
      "Epoch 1993/2000\n",
      "2800/2800 [==============================] - 0s 78us/step - loss: 37.1073 - val_loss: 1105.0040\n",
      "Epoch 1994/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 37.1768 - val_loss: 1149.3728\n",
      "Epoch 1995/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 40.2751 - val_loss: 1135.1151\n",
      "Epoch 1996/2000\n",
      "2800/2800 [==============================] - 0s 78us/step - loss: 35.3754 - val_loss: 1131.7100\n",
      "Epoch 1997/2000\n",
      "2800/2800 [==============================] - 0s 80us/step - loss: 36.9331 - val_loss: 1094.9270\n",
      "Epoch 1998/2000\n",
      "2800/2800 [==============================] - 0s 78us/step - loss: 37.6579 - val_loss: 1168.6523\n",
      "Epoch 1999/2000\n",
      "2800/2800 [==============================] - 0s 82us/step - loss: 36.1547 - val_loss: 1075.9788\n",
      "Epoch 2000/2000\n",
      "2800/2800 [==============================] - 0s 81us/step - loss: 35.1678 - val_loss: 1075.1534\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=X_train.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(100,kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train, y_train, epochs=2000, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x298ddb431d0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, im = plt.subplots()\n",
    "im.plot(history.history['val_loss'])\n",
    "im.plot(history.history['loss'])\n",
    "im.set(xlabel='Epochs', ylabel='Loss (MSE)',title='Entrenamiento con Relu and Mini Batch=10')\n",
    "im.legend(('Val_Loss', 'Loss' )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_1=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-44.674477 ],\n",
       "       [-21.300762 ],\n",
       "       [ 14.53437  ],\n",
       "       ...,\n",
       "       [ 59.173946 ],\n",
       "       [ 27.213917 ],\n",
       "       [  1.0071435]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28],\n",
       "       [28],\n",
       "       [28],\n",
       "       ...,\n",
       "       [10],\n",
       "       [10],\n",
       "       [10]], dtype=uint8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
